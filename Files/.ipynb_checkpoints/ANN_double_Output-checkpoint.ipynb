{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/elhamshaerirahbar-bot/my-colab-project/blob/main/Untitled13.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I8S8VCECinI2",
    "outputId": "442f5205-7a69-49da-a8a5-1f42a4fc8e16"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\brian\\pycharmprojects\\pythonproject1\\.venv\\lib\\site-packages (2.3.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\brian\\pycharmprojects\\pythonproject1\\.venv\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\brian\\pycharmprojects\\pythonproject1\\.venv\\lib\\site-packages (2.20.0rc0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\brian\\pycharmprojects\\pythonproject1\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\brian\\pycharmprojects\\pythonproject1\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\brian\\pycharmprojects\\pythonproject1\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\brian\\pycharmprojects\\pythonproject1\\.venv\\lib\\site-packages (from tensorflow) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\brian\\pycharmprojects\\pythonproject1\\.venv\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\brian\\pycharmprojects\\pythonproject1\\.venv\\lib\\site-packages (from tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\brian\\pycharmprojects\\pythonproject1\\.venv\\lib\\site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google_pasta>=0.1.1 in c:\\users\\brian\\pycharmprojects\\pythonproject1\\.venv\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\brian\\pycharmprojects\\pythonproject1\\.venv\\lib\\site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt_einsum>=2.3.2 in c:\\users\\brian\\pycharmprojects\\pythonproject1\\.venv\\lib\\site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\brian\\pycharmprojects\\pythonproject1\\.venv\\lib\\site-packages (from tensorflow) (25.0)\n",
      "Requirement already satisfied: protobuf>=5.28.0 in c:\\users\\brian\\pycharmprojects\\pythonproject1\\.venv\\lib\\site-packages (from tensorflow) (6.31.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\brian\\pycharmprojects\\pythonproject1\\.venv\\lib\\site-packages (from tensorflow) (2.32.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\brian\\pycharmprojects\\pythonproject1\\.venv\\lib\\site-packages (from tensorflow) (80.9.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\brian\\pycharmprojects\\pythonproject1\\.venv\\lib\\site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\brian\\pycharmprojects\\pythonproject1\\.venv\\lib\\site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in c:\\users\\brian\\pycharmprojects\\pythonproject1\\.venv\\lib\\site-packages (from tensorflow) (4.14.1)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\brian\\pycharmprojects\\pythonproject1\\.venv\\lib\\site-packages (from tensorflow) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\brian\\pycharmprojects\\pythonproject1\\.venv\\lib\\site-packages (from tensorflow) (1.74.0)\n",
      "Requirement already satisfied: tensorboard~=2.20.0 in c:\\users\\brian\\pycharmprojects\\pythonproject1\\.venv\\lib\\site-packages (from tensorflow) (2.20.0)\n",
      "Requirement already satisfied: keras>=3.10.0 in c:\\users\\brian\\pycharmprojects\\pythonproject1\\.venv\\lib\\site-packages (from tensorflow) (3.11.1)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\brian\\pycharmprojects\\pythonproject1\\.venv\\lib\\site-packages (from tensorflow) (3.14.0)\n",
      "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in c:\\users\\brian\\pycharmprojects\\pythonproject1\\.venv\\lib\\site-packages (from tensorflow) (0.5.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\brian\\pycharmprojects\\pythonproject1\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\brian\\pycharmprojects\\pythonproject1\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\brian\\pycharmprojects\\pythonproject1\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\brian\\pycharmprojects\\pythonproject1\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2025.8.3)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\brian\\pycharmprojects\\pythonproject1\\.venv\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.8.2)\n",
      "Requirement already satisfied: pillow in c:\\users\\brian\\pycharmprojects\\pythonproject1\\.venv\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (11.3.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\brian\\pycharmprojects\\pythonproject1\\.venv\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\brian\\pycharmprojects\\pythonproject1\\.venv\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\brian\\pycharmprojects\\pythonproject1\\.venv\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: rich in c:\\users\\brian\\pycharmprojects\\pythonproject1\\.venv\\lib\\site-packages (from keras>=3.10.0->tensorflow) (14.1.0)\n",
      "Requirement already satisfied: namex in c:\\users\\brian\\pycharmprojects\\pythonproject1\\.venv\\lib\\site-packages (from keras>=3.10.0->tensorflow) (0.1.0)\n",
      "Requirement already satisfied: optree in c:\\users\\brian\\pycharmprojects\\pythonproject1\\.venv\\lib\\site-packages (from keras>=3.10.0->tensorflow) (0.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\brian\\pycharmprojects\\pythonproject1\\.venv\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\brian\\pycharmprojects\\pythonproject1\\.venv\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\brian\\pycharmprojects\\pythonproject1\\.venv\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\brian\\pycharmprojects\\pythonproject1\\.venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy pandas tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "0Syqe_mYiwTx"
   },
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd, re, unicodedata\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, callbacks\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.metrics import mean_absolute_percentage_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "id": "mQM3DjQNi0he",
    "outputId": "b018038a-6aec-4a81-e135-9ae3d186ce03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Run  Factor A  Factor B  Factor C  Factor D  Response 1 (Experimental)  \\\n",
      "0    1       110         7        50        10                    1127.19   \n",
      "1    2        85        13        50        10                    1024.97   \n",
      "2    3       101         1       500        60                    1950.00   \n",
      "3    4       101         1       500        60                    2223.17   \n",
      "4    5        50        10        50        10                    1845.60   \n",
      "\n",
      "   Response 2 (Experimental)  \n",
      "0                    1321.65  \n",
      "1                    1339.35  \n",
      "2                    2878.90  \n",
      "3                    2989.00  \n",
      "4                    2690.50  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Just read the file\n",
    "df = pd.read_csv('Exp_Mn_Mw_Value.txt', sep='\\t')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7Y9BwNofi4-T",
    "outputId": "92a5901e-bb2a-48a0-bd86-613b27ddaa22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (25, 4)  y: (25, 2)\n"
     ]
    }
   ],
   "source": [
    "#select columns by position\n",
    "X = df.iloc[:, 1:5].astype(float).to_numpy()  # First 4 columns as features\n",
    "y = df.iloc[:, 5:7].astype(float).to_numpy()  # Next 2 columns as targets\n",
    "\n",
    "print(\"X:\", X.shape, \" y:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e2AA1C6Wi77R",
    "outputId": "187af667-98c6-49ba-857a-f7dab735a70b"
   },
   "outputs": [],
   "source": [
    "# For 20% test, 16% val, 64% train:\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, shuffle=True, random_state=SEED\n",
    ")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.20, shuffle=True, random_state=SEED  # 0.20 of 0.80 = 0.16 overall\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "WAeiXioBjGXg"
   },
   "outputs": [],
   "source": [
    "#scaling\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "# Ensure y has correct shape (samples, 2_targets)\n",
    "assert y_train.ndim == 2 and y_train.shape[1] == 2, \"y muss (n,2) sein\"\n",
    "\n",
    "# --- Scale features (X) using StandardScaler ---\n",
    "# Fit scaler only on training data to prevent data leakage\n",
    "x_scaler = StandardScaler().fit(X_train)\n",
    "\n",
    "# Transform all sets using the same scaler (mean=0, std=1)\n",
    "X_train_z = x_scaler.transform(X_train)  # Standardized training features\n",
    "X_val_z   = x_scaler.transform(X_val)    # Standardized validation features  \n",
    "X_test_z  = x_scaler.transform(X_test)   # Standardized test features\n",
    "\n",
    "# --- Scale targets (y) to [-1,1] range for tanh activation ---\n",
    "# Fit scaler only on training targets to prevent data leakage\n",
    "y_scaler  = MinMaxScaler(feature_range=(-1, 1)).fit(y_train)  # y_train: (n,2)\n",
    "\n",
    "# Transform all target sets using the same scaler\n",
    "y_train_s = y_scaler.transform(y_train)  # Scaled training targets [-1,1]\n",
    "y_val_s   = y_scaler.transform(y_val)    # Scaled validation targets [-1,1]\n",
    "y_test_s  = y_scaler.transform(y_test)   # Scaled test targets [-1,1]\n",
    "\n",
    "def inv_y(y_s):\n",
    "    \"\"\"\n",
    "    Inverse transform scaled predictions back to original units.\n",
    "    Args: y_s - scaled predictions in [-1,1] range\n",
    "    Returns: predictions in original scale (Mn, Mw units)\n",
    "    \"\"\"\n",
    "    return y_scaler.inverse_transform(y_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "dBaYl7yWjKR5"
   },
   "outputs": [],
   "source": [
    "ann = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "otPjam8rjLqF",
    "outputId": "c6ef832c-fff8-46ad-d52a-389edaf598dd"
   },
   "outputs": [],
   "source": [
    "#build ann\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, regularizers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "# Verify target shape (samples, 2_outputs)\n",
    "assert y_train_s.ndim == 2 and y_train_s.shape[1] == 2, \"y muss (n,2) sein!\"\n",
    "\n",
    "# Build Neural Network: 4 inputs -> 3 hidden layers (16,8,8) -> 2 outputs\n",
    "# Architecture: fully connected feedforward network with regularization\n",
    "# Purpose: predict 2 molecular weight responses (Mn, Mw) from 4 factors\n",
    "model = Sequential([\n",
    "    # Input layer: 4 features -> 16 neurons\n",
    "    # ReLU activation for non-linearity\n",
    "    # L2 regularization to prevent overfitting\n",
    "    layers.Dense(16, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(1e-5),\n",
    "                 input_shape=(X_train_z.shape[1],)),\n",
    "    \n",
    "    # Dropout to reduce overfitting (randomly disable 10% of neurons)\n",
    "    Dropout(0.1),\n",
    "    \n",
    "    # Hidden layer: 16 -> 8 neurons\n",
    "    layers.Dense(8, activation='relu', \n",
    "                 kernel_regularizer=regularizers.l2(1e-5)),\n",
    "    Dropout(0.1),\n",
    "    \n",
    "    # Hidden layer: 8 -> 8 neurons  \n",
    "    layers.Dense(8, activation='relu', \n",
    "                 kernel_regularizer=regularizers.l2(1e-5)),\n",
    "    Dropout(0.1),\n",
    "    \n",
    "    # Output layer: 8 -> 2 outputs (Mn, Mw)\n",
    "    # Tanh activation outputs [-1,1] to match our scaled targets\n",
    "    layers.Dense(2, activation='tanh')\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "# MSE loss for regression, MAE and MAPE as additional metrics\n",
    "model.compile(optimizer='adam', \n",
    "              loss='mse', \n",
    "              metrics=['mae', 'mape'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9SiGI9kdjLoU",
    "outputId": "a56e7194-c800-4b0d-945d-a91716792ef7",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - loss: 0.4955 - mae: 0.5618 - mape: 127.8901 - val_loss: 0.3090 - val_mae: 0.4277 - val_mape: 152.5448\n",
      "Epoch 2/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.4625 - mae: 0.5620 - mape: 144.5456 - val_loss: 0.3059 - val_mae: 0.4250 - val_mape: 149.7119\n",
      "Epoch 3/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 0.4403 - mae: 0.5414 - mape: 118.5616 - val_loss: 0.3029 - val_mae: 0.4223 - val_mape: 146.7029\n",
      "Epoch 4/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.4583 - mae: 0.5439 - mape: 128.5281 - val_loss: 0.3000 - val_mae: 0.4195 - val_mape: 143.1570\n",
      "Epoch 5/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.4455 - mae: 0.5269 - mape: 110.1732 - val_loss: 0.2973 - val_mae: 0.4167 - val_mape: 139.8304\n",
      "Epoch 6/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - loss: 0.4521 - mae: 0.5362 - mape: 108.6820 - val_loss: 0.2946 - val_mae: 0.4141 - val_mape: 136.7745\n",
      "Epoch 7/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - loss: 0.4686 - mae: 0.5346 - mape: 106.6530 - val_loss: 0.2921 - val_mae: 0.4113 - val_mape: 133.5207\n",
      "Epoch 8/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - loss: 0.4426 - mae: 0.5447 - mape: 121.1121 - val_loss: 0.2896 - val_mae: 0.4085 - val_mape: 130.0687\n",
      "Epoch 9/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.4391 - mae: 0.5405 - mape: 124.9520 - val_loss: 0.2872 - val_mae: 0.4073 - val_mape: 130.2601\n",
      "Epoch 10/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.4321 - mae: 0.5126 - mape: 100.0389 - val_loss: 0.2850 - val_mae: 0.4067 - val_mape: 131.7410\n",
      "Epoch 11/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.4503 - mae: 0.5486 - mape: 124.0016 - val_loss: 0.2829 - val_mae: 0.4060 - val_mape: 133.2179\n",
      "Epoch 12/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 0.4600 - mae: 0.5469 - mape: 125.3223 - val_loss: 0.2810 - val_mae: 0.4054 - val_mape: 134.5993\n",
      "Epoch 13/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.4494 - mae: 0.5361 - mape: 114.7793 - val_loss: 0.2792 - val_mae: 0.4048 - val_mape: 135.9072\n",
      "Epoch 14/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.4093 - mae: 0.5275 - mape: 122.2278 - val_loss: 0.2775 - val_mae: 0.4041 - val_mape: 136.8169\n",
      "Epoch 15/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.3905 - mae: 0.4922 - mape: 93.6728 - val_loss: 0.2757 - val_mae: 0.4021 - val_mape: 135.6983\n",
      "Epoch 16/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - loss: 0.4335 - mae: 0.5275 - mape: 117.2925 - val_loss: 0.2740 - val_mae: 0.4001 - val_mape: 134.4457\n",
      "Epoch 17/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - loss: 0.4241 - mae: 0.5305 - mape: 112.4470 - val_loss: 0.2724 - val_mae: 0.3981 - val_mape: 133.0817\n",
      "Epoch 18/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 0.3970 - mae: 0.5088 - mape: 104.6599 - val_loss: 0.2708 - val_mae: 0.3960 - val_mape: 131.6466\n",
      "Epoch 19/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 0.4000 - mae: 0.5052 - mape: 97.6986 - val_loss: 0.2693 - val_mae: 0.3939 - val_mape: 130.1493\n",
      "Epoch 20/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.3928 - mae: 0.5277 - mape: 121.1603 - val_loss: 0.2678 - val_mae: 0.3918 - val_mape: 128.6432\n",
      "Epoch 21/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.4191 - mae: 0.5175 - mape: 105.4299 - val_loss: 0.2664 - val_mae: 0.3898 - val_mape: 127.3198\n",
      "Epoch 22/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 0.3747 - mae: 0.5087 - mape: 113.1841 - val_loss: 0.2650 - val_mae: 0.3878 - val_mape: 126.0173\n",
      "Epoch 23/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.3850 - mae: 0.5031 - mape: 102.4468 - val_loss: 0.2636 - val_mae: 0.3857 - val_mape: 124.7362\n",
      "Epoch 24/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.3943 - mae: 0.4985 - mape: 99.7125 - val_loss: 0.2623 - val_mae: 0.3838 - val_mape: 123.4379\n",
      "Epoch 25/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.3700 - mae: 0.4946 - mape: 100.5555 - val_loss: 0.2610 - val_mae: 0.3819 - val_mape: 122.3044\n",
      "Epoch 26/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.4175 - mae: 0.5212 - mape: 109.2990 - val_loss: 0.2598 - val_mae: 0.3800 - val_mape: 121.2493\n",
      "Epoch 27/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.3582 - mae: 0.4861 - mape: 95.2796 - val_loss: 0.2586 - val_mae: 0.3782 - val_mape: 120.3495\n",
      "Epoch 28/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.3680 - mae: 0.4956 - mape: 97.8438 - val_loss: 0.2574 - val_mae: 0.3765 - val_mape: 119.5827\n",
      "Epoch 29/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.3399 - mae: 0.4797 - mape: 93.9762 - val_loss: 0.2562 - val_mae: 0.3747 - val_mape: 118.8101\n",
      "Epoch 30/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.3894 - mae: 0.4939 - mape: 95.6620 - val_loss: 0.2548 - val_mae: 0.3727 - val_mape: 117.8882\n",
      "Epoch 31/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.3670 - mae: 0.4696 - mape: 81.5771 - val_loss: 0.2535 - val_mae: 0.3708 - val_mape: 117.2513\n",
      "Epoch 32/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.3580 - mae: 0.4799 - mape: 96.0963 - val_loss: 0.2521 - val_mae: 0.3688 - val_mape: 116.3451\n",
      "Epoch 33/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.3487 - mae: 0.4768 - mape: 91.7332 - val_loss: 0.2507 - val_mae: 0.3667 - val_mape: 115.4080\n",
      "Epoch 34/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - loss: 0.3614 - mae: 0.4806 - mape: 92.8116 - val_loss: 0.2494 - val_mae: 0.3646 - val_mape: 114.5815\n",
      "Epoch 35/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.3504 - mae: 0.4763 - mape: 100.5626 - val_loss: 0.2480 - val_mae: 0.3624 - val_mape: 113.7072\n",
      "Epoch 36/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - loss: 0.3351 - mae: 0.4654 - mape: 87.0141 - val_loss: 0.2467 - val_mae: 0.3603 - val_mape: 112.8100\n",
      "Epoch 37/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.3511 - mae: 0.4780 - mape: 94.0371 - val_loss: 0.2454 - val_mae: 0.3583 - val_mape: 112.0982\n",
      "Epoch 38/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.3242 - mae: 0.4606 - mape: 86.1265 - val_loss: 0.2441 - val_mae: 0.3562 - val_mape: 111.4827\n",
      "Epoch 39/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.3292 - mae: 0.4537 - mape: 86.1595 - val_loss: 0.2427 - val_mae: 0.3541 - val_mape: 110.9048\n",
      "Epoch 40/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.3372 - mae: 0.4716 - mape: 91.5691 - val_loss: 0.2414 - val_mae: 0.3520 - val_mape: 110.2947\n",
      "Epoch 41/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - loss: 0.3484 - mae: 0.4757 - mape: 93.7151 - val_loss: 0.2401 - val_mae: 0.3497 - val_mape: 109.6109\n",
      "Epoch 42/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.3657 - mae: 0.4771 - mape: 89.4604 - val_loss: 0.2389 - val_mae: 0.3476 - val_mape: 108.9014\n",
      "Epoch 43/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.3749 - mae: 0.4759 - mape: 82.4397 - val_loss: 0.2378 - val_mae: 0.3455 - val_mape: 108.2777\n",
      "Epoch 44/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.3120 - mae: 0.4516 - mape: 91.1113 - val_loss: 0.2367 - val_mae: 0.3436 - val_mape: 107.7726\n",
      "Epoch 45/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.3242 - mae: 0.4582 - mape: 90.8523 - val_loss: 0.2356 - val_mae: 0.3416 - val_mape: 107.3214\n",
      "Epoch 46/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 0.3193 - mae: 0.4428 - mape: 80.3523 - val_loss: 0.2345 - val_mae: 0.3397 - val_mape: 106.9571\n",
      "Epoch 47/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.3140 - mae: 0.4440 - mape: 80.3394 - val_loss: 0.2334 - val_mae: 0.3377 - val_mape: 106.5831\n",
      "Epoch 48/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.3259 - mae: 0.4482 - mape: 83.9073 - val_loss: 0.2323 - val_mae: 0.3358 - val_mape: 106.2397\n",
      "Epoch 49/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.3361 - mae: 0.4447 - mape: 78.2714 - val_loss: 0.2314 - val_mae: 0.3340 - val_mape: 105.8925\n",
      "Epoch 50/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.3069 - mae: 0.4495 - mape: 90.2382 - val_loss: 0.2305 - val_mae: 0.3324 - val_mape: 105.6189\n",
      "Epoch 51/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.3386 - mae: 0.4693 - mape: 94.0044 - val_loss: 0.2296 - val_mae: 0.3307 - val_mape: 105.3201\n",
      "Epoch 52/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.2937 - mae: 0.4351 - mape: 86.9447 - val_loss: 0.2288 - val_mae: 0.3291 - val_mape: 105.1211\n",
      "Epoch 53/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 0.3438 - mae: 0.4567 - mape: 79.1913 - val_loss: 0.2279 - val_mae: 0.3276 - val_mape: 105.0443\n",
      "Epoch 54/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.3221 - mae: 0.4460 - mape: 77.9085 - val_loss: 0.2270 - val_mae: 0.3261 - val_mape: 105.0828\n",
      "Epoch 55/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.2994 - mae: 0.4371 - mape: 86.3772 - val_loss: 0.2261 - val_mae: 0.3249 - val_mape: 105.2532\n",
      "Epoch 56/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.2773 - mae: 0.4180 - mape: 80.3290 - val_loss: 0.2251 - val_mae: 0.3238 - val_mape: 105.5704\n",
      "Epoch 57/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - loss: 0.3093 - mae: 0.4424 - mape: 92.4426 - val_loss: 0.2240 - val_mae: 0.3225 - val_mape: 105.8122\n",
      "Epoch 58/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - loss: 0.3490 - mae: 0.4642 - mape: 98.1524 - val_loss: 0.2229 - val_mae: 0.3212 - val_mape: 105.9523\n",
      "Epoch 59/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 0.3102 - mae: 0.4556 - mape: 98.2069 - val_loss: 0.2218 - val_mae: 0.3198 - val_mape: 106.0219\n",
      "Epoch 60/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.2840 - mae: 0.4216 - mape: 83.4492 - val_loss: 0.2206 - val_mae: 0.3184 - val_mape: 106.0375\n",
      "Epoch 61/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - loss: 0.2734 - mae: 0.4289 - mape: 87.3666 - val_loss: 0.2194 - val_mae: 0.3170 - val_mape: 106.1905\n",
      "Epoch 62/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.3177 - mae: 0.4398 - mape: 91.9366 - val_loss: 0.2184 - val_mae: 0.3157 - val_mape: 106.2230\n",
      "Epoch 63/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - loss: 0.2647 - mae: 0.4088 - mape: 79.7984 - val_loss: 0.2174 - val_mae: 0.3145 - val_mape: 106.3990\n",
      "Epoch 64/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 0.2987 - mae: 0.4280 - mape: 83.7225 - val_loss: 0.2164 - val_mae: 0.3133 - val_mape: 106.6259\n",
      "Epoch 65/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.2950 - mae: 0.4341 - mape: 91.4927 - val_loss: 0.2154 - val_mae: 0.3121 - val_mape: 106.9609\n",
      "Epoch 66/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.2714 - mae: 0.4233 - mape: 91.3628 - val_loss: 0.2143 - val_mae: 0.3109 - val_mape: 107.1731\n",
      "Epoch 67/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - loss: 0.3338 - mae: 0.4582 - mape: 85.3824 - val_loss: 0.2133 - val_mae: 0.3100 - val_mape: 107.5350\n",
      "Epoch 68/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - loss: 0.2936 - mae: 0.4348 - mape: 92.6087 - val_loss: 0.2125 - val_mae: 0.3091 - val_mape: 107.8867\n",
      "Epoch 69/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.2672 - mae: 0.4093 - mape: 80.8212 - val_loss: 0.2116 - val_mae: 0.3083 - val_mape: 108.4456\n",
      "Epoch 70/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.2533 - mae: 0.4085 - mape: 84.8269 - val_loss: 0.2105 - val_mae: 0.3076 - val_mape: 109.1423\n",
      "Epoch 71/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.3224 - mae: 0.4396 - mape: 83.1440 - val_loss: 0.2096 - val_mae: 0.3069 - val_mape: 109.8253\n",
      "Epoch 72/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.2565 - mae: 0.3960 - mape: 80.1037 - val_loss: 0.2086 - val_mae: 0.3062 - val_mape: 110.4047\n",
      "Epoch 73/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.3249 - mae: 0.4291 - mape: 81.2509 - val_loss: 0.2078 - val_mae: 0.3055 - val_mape: 110.8501\n",
      "Epoch 74/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - loss: 0.2814 - mae: 0.4095 - mape: 77.7621 - val_loss: 0.2069 - val_mae: 0.3049 - val_mape: 111.3789\n",
      "Epoch 75/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.2532 - mae: 0.4073 - mape: 90.9295 - val_loss: 0.2059 - val_mae: 0.3042 - val_mape: 111.9041\n",
      "Epoch 76/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 0.2983 - mae: 0.4349 - mape: 97.0812 - val_loss: 0.2049 - val_mae: 0.3036 - val_mape: 112.4620\n",
      "Epoch 77/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - loss: 0.2685 - mae: 0.4087 - mape: 82.3518 - val_loss: 0.2038 - val_mae: 0.3028 - val_mape: 112.9787\n",
      "Epoch 78/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - loss: 0.3324 - mae: 0.4538 - mape: 96.7229 - val_loss: 0.2026 - val_mae: 0.3016 - val_mape: 113.3384\n",
      "Epoch 79/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 0.2600 - mae: 0.3995 - mape: 76.3116 - val_loss: 0.2013 - val_mae: 0.3004 - val_mape: 113.7880\n",
      "Epoch 80/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.2934 - mae: 0.4219 - mape: 86.4027 - val_loss: 0.2000 - val_mae: 0.2992 - val_mape: 114.1898\n",
      "Epoch 81/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 0.2567 - mae: 0.4113 - mape: 84.1657 - val_loss: 0.1987 - val_mae: 0.2980 - val_mape: 114.6313\n",
      "Epoch 82/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 0.2884 - mae: 0.4184 - mape: 85.6410 - val_loss: 0.1974 - val_mae: 0.2968 - val_mape: 115.0935\n",
      "Epoch 83/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.2669 - mae: 0.4140 - mape: 85.5771 - val_loss: 0.1960 - val_mae: 0.2955 - val_mape: 115.5354\n",
      "Epoch 84/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.2770 - mae: 0.4140 - mape: 80.1836 - val_loss: 0.1945 - val_mae: 0.2943 - val_mape: 116.1204\n",
      "Epoch 85/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - loss: 0.2788 - mae: 0.3969 - mape: 79.2742 - val_loss: 0.1932 - val_mae: 0.2930 - val_mape: 116.4322\n",
      "Epoch 86/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - loss: 0.2240 - mae: 0.3943 - mape: 96.5878 - val_loss: 0.1918 - val_mae: 0.2914 - val_mape: 116.4142\n",
      "Epoch 87/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - loss: 0.2598 - mae: 0.3925 - mape: 87.4532 - val_loss: 0.1904 - val_mae: 0.2897 - val_mape: 116.3441\n",
      "Epoch 88/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.2630 - mae: 0.3905 - mape: 78.3987 - val_loss: 0.1892 - val_mae: 0.2881 - val_mape: 116.2880\n",
      "Epoch 89/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.2242 - mae: 0.3793 - mape: 82.1846 - val_loss: 0.1878 - val_mae: 0.2864 - val_mape: 116.2747\n",
      "Epoch 90/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.3349 - mae: 0.4454 - mape: 101.2210 - val_loss: 0.1868 - val_mae: 0.2848 - val_mape: 116.2595\n",
      "Epoch 91/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.2492 - mae: 0.3856 - mape: 78.5474 - val_loss: 0.1858 - val_mae: 0.2834 - val_mape: 116.3634\n",
      "Epoch 92/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.2349 - mae: 0.3777 - mape: 73.4794 - val_loss: 0.1847 - val_mae: 0.2820 - val_mape: 116.5320\n",
      "Epoch 93/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.2563 - mae: 0.3807 - mape: 79.3529 - val_loss: 0.1836 - val_mae: 0.2805 - val_mape: 116.6251\n",
      "Epoch 94/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 0.2513 - mae: 0.4029 - mape: 95.6516 - val_loss: 0.1824 - val_mae: 0.2790 - val_mape: 116.7374\n",
      "Epoch 95/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 0.2734 - mae: 0.3950 - mape: 84.3624 - val_loss: 0.1813 - val_mae: 0.2776 - val_mape: 116.8462\n",
      "Epoch 96/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - loss: 0.2460 - mae: 0.3951 - mape: 92.0973 - val_loss: 0.1801 - val_mae: 0.2760 - val_mape: 116.8632\n",
      "Epoch 97/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.2515 - mae: 0.3990 - mape: 81.4591 - val_loss: 0.1788 - val_mae: 0.2744 - val_mape: 116.7851\n",
      "Epoch 98/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.2150 - mae: 0.3693 - mape: 76.9684 - val_loss: 0.1774 - val_mae: 0.2727 - val_mape: 116.6335\n",
      "Epoch 99/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.1870 - mae: 0.3507 - mape: 74.0465 - val_loss: 0.1760 - val_mae: 0.2710 - val_mape: 116.5464\n",
      "Epoch 100/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.2566 - mae: 0.4138 - mape: 104.6389 - val_loss: 0.1745 - val_mae: 0.2692 - val_mape: 116.3449\n",
      "Epoch 101/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.1914 - mae: 0.3510 - mape: 89.3809 - val_loss: 0.1729 - val_mae: 0.2674 - val_mape: 116.0063\n",
      "Epoch 102/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.2036 - mae: 0.3636 - mape: 81.4649 - val_loss: 0.1713 - val_mae: 0.2656 - val_mape: 115.7777\n",
      "Epoch 103/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.2354 - mae: 0.3679 - mape: 78.1174 - val_loss: 0.1696 - val_mae: 0.2636 - val_mape: 115.4948\n",
      "Epoch 104/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.1939 - mae: 0.3498 - mape: 83.0620 - val_loss: 0.1680 - val_mae: 0.2617 - val_mape: 115.2478\n",
      "Epoch 105/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.2434 - mae: 0.3967 - mape: 84.3885 - val_loss: 0.1664 - val_mae: 0.2605 - val_mape: 115.2778\n",
      "Epoch 106/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - loss: 0.1659 - mae: 0.3242 - mape: 72.4229 - val_loss: 0.1647 - val_mae: 0.2592 - val_mape: 115.3422\n",
      "Epoch 107/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.2239 - mae: 0.3648 - mape: 90.2801 - val_loss: 0.1629 - val_mae: 0.2579 - val_mape: 115.3481\n",
      "Epoch 108/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.2302 - mae: 0.3803 - mape: 99.3765 - val_loss: 0.1611 - val_mae: 0.2567 - val_mape: 115.4626\n",
      "Epoch 109/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.2265 - mae: 0.3794 - mape: 102.0412 - val_loss: 0.1594 - val_mae: 0.2553 - val_mape: 115.3946\n",
      "Epoch 110/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.2347 - mae: 0.3787 - mape: 87.6268 - val_loss: 0.1578 - val_mae: 0.2541 - val_mape: 115.3214\n",
      "Epoch 111/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.2301 - mae: 0.4012 - mape: 107.5481 - val_loss: 0.1562 - val_mae: 0.2529 - val_mape: 115.1935\n",
      "Epoch 112/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.2035 - mae: 0.3607 - mape: 86.7288 - val_loss: 0.1547 - val_mae: 0.2518 - val_mape: 115.1913\n",
      "Epoch 113/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - loss: 0.1929 - mae: 0.3646 - mape: 85.4608 - val_loss: 0.1531 - val_mae: 0.2508 - val_mape: 115.4245\n",
      "Epoch 114/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - loss: 0.1909 - mae: 0.3493 - mape: 84.7779 - val_loss: 0.1516 - val_mae: 0.2498 - val_mape: 115.6575\n",
      "Epoch 115/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - loss: 0.2419 - mae: 0.3876 - mape: 86.5337 - val_loss: 0.1503 - val_mae: 0.2490 - val_mape: 116.0563\n",
      "Epoch 116/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 0.1903 - mae: 0.3524 - mape: 92.4387 - val_loss: 0.1489 - val_mae: 0.2482 - val_mape: 116.4892\n",
      "Epoch 117/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.2161 - mae: 0.3723 - mape: 100.9482 - val_loss: 0.1477 - val_mae: 0.2473 - val_mape: 116.7159\n",
      "Epoch 118/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 0.1468 - mae: 0.2988 - mape: 66.2747 - val_loss: 0.1463 - val_mae: 0.2463 - val_mape: 116.9680\n",
      "Epoch 119/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 0.2596 - mae: 0.3592 - mape: 70.5537 - val_loss: 0.1447 - val_mae: 0.2453 - val_mape: 117.0196\n",
      "Epoch 120/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.1552 - mae: 0.3052 - mape: 66.1946 - val_loss: 0.1432 - val_mae: 0.2446 - val_mape: 117.1895\n",
      "Epoch 121/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.2030 - mae: 0.3673 - mape: 75.1030 - val_loss: 0.1417 - val_mae: 0.2439 - val_mape: 117.3729\n",
      "Epoch 122/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 0.1531 - mae: 0.3142 - mape: 80.8964 - val_loss: 0.1403 - val_mae: 0.2432 - val_mape: 117.6073\n",
      "Epoch 123/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 0.2004 - mae: 0.3567 - mape: 94.7255 - val_loss: 0.1388 - val_mae: 0.2424 - val_mape: 117.9323\n",
      "Epoch 124/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.1443 - mae: 0.2954 - mape: 60.2227 - val_loss: 0.1373 - val_mae: 0.2415 - val_mape: 118.2254\n",
      "Epoch 125/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.1670 - mae: 0.3179 - mape: 71.6309 - val_loss: 0.1357 - val_mae: 0.2407 - val_mape: 118.5939\n",
      "Epoch 126/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 0.1643 - mae: 0.3301 - mape: 84.3769 - val_loss: 0.1342 - val_mae: 0.2398 - val_mape: 119.0785\n",
      "Epoch 127/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.2312 - mae: 0.3784 - mape: 90.5402 - val_loss: 0.1326 - val_mae: 0.2387 - val_mape: 119.4610\n",
      "Epoch 128/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.2199 - mae: 0.3722 - mape: 100.4729 - val_loss: 0.1314 - val_mae: 0.2380 - val_mape: 119.9177\n",
      "Epoch 129/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - loss: 0.1528 - mae: 0.3065 - mape: 70.1413 - val_loss: 0.1299 - val_mae: 0.2369 - val_mape: 120.3335\n",
      "Epoch 130/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.1708 - mae: 0.3410 - mape: 96.4450 - val_loss: 0.1285 - val_mae: 0.2360 - val_mape: 120.8377\n",
      "Epoch 131/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 0.1560 - mae: 0.3184 - mape: 83.0876 - val_loss: 0.1271 - val_mae: 0.2352 - val_mape: 121.3781\n",
      "Epoch 132/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.2895 - mae: 0.4247 - mape: 100.1107 - val_loss: 0.1259 - val_mae: 0.2343 - val_mape: 121.7914\n",
      "Epoch 133/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.2100 - mae: 0.3384 - mape: 80.1903 - val_loss: 0.1249 - val_mae: 0.2337 - val_mape: 122.3471\n",
      "Epoch 134/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.2375 - mae: 0.3704 - mape: 89.3772 - val_loss: 0.1240 - val_mae: 0.2334 - val_mape: 123.0743\n",
      "Epoch 135/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.1597 - mae: 0.3083 - mape: 76.1572 - val_loss: 0.1231 - val_mae: 0.2330 - val_mape: 123.7964\n",
      "Epoch 136/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - loss: 0.2176 - mae: 0.3649 - mape: 111.1939 - val_loss: 0.1225 - val_mae: 0.2332 - val_mape: 124.8119\n",
      "Epoch 137/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - loss: 0.2305 - mae: 0.3678 - mape: 92.5749 - val_loss: 0.1221 - val_mae: 0.2334 - val_mape: 125.5543\n",
      "Epoch 138/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - loss: 0.1446 - mae: 0.3057 - mape: 81.5025 - val_loss: 0.1215 - val_mae: 0.2336 - val_mape: 126.3324\n",
      "Epoch 139/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - loss: 0.1924 - mae: 0.3516 - mape: 87.4352 - val_loss: 0.1211 - val_mae: 0.2338 - val_mape: 127.2303\n",
      "Epoch 140/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.1996 - mae: 0.3404 - mape: 91.6015 - val_loss: 0.1206 - val_mae: 0.2341 - val_mape: 128.0666\n",
      "Epoch 141/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - loss: 0.1670 - mae: 0.3207 - mape: 98.6911 - val_loss: 0.1201 - val_mae: 0.2343 - val_mape: 128.7433\n",
      "Epoch 142/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.2604 - mae: 0.3859 - mape: 91.5949 - val_loss: 0.1195 - val_mae: 0.2344 - val_mape: 129.2312\n",
      "Epoch 143/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - loss: 0.1563 - mae: 0.3146 - mape: 91.5723 - val_loss: 0.1191 - val_mae: 0.2346 - val_mape: 129.7354\n",
      "Epoch 144/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.2215 - mae: 0.3647 - mape: 114.0415 - val_loss: 0.1187 - val_mae: 0.2349 - val_mape: 130.1670\n",
      "Epoch 145/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.1321 - mae: 0.2914 - mape: 78.9520 - val_loss: 0.1181 - val_mae: 0.2348 - val_mape: 130.5456\n",
      "Epoch 146/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.1766 - mae: 0.3226 - mape: 85.2334 - val_loss: 0.1175 - val_mae: 0.2346 - val_mape: 130.8851\n",
      "Epoch 147/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.1563 - mae: 0.3375 - mape: 101.7924 - val_loss: 0.1169 - val_mae: 0.2344 - val_mape: 131.2184\n",
      "Epoch 148/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.1944 - mae: 0.3313 - mape: 94.0279 - val_loss: 0.1163 - val_mae: 0.2346 - val_mape: 131.7754\n",
      "Epoch 149/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.1554 - mae: 0.3020 - mape: 89.9646 - val_loss: 0.1158 - val_mae: 0.2347 - val_mape: 132.1145\n",
      "Epoch 150/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - loss: 0.2399 - mae: 0.3453 - mape: 90.0637 - val_loss: 0.1153 - val_mae: 0.2348 - val_mape: 132.3224\n",
      "Epoch 151/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.2107 - mae: 0.3525 - mape: 92.7201 - val_loss: 0.1149 - val_mae: 0.2349 - val_mape: 132.5712\n",
      "Epoch 152/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.2028 - mae: 0.3482 - mape: 92.3919 - val_loss: 0.1146 - val_mae: 0.2351 - val_mape: 132.8916\n",
      "Epoch 153/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.1389 - mae: 0.3083 - mape: 87.8854 - val_loss: 0.1141 - val_mae: 0.2352 - val_mape: 133.2236\n",
      "Epoch 154/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 0.1255 - mae: 0.2831 - mape: 80.2201 - val_loss: 0.1137 - val_mae: 0.2351 - val_mape: 133.5199\n",
      "Epoch 155/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 0.2257 - mae: 0.3642 - mape: 86.7559 - val_loss: 0.1131 - val_mae: 0.2348 - val_mape: 133.7590\n",
      "Epoch 156/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - loss: 0.1610 - mae: 0.3056 - mape: 81.3218 - val_loss: 0.1124 - val_mae: 0.2343 - val_mape: 133.7523\n",
      "Epoch 157/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - loss: 0.1120 - mae: 0.2810 - mape: 91.8530 - val_loss: 0.1118 - val_mae: 0.2340 - val_mape: 133.9508\n",
      "Epoch 158/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 0.2163 - mae: 0.3456 - mape: 93.9673 - val_loss: 0.1113 - val_mae: 0.2341 - val_mape: 134.1464\n",
      "Epoch 159/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 0.2124 - mae: 0.3328 - mape: 96.5998 - val_loss: 0.1109 - val_mae: 0.2344 - val_mape: 134.3958\n",
      "Epoch 160/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - loss: 0.1698 - mae: 0.3208 - mape: 85.1019 - val_loss: 0.1106 - val_mae: 0.2350 - val_mape: 134.8239\n",
      "Epoch 161/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - loss: 0.1844 - mae: 0.3330 - mape: 84.8643 - val_loss: 0.1101 - val_mae: 0.2352 - val_mape: 135.1761\n",
      "Epoch 162/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - loss: 0.1382 - mae: 0.2960 - mape: 88.9696 - val_loss: 0.1096 - val_mae: 0.2354 - val_mape: 135.3681\n",
      "Epoch 163/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - loss: 0.1961 - mae: 0.3632 - mape: 108.2723 - val_loss: 0.1091 - val_mae: 0.2357 - val_mape: 135.5472\n",
      "Epoch 164/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.1682 - mae: 0.3084 - mape: 83.8848 - val_loss: 0.1085 - val_mae: 0.2357 - val_mape: 135.5805\n",
      "Epoch 165/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.0778 - mae: 0.2204 - mape: 71.7900 - val_loss: 0.1077 - val_mae: 0.2353 - val_mape: 135.5144\n",
      "Epoch 166/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.1506 - mae: 0.2903 - mape: 84.6398 - val_loss: 0.1069 - val_mae: 0.2350 - val_mape: 135.4286\n",
      "Epoch 167/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 0.1312 - mae: 0.2840 - mape: 81.3065 - val_loss: 0.1061 - val_mae: 0.2348 - val_mape: 135.6117\n",
      "Epoch 168/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - loss: 0.1574 - mae: 0.3087 - mape: 89.5150 - val_loss: 0.1053 - val_mae: 0.2346 - val_mape: 135.8173\n",
      "Epoch 169/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.1278 - mae: 0.2660 - mape: 78.6947 - val_loss: 0.1046 - val_mae: 0.2344 - val_mape: 136.0213\n",
      "Epoch 170/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.1883 - mae: 0.3586 - mape: 99.7900 - val_loss: 0.1037 - val_mae: 0.2340 - val_mape: 136.1147\n",
      "Epoch 171/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - loss: 0.2175 - mae: 0.3345 - mape: 100.0094 - val_loss: 0.1029 - val_mae: 0.2338 - val_mape: 136.0069\n",
      "Epoch 172/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - loss: 0.1044 - mae: 0.2555 - mape: 80.3505 - val_loss: 0.1023 - val_mae: 0.2338 - val_mape: 135.9691\n",
      "Epoch 173/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 0.0983 - mae: 0.2485 - mape: 81.6575 - val_loss: 0.1016 - val_mae: 0.2338 - val_mape: 136.1690\n",
      "Epoch 174/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - loss: 0.1573 - mae: 0.2961 - mape: 89.9514 - val_loss: 0.1010 - val_mae: 0.2340 - val_mape: 136.3277\n",
      "Epoch 175/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.2167 - mae: 0.2987 - mape: 56.9713 - val_loss: 0.1004 - val_mae: 0.2340 - val_mape: 136.2719\n",
      "Epoch 176/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - loss: 0.1680 - mae: 0.3300 - mape: 97.8918 - val_loss: 0.0998 - val_mae: 0.2341 - val_mape: 136.2179\n",
      "Epoch 177/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - loss: 0.1474 - mae: 0.2958 - mape: 90.5141 - val_loss: 0.0993 - val_mae: 0.2343 - val_mape: 136.3918\n",
      "Epoch 178/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.1371 - mae: 0.2766 - mape: 88.3307 - val_loss: 0.0988 - val_mae: 0.2344 - val_mape: 136.5027\n",
      "Epoch 179/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - loss: 0.0838 - mae: 0.2291 - mape: 71.4315 - val_loss: 0.0981 - val_mae: 0.2343 - val_mape: 136.5598\n",
      "Epoch 180/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - loss: 0.1999 - mae: 0.3253 - mape: 104.3610 - val_loss: 0.0977 - val_mae: 0.2344 - val_mape: 136.6372\n",
      "Epoch 181/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.1545 - mae: 0.2677 - mape: 89.9829 - val_loss: 0.0972 - val_mae: 0.2344 - val_mape: 136.6875\n",
      "Epoch 182/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.1880 - mae: 0.3242 - mape: 94.7191 - val_loss: 0.0969 - val_mae: 0.2346 - val_mape: 136.9107\n",
      "Epoch 183/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - loss: 0.1551 - mae: 0.3005 - mape: 89.8669 - val_loss: 0.0964 - val_mae: 0.2345 - val_mape: 137.0131\n",
      "Epoch 184/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.1447 - mae: 0.2911 - mape: 92.8330 - val_loss: 0.0960 - val_mae: 0.2345 - val_mape: 137.2274\n",
      "Epoch 185/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - loss: 0.1960 - mae: 0.2933 - mape: 88.3458 - val_loss: 0.0959 - val_mae: 0.2349 - val_mape: 137.7193\n",
      "Epoch 186/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.1580 - mae: 0.2862 - mape: 96.9257 - val_loss: 0.0956 - val_mae: 0.2346 - val_mape: 137.7447\n",
      "Epoch 187/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 0.2049 - mae: 0.3355 - mape: 89.3231 - val_loss: 0.0954 - val_mae: 0.2347 - val_mape: 138.0544\n",
      "Epoch 188/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - loss: 0.0934 - mae: 0.2454 - mape: 84.5058 - val_loss: 0.0950 - val_mae: 0.2346 - val_mape: 138.3314\n",
      "Epoch 189/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - loss: 0.1507 - mae: 0.3040 - mape: 97.5829 - val_loss: 0.0946 - val_mae: 0.2345 - val_mape: 138.6026\n",
      "Epoch 190/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - loss: 0.1160 - mae: 0.2627 - mape: 87.6489 - val_loss: 0.0942 - val_mae: 0.2344 - val_mape: 138.8968\n",
      "Epoch 191/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - loss: 0.1704 - mae: 0.3087 - mape: 87.2964 - val_loss: 0.0939 - val_mae: 0.2344 - val_mape: 139.2364\n",
      "Epoch 192/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.1526 - mae: 0.2884 - mape: 96.8550 - val_loss: 0.0937 - val_mae: 0.2347 - val_mape: 139.6918\n",
      "Epoch 193/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - loss: 0.0886 - mae: 0.2352 - mape: 100.5912 - val_loss: 0.0935 - val_mae: 0.2349 - val_mape: 140.1508\n",
      "Epoch 194/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - loss: 0.1337 - mae: 0.2788 - mape: 108.5320 - val_loss: 0.0934 - val_mae: 0.2353 - val_mape: 140.6580\n",
      "Epoch 195/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - loss: 0.2408 - mae: 0.3466 - mape: 98.9682 - val_loss: 0.0933 - val_mae: 0.2356 - val_mape: 141.0803\n",
      "Epoch 196/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.2362 - mae: 0.3469 - mape: 100.7073 - val_loss: 0.0931 - val_mae: 0.2355 - val_mape: 141.0927\n",
      "Epoch 197/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.1302 - mae: 0.2558 - mape: 70.4658 - val_loss: 0.0929 - val_mae: 0.2355 - val_mape: 141.2739\n",
      "Epoch 198/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - loss: 0.0996 - mae: 0.2427 - mape: 80.8828 - val_loss: 0.0927 - val_mae: 0.2358 - val_mape: 141.6918\n",
      "Epoch 199/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 0.0971 - mae: 0.2316 - mape: 81.0821 - val_loss: 0.0924 - val_mae: 0.2357 - val_mape: 141.8665\n",
      "Epoch 200/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - loss: 0.2458 - mae: 0.3171 - mape: 87.7629 - val_loss: 0.0922 - val_mae: 0.2356 - val_mape: 141.8345\n",
      "Epoch 201/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.0945 - mae: 0.2452 - mape: 87.3722 - val_loss: 0.0919 - val_mae: 0.2354 - val_mape: 141.8231\n",
      "Epoch 202/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - loss: 0.1138 - mae: 0.2595 - mape: 85.9172 - val_loss: 0.0913 - val_mae: 0.2347 - val_mape: 141.5047\n",
      "Epoch 203/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - loss: 0.1694 - mae: 0.2803 - mape: 94.2080 - val_loss: 0.0908 - val_mae: 0.2340 - val_mape: 141.1780\n",
      "Epoch 204/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.1286 - mae: 0.2462 - mape: 81.7793 - val_loss: 0.0904 - val_mae: 0.2336 - val_mape: 141.1275\n",
      "Epoch 205/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - loss: 0.0999 - mae: 0.2476 - mape: 99.8408 - val_loss: 0.0900 - val_mae: 0.2332 - val_mape: 140.8983\n",
      "Epoch 206/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 0.1331 - mae: 0.2810 - mape: 106.1406 - val_loss: 0.0897 - val_mae: 0.2331 - val_mape: 140.8674\n",
      "Epoch 207/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 0.1158 - mae: 0.2653 - mape: 93.3438 - val_loss: 0.0892 - val_mae: 0.2326 - val_mape: 140.5534\n",
      "Epoch 208/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.1358 - mae: 0.2767 - mape: 83.5624 - val_loss: 0.0887 - val_mae: 0.2321 - val_mape: 140.3163\n",
      "Epoch 209/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 0.1635 - mae: 0.2818 - mape: 89.3780 - val_loss: 0.0883 - val_mae: 0.2318 - val_mape: 140.2006\n",
      "Epoch 210/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 0.1202 - mae: 0.2587 - mape: 97.5330 - val_loss: 0.0878 - val_mae: 0.2313 - val_mape: 140.0021\n",
      "Epoch 211/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - loss: 0.0667 - mae: 0.1994 - mape: 77.9628 - val_loss: 0.0873 - val_mae: 0.2306 - val_mape: 139.6331\n",
      "Epoch 212/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.1519 - mae: 0.2760 - mape: 82.9314 - val_loss: 0.0867 - val_mae: 0.2298 - val_mape: 139.1284\n",
      "Epoch 213/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 0.0902 - mae: 0.2521 - mape: 78.9710 - val_loss: 0.0863 - val_mae: 0.2293 - val_mape: 138.8717\n",
      "Epoch 214/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 0.0869 - mae: 0.2116 - mape: 76.6177 - val_loss: 0.0858 - val_mae: 0.2289 - val_mape: 138.6352\n",
      "Epoch 215/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - loss: 0.1461 - mae: 0.2633 - mape: 83.7674 - val_loss: 0.0855 - val_mae: 0.2286 - val_mape: 138.3829\n",
      "Epoch 216/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 0.1072 - mae: 0.2456 - mape: 97.4876 - val_loss: 0.0851 - val_mae: 0.2282 - val_mape: 138.0642\n",
      "Epoch 217/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.1398 - mae: 0.2235 - mape: 73.6063 - val_loss: 0.0849 - val_mae: 0.2281 - val_mape: 138.0621\n",
      "Epoch 218/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - loss: 0.1072 - mae: 0.2357 - mape: 76.8453 - val_loss: 0.0847 - val_mae: 0.2281 - val_mape: 138.0432\n",
      "Epoch 219/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - loss: 0.1781 - mae: 0.2952 - mape: 82.9934 - val_loss: 0.0845 - val_mae: 0.2280 - val_mape: 137.9769\n",
      "Epoch 220/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - loss: 0.0776 - mae: 0.2275 - mape: 86.3827 - val_loss: 0.0842 - val_mae: 0.2278 - val_mape: 137.8327\n",
      "Epoch 221/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.1122 - mae: 0.2315 - mape: 78.5841 - val_loss: 0.0840 - val_mae: 0.2276 - val_mape: 137.8040\n",
      "Epoch 222/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.0914 - mae: 0.2203 - mape: 72.5533 - val_loss: 0.0838 - val_mae: 0.2276 - val_mape: 137.9300\n",
      "Epoch 223/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - loss: 0.0869 - mae: 0.2372 - mape: 82.5088 - val_loss: 0.0834 - val_mae: 0.2273 - val_mape: 137.7996\n",
      "Epoch 224/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 0.1349 - mae: 0.2355 - mape: 71.3409 - val_loss: 0.0829 - val_mae: 0.2267 - val_mape: 137.4897\n",
      "Epoch 225/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - loss: 0.1773 - mae: 0.3071 - mape: 114.9667 - val_loss: 0.0823 - val_mae: 0.2261 - val_mape: 137.1086\n",
      "Epoch 226/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.1965 - mae: 0.3089 - mape: 78.3691 - val_loss: 0.0817 - val_mae: 0.2255 - val_mape: 136.7629\n",
      "Epoch 227/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 0.1372 - mae: 0.2915 - mape: 106.5073 - val_loss: 0.0810 - val_mae: 0.2249 - val_mape: 136.5020\n",
      "Epoch 228/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - loss: 0.1365 - mae: 0.2551 - mape: 84.9948 - val_loss: 0.0805 - val_mae: 0.2246 - val_mape: 136.2679\n",
      "Epoch 229/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.1302 - mae: 0.2603 - mape: 89.4089 - val_loss: 0.0802 - val_mae: 0.2246 - val_mape: 136.3431\n",
      "Epoch 230/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.0862 - mae: 0.2202 - mape: 74.8243 - val_loss: 0.0797 - val_mae: 0.2247 - val_mape: 136.4696\n",
      "Epoch 231/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - loss: 0.1913 - mae: 0.3063 - mape: 113.1491 - val_loss: 0.0791 - val_mae: 0.2242 - val_mape: 136.1310\n",
      "Epoch 232/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - loss: 0.1540 - mae: 0.2721 - mape: 104.0424 - val_loss: 0.0785 - val_mae: 0.2238 - val_mape: 135.7368\n",
      "Epoch 233/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - loss: 0.1798 - mae: 0.3276 - mape: 113.7716 - val_loss: 0.0779 - val_mae: 0.2231 - val_mape: 135.0900\n",
      "Epoch 234/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.0812 - mae: 0.2064 - mape: 83.1462 - val_loss: 0.0774 - val_mae: 0.2225 - val_mape: 134.4500\n",
      "Epoch 235/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - loss: 0.1595 - mae: 0.2635 - mape: 68.7302 - val_loss: 0.0768 - val_mae: 0.2218 - val_mape: 133.7655\n",
      "Epoch 236/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - loss: 0.1395 - mae: 0.2643 - mape: 96.2061 - val_loss: 0.0762 - val_mae: 0.2211 - val_mape: 133.0802\n",
      "Epoch 237/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - loss: 0.1729 - mae: 0.3054 - mape: 105.0069 - val_loss: 0.0755 - val_mae: 0.2200 - val_mape: 132.1754\n",
      "Epoch 238/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 0.0563 - mae: 0.1871 - mape: 84.8187 - val_loss: 0.0747 - val_mae: 0.2186 - val_mape: 131.0787\n",
      "Epoch 239/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.1003 - mae: 0.2325 - mape: 90.7938 - val_loss: 0.0739 - val_mae: 0.2174 - val_mape: 130.0687\n",
      "Epoch 240/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - loss: 0.1447 - mae: 0.2624 - mape: 80.3179 - val_loss: 0.0734 - val_mae: 0.2166 - val_mape: 129.3048\n",
      "Epoch 241/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 0.1715 - mae: 0.2761 - mape: 88.2179 - val_loss: 0.0731 - val_mae: 0.2164 - val_mape: 129.1318\n",
      "Epoch 242/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 0.1327 - mae: 0.2702 - mape: 101.5721 - val_loss: 0.0729 - val_mae: 0.2164 - val_mape: 129.0877\n",
      "Epoch 243/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 0.1020 - mae: 0.2280 - mape: 92.8950 - val_loss: 0.0726 - val_mae: 0.2163 - val_mape: 128.8354\n",
      "Epoch 244/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.1096 - mae: 0.2264 - mape: 67.1022 - val_loss: 0.0722 - val_mae: 0.2157 - val_mape: 128.3910\n",
      "Epoch 245/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - loss: 0.1544 - mae: 0.2464 - mape: 79.9500 - val_loss: 0.0717 - val_mae: 0.2153 - val_mape: 128.0488\n",
      "Epoch 246/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - loss: 0.1369 - mae: 0.2587 - mape: 98.2579 - val_loss: 0.0715 - val_mae: 0.2150 - val_mape: 127.7619\n",
      "Epoch 247/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.0953 - mae: 0.2213 - mape: 90.6465 - val_loss: 0.0712 - val_mae: 0.2149 - val_mape: 127.7598\n",
      "Epoch 248/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.1482 - mae: 0.2875 - mape: 108.0843 - val_loss: 0.0710 - val_mae: 0.2148 - val_mape: 127.5912\n",
      "Epoch 249/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - loss: 0.1200 - mae: 0.2480 - mape: 108.7791 - val_loss: 0.0708 - val_mae: 0.2146 - val_mape: 127.3989\n",
      "Epoch 250/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 0.0895 - mae: 0.2205 - mape: 102.6212 - val_loss: 0.0705 - val_mae: 0.2143 - val_mape: 127.0265\n",
      "Epoch 251/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.0581 - mae: 0.1604 - mape: 56.6400 - val_loss: 0.0702 - val_mae: 0.2138 - val_mape: 126.5720\n",
      "Epoch 252/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.0616 - mae: 0.1901 - mape: 72.5898 - val_loss: 0.0699 - val_mae: 0.2135 - val_mape: 126.4520\n",
      "Epoch 253/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.1283 - mae: 0.2507 - mape: 99.6891 - val_loss: 0.0697 - val_mae: 0.2133 - val_mape: 126.3306\n",
      "Epoch 254/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - loss: 0.0925 - mae: 0.2158 - mape: 80.6814 - val_loss: 0.0695 - val_mae: 0.2132 - val_mape: 126.1985\n",
      "Epoch 255/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - loss: 0.1150 - mae: 0.2219 - mape: 68.5340 - val_loss: 0.0692 - val_mae: 0.2129 - val_mape: 126.1287\n",
      "Epoch 256/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 0.1395 - mae: 0.2382 - mape: 88.1807 - val_loss: 0.0688 - val_mae: 0.2127 - val_mape: 126.0076\n",
      "Epoch 257/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.0607 - mae: 0.1984 - mape: 79.9694 - val_loss: 0.0684 - val_mae: 0.2123 - val_mape: 125.8054\n",
      "Epoch 258/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.1423 - mae: 0.2629 - mape: 85.7645 - val_loss: 0.0680 - val_mae: 0.2118 - val_mape: 125.5699\n",
      "Epoch 259/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - loss: 0.1353 - mae: 0.2688 - mape: 99.2023 - val_loss: 0.0677 - val_mae: 0.2116 - val_mape: 125.4433\n",
      "Epoch 260/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 0.1553 - mae: 0.2919 - mape: 82.9200 - val_loss: 0.0673 - val_mae: 0.2112 - val_mape: 125.2345\n",
      "Epoch 261/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.1107 - mae: 0.2301 - mape: 68.4797 - val_loss: 0.0667 - val_mae: 0.2103 - val_mape: 124.7887\n",
      "Epoch 262/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 0.0879 - mae: 0.2186 - mape: 73.2489 - val_loss: 0.0661 - val_mae: 0.2094 - val_mape: 124.2944\n",
      "Epoch 263/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - loss: 0.2182 - mae: 0.3068 - mape: 97.1802 - val_loss: 0.0656 - val_mae: 0.2085 - val_mape: 123.7830\n",
      "Epoch 264/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.0739 - mae: 0.1906 - mape: 61.2615 - val_loss: 0.0651 - val_mae: 0.2077 - val_mape: 123.2844\n",
      "Epoch 265/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - loss: 0.1563 - mae: 0.2800 - mape: 86.9017 - val_loss: 0.0647 - val_mae: 0.2069 - val_mape: 122.8288\n",
      "Epoch 266/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - loss: 0.0905 - mae: 0.2157 - mape: 83.7595 - val_loss: 0.0643 - val_mae: 0.2063 - val_mape: 122.4038\n",
      "Epoch 267/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 0.1704 - mae: 0.2824 - mape: 91.2181 - val_loss: 0.0640 - val_mae: 0.2059 - val_mape: 122.0558\n",
      "Epoch 268/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: 0.1221 - mae: 0.2158 - mape: 71.7139 - val_loss: 0.0639 - val_mae: 0.2056 - val_mape: 121.7189\n",
      "Epoch 269/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - loss: 0.0945 - mae: 0.2195 - mape: 80.2530 - val_loss: 0.0636 - val_mae: 0.2051 - val_mape: 121.3511\n",
      "Epoch 270/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 0.1612 - mae: 0.2572 - mape: 86.1038 - val_loss: 0.0634 - val_mae: 0.2046 - val_mape: 120.6927\n",
      "Epoch 271/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.1180 - mae: 0.2504 - mape: 87.0151 - val_loss: 0.0629 - val_mae: 0.2031 - val_mape: 119.3778\n",
      "Epoch 272/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - loss: 0.0772 - mae: 0.1856 - mape: 66.9440 - val_loss: 0.0623 - val_mae: 0.2014 - val_mape: 118.1093\n",
      "Epoch 273/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - loss: 0.0508 - mae: 0.1656 - mape: 69.9823 - val_loss: 0.0617 - val_mae: 0.1999 - val_mape: 117.0166\n",
      "Epoch 274/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.1218 - mae: 0.2404 - mape: 88.3678 - val_loss: 0.0614 - val_mae: 0.1989 - val_mape: 116.1758\n",
      "Epoch 275/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 0.0844 - mae: 0.2002 - mape: 76.6323 - val_loss: 0.0611 - val_mae: 0.1982 - val_mape: 115.5796\n",
      "Epoch 276/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.1122 - mae: 0.2261 - mape: 82.8798 - val_loss: 0.0610 - val_mae: 0.1978 - val_mape: 115.1693\n",
      "Epoch 277/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - loss: 0.1276 - mae: 0.2262 - mape: 80.9055 - val_loss: 0.0608 - val_mae: 0.1977 - val_mape: 114.8889\n",
      "Epoch 278/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - loss: 0.0982 - mae: 0.2097 - mape: 66.1991 - val_loss: 0.0608 - val_mae: 0.1976 - val_mape: 114.7543\n",
      "Epoch 279/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.1071 - mae: 0.2007 - mape: 72.7677 - val_loss: 0.0606 - val_mae: 0.1975 - val_mape: 114.5964\n",
      "Epoch 280/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - loss: 0.1013 - mae: 0.2049 - mape: 73.0370 - val_loss: 0.0603 - val_mae: 0.1969 - val_mape: 114.1437\n",
      "Epoch 281/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.1760 - mae: 0.2557 - mape: 82.5261 - val_loss: 0.0601 - val_mae: 0.1970 - val_mape: 114.2646\n",
      "Epoch 282/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - loss: 0.0674 - mae: 0.1959 - mape: 75.1743 - val_loss: 0.0599 - val_mae: 0.1970 - val_mape: 114.5833\n",
      "Epoch 283/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - loss: 0.1119 - mae: 0.2327 - mape: 74.5323 - val_loss: 0.0597 - val_mae: 0.1972 - val_mape: 115.0587\n",
      "Epoch 284/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 0.1552 - mae: 0.2887 - mape: 80.0291 - val_loss: 0.0594 - val_mae: 0.1974 - val_mape: 115.5184\n",
      "Epoch 285/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 0.1268 - mae: 0.2191 - mape: 74.1242 - val_loss: 0.0590 - val_mae: 0.1972 - val_mape: 115.6743\n",
      "Epoch 286/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.2026 - mae: 0.3034 - mape: 94.3727 - val_loss: 0.0590 - val_mae: 0.1978 - val_mape: 116.3482\n",
      "Epoch 287/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 0.1345 - mae: 0.2178 - mape: 88.1861 - val_loss: 0.0590 - val_mae: 0.1984 - val_mape: 116.9109\n",
      "Epoch 288/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - loss: 0.1444 - mae: 0.2561 - mape: 102.8198 - val_loss: 0.0591 - val_mae: 0.1993 - val_mape: 117.6203\n",
      "Epoch 289/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 0.1554 - mae: 0.2663 - mape: 83.0940 - val_loss: 0.0592 - val_mae: 0.2001 - val_mape: 118.2279\n",
      "Epoch 290/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - loss: 0.0602 - mae: 0.1693 - mape: 77.5348 - val_loss: 0.0592 - val_mae: 0.2006 - val_mape: 118.6246\n",
      "Epoch 291/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - loss: 0.1262 - mae: 0.2151 - mape: 70.8518 - val_loss: 0.0592 - val_mae: 0.2010 - val_mape: 118.9202\n",
      "Epoch 292/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - loss: 0.1548 - mae: 0.2410 - mape: 88.1348 - val_loss: 0.0593 - val_mae: 0.2016 - val_mape: 119.4256\n",
      "Epoch 293/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 0.1121 - mae: 0.2080 - mape: 77.8322 - val_loss: 0.0594 - val_mae: 0.2022 - val_mape: 119.7858\n",
      "Epoch 294/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - loss: 0.0635 - mae: 0.1676 - mape: 70.6055 - val_loss: 0.0597 - val_mae: 0.2031 - val_mape: 120.4728\n",
      "Epoch 295/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - loss: 0.1643 - mae: 0.2716 - mape: 75.8131 - val_loss: 0.0600 - val_mae: 0.2041 - val_mape: 121.1463\n",
      "Epoch 296/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 0.0970 - mae: 0.2313 - mape: 89.3963 - val_loss: 0.0603 - val_mae: 0.2052 - val_mape: 121.8876\n",
      "Epoch 297/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - loss: 0.2074 - mae: 0.3028 - mape: 90.4888 - val_loss: 0.0606 - val_mae: 0.2060 - val_mape: 122.2533\n",
      "Epoch 298/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 0.1310 - mae: 0.2546 - mape: 98.4205 - val_loss: 0.0608 - val_mae: 0.2065 - val_mape: 122.4161\n",
      "Epoch 299/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - loss: 0.1387 - mae: 0.2350 - mape: 78.3100 - val_loss: 0.0611 - val_mae: 0.2075 - val_mape: 122.8882\n",
      "Epoch 300/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.0752 - mae: 0.1772 - mape: 59.7490 - val_loss: 0.0613 - val_mae: 0.2080 - val_mape: 123.0066\n",
      "Epoch 301/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - loss: 0.0982 - mae: 0.2161 - mape: 75.9148 - val_loss: 0.0614 - val_mae: 0.2085 - val_mape: 123.1624\n",
      "Epoch 302/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.1199 - mae: 0.2410 - mape: 84.9762 - val_loss: 0.0615 - val_mae: 0.2087 - val_mape: 123.1484\n",
      "Epoch 303/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 0.2300 - mae: 0.3359 - mape: 96.0552 - val_loss: 0.0615 - val_mae: 0.2089 - val_mape: 122.9085\n",
      "Epoch 304/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 0.0485 - mae: 0.1569 - mape: 69.6720 - val_loss: 0.0615 - val_mae: 0.2092 - val_mape: 122.7575\n",
      "Epoch 305/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - loss: 0.1868 - mae: 0.2725 - mape: 76.2679 - val_loss: 0.0615 - val_mae: 0.2091 - val_mape: 122.1919\n",
      "Epoch 306/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - loss: 0.0986 - mae: 0.1952 - mape: 77.9683 - val_loss: 0.0612 - val_mae: 0.2086 - val_mape: 121.4486\n",
      "Epoch 307/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 0.1508 - mae: 0.2504 - mape: 75.3233 - val_loss: 0.0610 - val_mae: 0.2083 - val_mape: 120.8691\n",
      "Epoch 308/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.1013 - mae: 0.2039 - mape: 80.7251 - val_loss: 0.0611 - val_mae: 0.2084 - val_mape: 120.5211\n",
      "Epoch 309/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 0.1196 - mae: 0.2023 - mape: 72.1950 - val_loss: 0.0611 - val_mae: 0.2084 - val_mape: 120.0980\n",
      "Epoch 310/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 0.0705 - mae: 0.1749 - mape: 58.9008 - val_loss: 0.0608 - val_mae: 0.2078 - val_mape: 119.5741\n",
      "Epoch 311/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.0653 - mae: 0.1884 - mape: 79.9548 - val_loss: 0.0607 - val_mae: 0.2080 - val_mape: 119.6101\n",
      "Epoch 312/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.1498 - mae: 0.2385 - mape: 72.6069 - val_loss: 0.0608 - val_mae: 0.2083 - val_mape: 119.6367\n"
     ]
    }
   ],
   "source": [
    "#model training\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Compile model with optimizer and loss function\n",
    "model.compile(optimizer='adam',    # Adaptive learning rate optimizer\n",
    "              loss='mse',          # Mean Squared Error for regression\n",
    "              metrics=['mae', 'mape'])  # Track Mean Absolute Error and Mean Absolute Percentage Error\n",
    "\n",
    "# Early stopping to prevent overfitting\n",
    "# Stops training when validation loss doesn't improve for 25 epochs\n",
    "# Restores the best weights found during training\n",
    "early = EarlyStopping(monitor='val_loss',        # Watch validation loss\n",
    "                     patience=25,                # Wait 25 epochs without improvement\n",
    "                     restore_best_weights=True)  # Keep best model weights\n",
    "\n",
    "# Train the model\n",
    "# Uses scaled features (X_train_z) and scaled targets (y_train_s) in [-1,1] range\n",
    "# Validates on separate validation set to monitor overfitting\n",
    "history = model.fit(\n",
    "    X_train_z, y_train_s,                    # Training data (scaled)\n",
    "    validation_data=(X_val_z, y_val_s),     # Validation data (scaled)\n",
    "    epochs=500,                              # Maximum 500 epochs\n",
    "    batch_size=16,                           # Process 16 samples at a time\n",
    "    verbose=1,                               # Print progress during training\n",
    "    callbacks=[early]                        # Apply early stopping\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HLV4VDdDjLmG",
    "outputId": "e06804a3-2b4d-4bc0-ad9d-206f2ab4cdb8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 7 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001D3C805F380> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "WARNING:tensorflow:6 out of the last 8 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001D3C805F380> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Prediction shapes:\n",
      "Train: (16, 2)\n",
      "Val:   (4, 2)\n",
      "Test:  (5, 2)\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on all sets\n",
    "y_hat_train_s = model.predict(X_train_z)\n",
    "y_hat_val_s   = model.predict(X_val_z)\n",
    "y_hat_test_s  = model.predict(X_test_z)\n",
    "\n",
    "# Convert predictions back to original scale\n",
    "y_hat_train = inv_y(y_hat_train_s)\n",
    "y_hat_val   = inv_y(y_hat_val_s)\n",
    "y_hat_test  = inv_y(y_hat_test_s)\n",
    "\n",
    "# Print prediction shapes\n",
    "print(\"Prediction shapes:\")\n",
    "print(f\"Train: {y_hat_train.shape}\")\n",
    "print(f\"Val:   {y_hat_val.shape}\")\n",
    "print(f\"Test:  {y_hat_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "xeWwgdE0jLkG"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error\n",
    "\n",
    "def report_2out(name, y_true, y_pred, labels=(\"Mn\", \"Mw\")):\n",
    "    for i, label in enumerate(labels):\n",
    "        # Extract single output column\n",
    "        yt = y_true[:, i]\n",
    "        yp = y_pred[:, i]\n",
    "        \n",
    "        # Calculate metrics\n",
    "        mae  = mean_absolute_error(yt, yp)\n",
    "        mse  = mean_squared_error(yt, yp)\n",
    "        r2   = r2_score(yt, yp)\n",
    "        mape = mean_absolute_percentage_error(yt, yp)\n",
    "        \n",
    "        # Print results\n",
    "        print(f\"[{name}] {label}: MAE={mae:.6f}  MSE={mse:.6f}  R²={r2:.4f}  MAPE={mape:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ptHevHX-jLh_",
    "outputId": "ca2a17b0-6b4b-4e41-8e90-50bde0ab8b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Mn: MAE=322.452485  MSE=180046.949402  R²=0.8214  MAPE=0.157742\n",
      "[Train] Mw: MAE=427.037841  MSE=375449.683604  R²=0.8239  MAPE=0.144062\n",
      "[Val  ] Mn: MAE=328.923459  MSE=204798.662326  R²=0.7610  MAPE=0.092835\n",
      "[Val  ] Mw: MAE=496.710432  MSE=293541.847717  R²=0.7925  MAPE=0.127566\n",
      "[Test ] Mn: MAE=419.977333  MSE=314200.674946  R²=-7.8758  MAPE=0.193456\n",
      "[Test ] Mw: MAE=482.502738  MSE=589466.148496  R²=-45.9118  MAPE=0.162727\n"
     ]
    }
   ],
   "source": [
    "#print out\n",
    "report_2out(\"Train\", y_train, y_hat_train)\n",
    "report_2out(\"Val  \", y_val,   y_hat_val)\n",
    "report_2out(\"Test \", y_test,  y_hat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fGS2LHAAjLfx",
    "outputId": "c5ef78f4-c870-4658-9a4c-388ffdd530b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\users\\brian\\pycharmprojects\\pythonproject1\\.venv\\lib\\site-packages (3.10.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\brian\\pycharmprojects\\pythonproject1\\.venv\\lib\\site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\brian\\pycharmprojects\\pythonproject1\\.venv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\brian\\pycharmprojects\\pythonproject1\\.venv\\lib\\site-packages (from matplotlib) (4.59.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\brian\\pycharmprojects\\pythonproject1\\.venv\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\brian\\pycharmprojects\\pythonproject1\\.venv\\lib\\site-packages (from matplotlib) (2.3.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\brian\\pycharmprojects\\pythonproject1\\.venv\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\brian\\pycharmprojects\\pythonproject1\\.venv\\lib\\site-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\brian\\pycharmprojects\\pythonproject1\\.venv\\lib\\site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\brian\\pycharmprojects\\pythonproject1\\.venv\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\brian\\pycharmprojects\\pythonproject1\\.venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZovIuGAXjLd4"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g-ZWVSsjjLby",
    "outputId": "224eda97-b2d7-4034-eb93-9c2c243abade"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split target   actual   predicted     residual   abs_error  pct_error\n",
      "Train     Mn 4663.040 3809.883057   853.156943  853.156943  18.296153\n",
      "Train     Mn 1024.970 1560.011108  -535.041108  535.041108  52.200660\n",
      "Train     Mn 2223.170 2365.288086  -142.118086  142.118086   6.392587\n",
      "Train     Mn 1265.880 1505.385498  -239.505498  239.505498  18.920079\n",
      "Train     Mn 2298.620 3433.648682 -1135.028682 1135.028682  49.378700\n",
      "Train     Mn 1264.440 1505.385498  -240.945498  240.945498  19.055511\n",
      "Train     Mn 3764.530 3515.057861   249.472139  249.472139   6.626913\n",
      "Train     Mn 2951.900 2726.686279   225.213721  225.213721   7.629450\n",
      "Train     Mn 1127.190 1420.009399  -292.819399  292.819399  25.977821\n",
      "Train     Mn 2525.270 2321.235352   204.034648  204.034648   8.079716\n",
      "Train     Mn 1846.180 1998.319214  -152.139214  152.139214   8.240757\n",
      "Train     Mn 2074.517 2208.165283  -133.648283  133.648283   6.442381\n",
      "Train     Mn 2955.830 2726.686279   229.143721  229.143721   7.752263\n",
      "Train     Mn 2525.910 2321.235352   204.674648  204.674648   8.103006\n",
      "Train     Mn 2752.800 2678.323975    74.476025   74.476025   2.705464\n",
      "Train     Mn 3762.880 3515.057861   247.822139  247.822139   6.585970\n",
      "Train     Mw 5921.490 5179.989746   741.500254  741.500254  12.522190\n",
      "Train     Mw 1339.350 1726.252930  -386.902930  386.902930  28.887365\n",
      "Train     Mw 2989.000 3031.922852   -42.922852   42.922852   1.436027\n",
      "Train     Mw 1458.130 1711.384399  -253.254399  253.254399  17.368438\n",
      "Train     Mw 2972.980 4602.679688 -1629.699687 1629.699687  54.817042\n",
      "Train     Mw 1456.220 1711.384399  -255.164399  255.164399  17.522380\n",
      "Train     Mw 5752.150 4750.120117  1002.029883 1002.029883  17.420093\n",
      "Train     Mw 2965.540 2992.715820   -27.175820   27.175820   0.916387\n",
      "Train     Mw 1321.650 1699.776489  -378.126489  378.126489  28.610183\n",
      "Train     Mw 3441.190 3030.746338   410.443662  410.443662  11.927376\n",
      "Train     Mw 2689.910 2653.578369    36.331631   36.331631   1.350663\n",
      "Train     Mw 2970.820 2891.140625    79.679375   79.679375   2.682067\n",
      "Train     Mw 2966.910 2992.715820   -25.805820   25.805820   0.869788\n",
      "Train     Mw 3440.430 3030.746338   409.683662  409.683662  11.907920\n",
      "Train     Mw 3129.610 2978.415283   151.194717  151.194717   4.831104\n",
      "Train     Mw 5752.810 4750.120117  1002.689883 1002.689883  17.429567\n",
      "  Val     Mn 4663.770 3809.883057   853.886943  853.886943  18.308942\n",
      "  Val     Mn 2752.840 2678.323975    74.516025   74.516025   2.706878\n",
      "  Val     Mn 2322.860 2060.214600   262.645400  262.645400  11.306984\n",
      "  Val     Mn 2590.790 2466.144531   124.645469  124.645469   4.811099\n",
      "  Val     Mw 5921.610 5179.989746   741.620254  741.620254  12.523963\n",
      "  Val     Mw 3129.570 2978.415283   151.154717  151.154717   4.829888\n",
      "  Val     Mw 2987.280 2477.849854   509.430146  509.430146  17.053311\n",
      "  Val     Mw 3517.860 2933.223389   584.636611  584.636611  16.619098\n",
      " Test     Mn 2073.900 2208.165283  -134.265283  134.265283   6.474048\n",
      " Test     Mn 1845.600 1998.319214  -152.719214  152.719214   8.274773\n",
      " Test     Mn 2322.830 2060.214600   262.615400  262.615400  11.305838\n",
      " Test     Mn 2298.650 3433.648682 -1134.998682 1134.998682  49.376751\n",
      " Test     Mn 1950.000 2365.288086  -415.288086  415.288086  21.296825\n",
      " Test     Mw 2974.510 2891.140625    83.369375   83.369375   2.802794\n",
      " Test     Mw 2690.500 2653.578369    36.921631   36.921631   1.372296\n",
      " Test     Mw 2987.310 2477.849854   509.460146  509.460146  17.054144\n",
      " Test     Mw 2972.940 4602.679688 -1629.739687 1629.739687  54.819125\n",
      " Test     Mw 2878.900 3031.922852  -153.022852  153.022852   5.315324\n"
     ]
    }
   ],
   "source": [
    "# Create detailed comparison table showing actual vs predicted values\n",
    "# Shows sample-by-sample performance for both outputs (Mn, Mw) across all datasets\n",
    "# Includes error metrics: residual, absolute error, and percentage error\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def make_table_multi(split, y_true, y_pred, target_names=(\"Mn\",\"Mw\"), n=None):\n",
    "    \"\"\"Create comparison table for actual vs predicted values\"\"\"\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    \n",
    "    parts = []\n",
    "    for j, name in enumerate(target_names):\n",
    "        df = pd.DataFrame({\n",
    "            \"split\":     split,\n",
    "            \"target\":    name,\n",
    "            \"actual\":    y_true[:, j],\n",
    "            \"predicted\": y_pred[:, j],\n",
    "        })\n",
    "        # Calculate error metrics\n",
    "        df[\"residual\"]  = df[\"actual\"] - df[\"predicted\"]\n",
    "        df[\"abs_error\"] = df[\"residual\"].abs()\n",
    "        df[\"pct_error\"] = 100 * df[\"abs_error\"] / df[\"actual\"].abs().clip(lower=1e-8)\n",
    "        \n",
    "        parts.append(df if n is None else df.head(n))\n",
    "    \n",
    "    return pd.concat(parts, ignore_index=True)\n",
    "\n",
    "# Create comparison tables for all datasets\n",
    "tbl_train = make_table_multi(\"Train\", y_train, y_hat_train)\n",
    "tbl_val   = make_table_multi(\"Val\",   y_val,   y_hat_val)\n",
    "tbl_test  = make_table_multi(\"Test\",  y_test,  y_hat_test)\n",
    "\n",
    "# Combine all tables\n",
    "tbl_all = pd.concat([tbl_train, tbl_val, tbl_test], ignore_index=True)\n",
    "print(tbl_all.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "Q9ZIfV65BSWm",
    "outputId": "6998d8e4-f6a1-4037-c092-cbc25bf21aaf"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "transfer_functions = {\n",
    "    'relu': 'ReLU',\n",
    "    'tanh': 'Tanh', \n",
    "    'sigmoid': 'Sigmoid',\n",
    "    'softplus': 'Softplus'\n",
    "}\n",
    "\n",
    "mse_results = []\n",
    "\n",
    "for activation, name in transfer_functions.items():\n",
    "    # Clear session and reset seeds\n",
    "    tf.keras.backend.clear_session()\n",
    "    np.random.seed(SEED)\n",
    "    tf.random.set_seed(SEED)\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(X_train_z.shape[1],)))  # Use scaled data\n",
    "    model.add(Dense(16, activation=activation))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(2, activation='tanh'))\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    \n",
    "    # Use scaled data and proper batch size/epochs\n",
    "    model.fit(X_train_z, y_train_s,             # Scaled data\n",
    "              validation_data=(X_val_z, y_val_s), # Add validation\n",
    "              batch_size=16, epochs=200, verbose=0)  # Better parameters\n",
    "    \n",
    "    # Predict and convert back to real units\n",
    "    y_pred_s = model.predict(X_test_z, verbose=0)\n",
    "    y_pred = inv_y(y_pred_s)  # Convert to real units\n",
    "    \n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mse_results.append(mse)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(list(transfer_functions.values()), mse_results, marker='o', linestyle='-')\n",
    "plt.title('Mean MSE (Mn & Mw) vs. Transfer Function')\n",
    "plt.xlabel('Transfer Function')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ky2flHEEBSUj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mEyyme3-BSSL"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KhVOphxKBSPj"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyM6viJ1X4oRLwr65SDqw9Ni",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
