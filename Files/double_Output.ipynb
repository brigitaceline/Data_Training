{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/elhamshaerirahbar-bot/my-colab-project/blob/main/Untitled13.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I8S8VCECinI2",
    "outputId": "442f5205-7a69-49da-a8a5-1f42a4fc8e16"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\brian\\pycharmprojects\\pythonproject1\\.venv\\lib\\site-packages (2.3.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\brian\\pycharmprojects\\pythonproject1\\.venv\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\brian\\pycharmprojects\\pythonproject1\\.venv\\lib\\site-packages (2.20.0rc0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\brian\\pycharmprojects\\pythonproject1\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\brian\\pycharmprojects\\pythonproject1\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\brian\\pycharmprojects\\pythonproject1\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\brian\\pycharmprojects\\pythonproject1\\.venv\\lib\\site-packages (from tensorflow) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\brian\\pycharmprojects\\pythonproject1\\.venv\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\brian\\pycharmprojects\\pythonproject1\\.venv\\lib\\site-packages (from tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\brian\\pycharmprojects\\pythonproject1\\.venv\\lib\\site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google_pasta>=0.1.1 in c:\\users\\brian\\pycharmprojects\\pythonproject1\\.venv\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\brian\\pycharmprojects\\pythonproject1\\.venv\\lib\\site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt_einsum>=2.3.2 in c:\\users\\brian\\pycharmprojects\\pythonproject1\\.venv\\lib\\site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\brian\\pycharmprojects\\pythonproject1\\.venv\\lib\\site-packages (from tensorflow) (25.0)\n",
      "Requirement already satisfied: protobuf>=5.28.0 in c:\\users\\brian\\pycharmprojects\\pythonproject1\\.venv\\lib\\site-packages (from tensorflow) (6.31.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\brian\\pycharmprojects\\pythonproject1\\.venv\\lib\\site-packages (from tensorflow) (2.32.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\brian\\pycharmprojects\\pythonproject1\\.venv\\lib\\site-packages (from tensorflow) (80.9.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\brian\\pycharmprojects\\pythonproject1\\.venv\\lib\\site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\brian\\pycharmprojects\\pythonproject1\\.venv\\lib\\site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in c:\\users\\brian\\pycharmprojects\\pythonproject1\\.venv\\lib\\site-packages (from tensorflow) (4.14.1)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\brian\\pycharmprojects\\pythonproject1\\.venv\\lib\\site-packages (from tensorflow) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\brian\\pycharmprojects\\pythonproject1\\.venv\\lib\\site-packages (from tensorflow) (1.74.0)\n",
      "Requirement already satisfied: tensorboard~=2.20.0 in c:\\users\\brian\\pycharmprojects\\pythonproject1\\.venv\\lib\\site-packages (from tensorflow) (2.20.0)\n",
      "Requirement already satisfied: keras>=3.10.0 in c:\\users\\brian\\pycharmprojects\\pythonproject1\\.venv\\lib\\site-packages (from tensorflow) (3.11.1)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\brian\\pycharmprojects\\pythonproject1\\.venv\\lib\\site-packages (from tensorflow) (3.14.0)\n",
      "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in c:\\users\\brian\\pycharmprojects\\pythonproject1\\.venv\\lib\\site-packages (from tensorflow) (0.5.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\brian\\pycharmprojects\\pythonproject1\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\brian\\pycharmprojects\\pythonproject1\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\brian\\pycharmprojects\\pythonproject1\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\brian\\pycharmprojects\\pythonproject1\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2025.8.3)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\brian\\pycharmprojects\\pythonproject1\\.venv\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.8.2)\n",
      "Requirement already satisfied: pillow in c:\\users\\brian\\pycharmprojects\\pythonproject1\\.venv\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (11.3.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\brian\\pycharmprojects\\pythonproject1\\.venv\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\brian\\pycharmprojects\\pythonproject1\\.venv\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\brian\\pycharmprojects\\pythonproject1\\.venv\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: rich in c:\\users\\brian\\pycharmprojects\\pythonproject1\\.venv\\lib\\site-packages (from keras>=3.10.0->tensorflow) (14.1.0)\n",
      "Requirement already satisfied: namex in c:\\users\\brian\\pycharmprojects\\pythonproject1\\.venv\\lib\\site-packages (from keras>=3.10.0->tensorflow) (0.1.0)\n",
      "Requirement already satisfied: optree in c:\\users\\brian\\pycharmprojects\\pythonproject1\\.venv\\lib\\site-packages (from keras>=3.10.0->tensorflow) (0.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\brian\\pycharmprojects\\pythonproject1\\.venv\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\brian\\pycharmprojects\\pythonproject1\\.venv\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\brian\\pycharmprojects\\pythonproject1\\.venv\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\brian\\pycharmprojects\\pythonproject1\\.venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy pandas tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "0Syqe_mYiwTx"
   },
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd, re, unicodedata\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, callbacks\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.metrics import mean_absolute_percentage_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "id": "mQM3DjQNi0he",
    "outputId": "b018038a-6aec-4a81-e135-9ae3d186ce03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing: []\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Factor A</th>\n",
       "      <th>Factor B</th>\n",
       "      <th>Factor C</th>\n",
       "      <th>Factor D</th>\n",
       "      <th>Response 2 (Experimental)</th>\n",
       "      <th>Response 1 (Experimental)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>110</td>\n",
       "      <td>7</td>\n",
       "      <td>50</td>\n",
       "      <td>10</td>\n",
       "      <td>1321.65</td>\n",
       "      <td>1127.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>85</td>\n",
       "      <td>13</td>\n",
       "      <td>50</td>\n",
       "      <td>10</td>\n",
       "      <td>1339.35</td>\n",
       "      <td>1024.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>101</td>\n",
       "      <td>1</td>\n",
       "      <td>500</td>\n",
       "      <td>60</td>\n",
       "      <td>2878.90</td>\n",
       "      <td>1950.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>101</td>\n",
       "      <td>1</td>\n",
       "      <td>500</td>\n",
       "      <td>60</td>\n",
       "      <td>2989.00</td>\n",
       "      <td>2223.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50</td>\n",
       "      <td>10</td>\n",
       "      <td>50</td>\n",
       "      <td>10</td>\n",
       "      <td>2690.50</td>\n",
       "      <td>1845.60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Factor A  Factor B  Factor C  Factor D  Response 2 (Experimental)  \\\n",
       "0       110         7        50        10                    1321.65   \n",
       "1        85        13        50        10                    1339.35   \n",
       "2       101         1       500        60                    2878.90   \n",
       "3       101         1       500        60                    2989.00   \n",
       "4        50        10        50        10                    2690.50   \n",
       "\n",
       "   Response 1 (Experimental)  \n",
       "0                    1127.19  \n",
       "1                    1024.97  \n",
       "2                    1950.00  \n",
       "3                    2223.17  \n",
       "4                    1845.60  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd, re, unicodedata\n",
    "from difflib import get_close_matches\n",
    "\n",
    "WANTED = ['factor A','factor B','factor C','factor D','Response 2 (Mw)', 'Response 1 (Mn)']\n",
    "\n",
    "def norm(s: str) -> str:\n",
    "    s = unicodedata.normalize(\"NFKC\", str(s))\n",
    "    s = s.replace(\"\\ufeff\",\"\").replace(\"\\u00a0\",\" \")\n",
    "    s = re.sub(r\"\\s+\",\" \", s).strip()\n",
    "    return s.lower()\n",
    "\n",
    "df = pd.read_csv('Exp_Mn_Mw_Value.txt', sep='\\t')\n",
    "\n",
    "\n",
    "name_map = {norm(c): c for c in df.columns}\n",
    "\n",
    "def map_col(wanted: str):\n",
    "    k = norm(wanted)\n",
    "    if k in name_map:\n",
    "        return name_map[k]\n",
    "\n",
    "    cand = get_close_matches(k, list(name_map.keys()), n=1, cutoff=0.7)\n",
    "    return name_map[cand[0]] if cand else None\n",
    "\n",
    "mapped = [(w, map_col(w)) for w in WANTED]\n",
    "missing = [w for w,c in mapped if c is None]\n",
    "cols    = [c for _,c in mapped if c is not None]\n",
    "\n",
    "print(f\"Missing: {missing}\")\n",
    "\n",
    "n = 5\n",
    "view = df[cols].head(n)\n",
    "try:\n",
    "    display(view)\n",
    "except NameError:\n",
    "    print(view.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7Y9BwNofi4-T",
    "outputId": "92a5901e-bb2a-48a0-bd86-613b27ddaa22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (25, 4)  y: (25, 2)\n"
     ]
    }
   ],
   "source": [
    "# ---- pick columns via your map_col ----\n",
    "FEATURES = [map_col('factor A'), map_col('factor B'),\n",
    "            map_col('factor C'), map_col('factor D')]\n",
    "\n",
    "TARGETS  = [map_col('Response 1 (Mn)'), map_col('Response 2 (Mw)')]  # دو خروجی\n",
    "\n",
    "X = df[FEATURES].astype(float).to_numpy()   # shape: (n, 4)\n",
    "y = df[TARGETS].astype(float).to_numpy()    # shape: (n, 2)\n",
    "\n",
    "print(\"X:\", X.shape, \" y:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e2AA1C6Wi77R",
    "outputId": "187af667-98c6-49ba-857a-f7dab735a70b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 4) (6, 4) (9, 4)\n",
      "(10, 2) (6, 2) (9, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# X: (n, 4), y: (n, 2)  -> z.B. y = df[['Response 1 (Mn)','Response 2 (Mw)']].to_numpy()\n",
    "SEED = 55\n",
    "\n",
    "# 1) 20% Test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.35, shuffle=True, random_state=SEED\n",
    ")\n",
    "# 2) 20% von Train für Val  -> insgesamt: Train 64%, Val 16%, Test 20%\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.35, shuffle=True, random_state=SEED\n",
    ")\n",
    "\n",
    "print(X_train.shape, X_val.shape, X_test.shape)  # Kontrolle\n",
    "print(y_train.shape, y_val.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "WAeiXioBjGXg"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "# X: (n,4), y: (n,2)  -> z.B. y[:,0]=Mn, y[:,1]=Mw\n",
    "assert y_train.ndim == 2 and y_train.shape[1] == 2, \"y muss (n,2) sein\"\n",
    "\n",
    "# --- X skalieren (nur auf Train fitten) ---\n",
    "x_scaler = StandardScaler().fit(X_train)\n",
    "X_train_z = x_scaler.transform(X_train)\n",
    "X_val_z   = x_scaler.transform(X_val)\n",
    "X_test_z  = x_scaler.transform(X_test)\n",
    "\n",
    "# --- y skalieren (für tanh: [-1,1]) -> spaltenweise, automatisch ---\n",
    "y_scaler  = MinMaxScaler(feature_range=(-1, 1)).fit(y_train)  # y_train: (n,2)\n",
    "y_train_s = y_scaler.transform(y_train)\n",
    "y_val_s   = y_scaler.transform(y_val)\n",
    "y_test_s  = y_scaler.transform(y_test)\n",
    "\n",
    "def inv_y(y_s):\n",
    "    \"\"\"Skalierung der Targets rückgängig machen -> echte Einheiten (Mn, Mw).\"\"\"\n",
    "    return y_scaler.inverse_transform(y_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "dBaYl7yWjKR5"
   },
   "outputs": [],
   "source": [
    "ann = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "otPjam8rjLqF",
    "outputId": "c6ef832c-fff8-46ad-d52a-389edaf598dd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\brian\\PycharmProjects\\PythonProject1\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import layers, regularizers, callbacks\n",
    "\n",
    "# Annahmen:\n",
    "# X_train_z, X_val_z, X_test_z = standardisierte Features\n",
    "# y_train_s, y_val_s, y_test_s = MinMax(-1,1) skalierte Targets, Form (n, 2)\n",
    "# inv_y(...)  -> macht die Skalierung von y rückgängig (zurück in echte Einheiten)\n",
    "\n",
    "assert y_train_s.ndim == 2 and y_train_s.shape[1] == 2, \"y muss (n,2) sein!\"\n",
    "\n",
    "model = Sequential([\n",
    "    layers.Dense(16, activation='relu',\n",
    "                 kernel_regularizer=regularizers.l2(1e-5),\n",
    "                 input_shape=(X_train_z.shape[1],)),\n",
    "    Dropout(0.1),\n",
    "        layers.Dense(8, activation='relu', kernel_regularizer=regularizers.l2(1e-5)),\n",
    "    Dropout(0.1),\n",
    "        layers.Dense(8, activation='relu', kernel_regularizer=regularizers.l2(1e-5)),\n",
    "    Dropout(0.1),\n",
    "    layers.Dense(2, activation='tanh')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae', 'mape'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9SiGI9kdjLoU",
    "outputId": "a56e7194-c800-4b0d-945d-a91716792ef7",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - loss: 0.3325 - mae: 0.4554 - mape: 150.3302 - val_loss: 0.1860 - val_mae: 0.3279 - val_mape: 133.2568\n",
      "Epoch 2/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.2985 - mae: 0.4136 - mape: 111.3289 - val_loss: 0.1847 - val_mae: 0.3261 - val_mape: 132.1751\n",
      "Epoch 3/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.2891 - mae: 0.3982 - mape: 115.8785 - val_loss: 0.1838 - val_mae: 0.3245 - val_mape: 131.2116\n",
      "Epoch 4/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.3517 - mae: 0.4688 - mape: 127.0685 - val_loss: 0.1827 - val_mae: 0.3229 - val_mape: 130.5898\n",
      "Epoch 5/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.3128 - mae: 0.4151 - mape: 109.0199 - val_loss: 0.1817 - val_mae: 0.3214 - val_mape: 129.8776\n",
      "Epoch 6/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.3081 - mae: 0.4312 - mape: 142.6118 - val_loss: 0.1805 - val_mae: 0.3199 - val_mape: 129.2465\n",
      "Epoch 7/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.3599 - mae: 0.4612 - mape: 165.5686 - val_loss: 0.1792 - val_mae: 0.3184 - val_mape: 128.4391\n",
      "Epoch 8/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.3274 - mae: 0.4038 - mape: 103.2094 - val_loss: 0.1781 - val_mae: 0.3169 - val_mape: 127.4823\n",
      "Epoch 9/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.3601 - mae: 0.4396 - mape: 83.9728 - val_loss: 0.1773 - val_mae: 0.3153 - val_mape: 126.4904\n",
      "Epoch 10/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.3566 - mae: 0.4784 - mape: 133.4995 - val_loss: 0.1765 - val_mae: 0.3138 - val_mape: 125.5530\n",
      "Epoch 11/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.3191 - mae: 0.3856 - mape: 104.2930 - val_loss: 0.1756 - val_mae: 0.3122 - val_mape: 124.6767\n",
      "Epoch 12/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.3204 - mae: 0.4316 - mape: 136.1971 - val_loss: 0.1747 - val_mae: 0.3107 - val_mape: 123.8100\n",
      "Epoch 13/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.3798 - mae: 0.4557 - mape: 118.8685 - val_loss: 0.1735 - val_mae: 0.3087 - val_mape: 122.4797\n",
      "Epoch 14/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.2866 - mae: 0.4241 - mape: 168.5107 - val_loss: 0.1723 - val_mae: 0.3067 - val_mape: 121.2585\n",
      "Epoch 15/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.3277 - mae: 0.4127 - mape: 90.5141 - val_loss: 0.1709 - val_mae: 0.3044 - val_mape: 119.9523\n",
      "Epoch 16/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.2938 - mae: 0.4160 - mape: 125.3001 - val_loss: 0.1696 - val_mae: 0.3021 - val_mape: 118.6327\n",
      "Epoch 17/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.2519 - mae: 0.3671 - mape: 140.0738 - val_loss: 0.1682 - val_mae: 0.2998 - val_mape: 117.2852\n",
      "Epoch 18/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.3259 - mae: 0.4604 - mape: 137.4111 - val_loss: 0.1669 - val_mae: 0.2982 - val_mape: 116.2292\n",
      "Epoch 19/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.3602 - mae: 0.4419 - mape: 100.4558 - val_loss: 0.1657 - val_mae: 0.2968 - val_mape: 115.2924\n",
      "Epoch 20/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.3282 - mae: 0.4400 - mape: 130.8684 - val_loss: 0.1646 - val_mae: 0.2952 - val_mape: 114.1720\n",
      "Epoch 21/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.2376 - mae: 0.3399 - mape: 84.1819 - val_loss: 0.1635 - val_mae: 0.2936 - val_mape: 113.1383\n",
      "Epoch 22/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.2537 - mae: 0.3785 - mape: 139.1620 - val_loss: 0.1623 - val_mae: 0.2922 - val_mape: 112.2715\n",
      "Epoch 23/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.2932 - mae: 0.3998 - mape: 98.4633 - val_loss: 0.1611 - val_mae: 0.2907 - val_mape: 111.5340\n",
      "Epoch 24/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.2720 - mae: 0.3968 - mape: 166.9418 - val_loss: 0.1597 - val_mae: 0.2893 - val_mape: 110.8791\n",
      "Epoch 25/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.3098 - mae: 0.4156 - mape: 153.3640 - val_loss: 0.1585 - val_mae: 0.2880 - val_mape: 110.4923\n",
      "Epoch 26/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.2582 - mae: 0.3717 - mape: 119.6178 - val_loss: 0.1572 - val_mae: 0.2866 - val_mape: 110.0248\n",
      "Epoch 27/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.2494 - mae: 0.3600 - mape: 106.6604 - val_loss: 0.1561 - val_mae: 0.2852 - val_mape: 109.5878\n",
      "Epoch 28/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.3034 - mae: 0.4184 - mape: 140.8707 - val_loss: 0.1549 - val_mae: 0.2838 - val_mape: 109.0894\n",
      "Epoch 29/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 0.2550 - mae: 0.3630 - mape: 99.6625 - val_loss: 0.1538 - val_mae: 0.2825 - val_mape: 108.5545\n",
      "Epoch 30/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.3482 - mae: 0.4757 - mape: 168.8913 - val_loss: 0.1529 - val_mae: 0.2811 - val_mape: 107.9455\n",
      "Epoch 31/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 0.2369 - mae: 0.3513 - mape: 100.7611 - val_loss: 0.1520 - val_mae: 0.2799 - val_mape: 107.4097\n",
      "Epoch 32/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.2406 - mae: 0.3610 - mape: 115.0756 - val_loss: 0.1512 - val_mae: 0.2785 - val_mape: 106.7314\n",
      "Epoch 33/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.3291 - mae: 0.4428 - mape: 147.9888 - val_loss: 0.1503 - val_mae: 0.2770 - val_mape: 106.2338\n",
      "Epoch 34/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.2411 - mae: 0.3745 - mape: 132.9095 - val_loss: 0.1495 - val_mae: 0.2755 - val_mape: 105.7087\n",
      "Epoch 35/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.2109 - mae: 0.3181 - mape: 111.5610 - val_loss: 0.1487 - val_mae: 0.2739 - val_mape: 105.0736\n",
      "Epoch 36/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.2251 - mae: 0.3449 - mape: 136.4248 - val_loss: 0.1479 - val_mae: 0.2726 - val_mape: 104.4723\n",
      "Epoch 37/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.2722 - mae: 0.3726 - mape: 119.6498 - val_loss: 0.1472 - val_mae: 0.2712 - val_mape: 103.7849\n",
      "Epoch 38/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.2974 - mae: 0.3875 - mape: 126.5415 - val_loss: 0.1466 - val_mae: 0.2700 - val_mape: 103.1072\n",
      "Epoch 39/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.3237 - mae: 0.4446 - mape: 136.4746 - val_loss: 0.1460 - val_mae: 0.2689 - val_mape: 102.4824\n",
      "Epoch 40/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.2254 - mae: 0.3443 - mape: 114.2652 - val_loss: 0.1454 - val_mae: 0.2680 - val_mape: 101.9247\n",
      "Epoch 41/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.2527 - mae: 0.3673 - mape: 88.1419 - val_loss: 0.1448 - val_mae: 0.2671 - val_mape: 101.4084\n",
      "Epoch 42/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.2509 - mae: 0.3706 - mape: 105.1515 - val_loss: 0.1441 - val_mae: 0.2665 - val_mape: 101.1070\n",
      "Epoch 43/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.3444 - mae: 0.4366 - mape: 101.5877 - val_loss: 0.1435 - val_mae: 0.2660 - val_mape: 100.8827\n",
      "Epoch 44/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.2599 - mae: 0.3939 - mape: 138.4146 - val_loss: 0.1428 - val_mae: 0.2652 - val_mape: 100.4750\n",
      "Epoch 45/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.3443 - mae: 0.4508 - mape: 183.7702 - val_loss: 0.1422 - val_mae: 0.2645 - val_mape: 99.9825\n",
      "Epoch 46/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.2246 - mae: 0.3394 - mape: 110.0347 - val_loss: 0.1417 - val_mae: 0.2636 - val_mape: 99.2738\n",
      "Epoch 47/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - loss: 0.2496 - mae: 0.3827 - mape: 109.1419 - val_loss: 0.1411 - val_mae: 0.2627 - val_mape: 98.6301\n",
      "Epoch 48/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.3268 - mae: 0.4336 - mape: 121.5531 - val_loss: 0.1407 - val_mae: 0.2618 - val_mape: 97.8622\n",
      "Epoch 49/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.3007 - mae: 0.4113 - mape: 114.2159 - val_loss: 0.1404 - val_mae: 0.2610 - val_mape: 97.0203\n",
      "Epoch 50/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.2883 - mae: 0.3909 - mape: 100.4581 - val_loss: 0.1400 - val_mae: 0.2603 - val_mape: 96.4541\n",
      "Epoch 51/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.2068 - mae: 0.3081 - mape: 86.7616 - val_loss: 0.1398 - val_mae: 0.2596 - val_mape: 95.8841\n",
      "Epoch 52/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.2220 - mae: 0.3401 - mape: 102.6794 - val_loss: 0.1396 - val_mae: 0.2591 - val_mape: 95.4651\n",
      "Epoch 53/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.3858 - mae: 0.4246 - mape: 96.3618 - val_loss: 0.1395 - val_mae: 0.2587 - val_mape: 95.1866\n",
      "Epoch 54/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.2165 - mae: 0.3219 - mape: 102.1645 - val_loss: 0.1394 - val_mae: 0.2583 - val_mape: 94.9367\n",
      "Epoch 55/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.2475 - mae: 0.3620 - mape: 104.9864 - val_loss: 0.1393 - val_mae: 0.2581 - val_mape: 94.8625\n",
      "Epoch 56/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.2893 - mae: 0.4026 - mape: 131.9684 - val_loss: 0.1394 - val_mae: 0.2577 - val_mape: 94.6358\n",
      "Epoch 57/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.2395 - mae: 0.3702 - mape: 114.7616 - val_loss: 0.1394 - val_mae: 0.2574 - val_mape: 94.4064\n",
      "Epoch 58/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.2956 - mae: 0.3727 - mape: 96.2696 - val_loss: 0.1392 - val_mae: 0.2570 - val_mape: 94.2369\n",
      "Epoch 59/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.2958 - mae: 0.4174 - mape: 134.6978 - val_loss: 0.1391 - val_mae: 0.2565 - val_mape: 93.8663\n",
      "Epoch 60/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.4055 - mae: 0.4769 - mape: 139.4277 - val_loss: 0.1386 - val_mae: 0.2555 - val_mape: 93.2709\n",
      "Epoch 61/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - loss: 0.1916 - mae: 0.2850 - mape: 72.3626 - val_loss: 0.1381 - val_mae: 0.2545 - val_mape: 92.5875\n",
      "Epoch 62/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.2199 - mae: 0.3447 - mape: 130.5565 - val_loss: 0.1375 - val_mae: 0.2535 - val_mape: 91.8290\n",
      "Epoch 63/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.2430 - mae: 0.3412 - mape: 89.4645 - val_loss: 0.1370 - val_mae: 0.2525 - val_mape: 91.1473\n",
      "Epoch 64/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.2408 - mae: 0.3590 - mape: 103.1203 - val_loss: 0.1366 - val_mae: 0.2517 - val_mape: 90.4539\n",
      "Epoch 65/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.2140 - mae: 0.3318 - mape: 141.7802 - val_loss: 0.1361 - val_mae: 0.2506 - val_mape: 89.5556\n",
      "Epoch 66/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.2876 - mae: 0.3917 - mape: 125.9105 - val_loss: 0.1355 - val_mae: 0.2495 - val_mape: 88.7926\n",
      "Epoch 67/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.2711 - mae: 0.4107 - mape: 108.4894 - val_loss: 0.1349 - val_mae: 0.2488 - val_mape: 88.3281\n",
      "Epoch 68/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.2069 - mae: 0.3138 - mape: 94.2991 - val_loss: 0.1345 - val_mae: 0.2481 - val_mape: 87.7947\n",
      "Epoch 69/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 0.2961 - mae: 0.4254 - mape: 117.6638 - val_loss: 0.1342 - val_mae: 0.2475 - val_mape: 87.3058\n",
      "Epoch 70/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.3168 - mae: 0.3815 - mape: 118.5986 - val_loss: 0.1337 - val_mae: 0.2471 - val_mape: 87.1305\n",
      "Epoch 71/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.2370 - mae: 0.3760 - mape: 107.9144 - val_loss: 0.1334 - val_mae: 0.2467 - val_mape: 87.0360\n",
      "Epoch 72/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - loss: 0.1965 - mae: 0.3109 - mape: 103.1441 - val_loss: 0.1332 - val_mae: 0.2465 - val_mape: 87.0143\n",
      "Epoch 73/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.2160 - mae: 0.3485 - mape: 148.2195 - val_loss: 0.1331 - val_mae: 0.2464 - val_mape: 86.9789\n",
      "Epoch 74/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.2190 - mae: 0.3453 - mape: 162.1934 - val_loss: 0.1330 - val_mae: 0.2462 - val_mape: 86.8586\n",
      "Epoch 75/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - loss: 0.2025 - mae: 0.3148 - mape: 109.8371 - val_loss: 0.1329 - val_mae: 0.2460 - val_mape: 86.6945\n",
      "Epoch 76/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - loss: 0.2199 - mae: 0.3617 - mape: 149.1390 - val_loss: 0.1327 - val_mae: 0.2459 - val_mape: 86.5837\n",
      "Epoch 77/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.2089 - mae: 0.3255 - mape: 93.9572 - val_loss: 0.1325 - val_mae: 0.2459 - val_mape: 86.6361\n",
      "Epoch 78/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.2417 - mae: 0.3722 - mape: 104.2180 - val_loss: 0.1325 - val_mae: 0.2461 - val_mape: 86.7135\n",
      "Epoch 79/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - loss: 0.2756 - mae: 0.3725 - mape: 100.1410 - val_loss: 0.1324 - val_mae: 0.2462 - val_mape: 86.8038\n",
      "Epoch 80/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - loss: 0.3122 - mae: 0.3934 - mape: 92.6451 - val_loss: 0.1325 - val_mae: 0.2462 - val_mape: 86.8578\n",
      "Epoch 81/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.1886 - mae: 0.2922 - mape: 91.8784 - val_loss: 0.1325 - val_mae: 0.2462 - val_mape: 86.8540\n",
      "Epoch 82/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.1948 - mae: 0.2964 - mape: 104.4881 - val_loss: 0.1325 - val_mae: 0.2461 - val_mape: 86.7113\n",
      "Epoch 83/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 0.2009 - mae: 0.3160 - mape: 109.2203 - val_loss: 0.1325 - val_mae: 0.2458 - val_mape: 86.3325\n",
      "Epoch 84/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - loss: 0.2589 - mae: 0.3515 - mape: 113.2788 - val_loss: 0.1326 - val_mae: 0.2455 - val_mape: 85.7584\n",
      "Epoch 85/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.2890 - mae: 0.3756 - mape: 84.1780 - val_loss: 0.1327 - val_mae: 0.2454 - val_mape: 85.5252\n",
      "Epoch 86/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.2095 - mae: 0.3329 - mape: 127.8163 - val_loss: 0.1327 - val_mae: 0.2452 - val_mape: 85.1285\n",
      "Epoch 87/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.2053 - mae: 0.3191 - mape: 99.3816 - val_loss: 0.1329 - val_mae: 0.2449 - val_mape: 84.6292\n",
      "Epoch 88/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.1885 - mae: 0.3000 - mape: 102.1389 - val_loss: 0.1330 - val_mae: 0.2446 - val_mape: 84.0762\n",
      "Epoch 89/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.1812 - mae: 0.2611 - mape: 96.6702 - val_loss: 0.1331 - val_mae: 0.2442 - val_mape: 83.5133\n",
      "Epoch 90/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 0.1909 - mae: 0.3062 - mape: 116.2871 - val_loss: 0.1330 - val_mae: 0.2436 - val_mape: 82.7046\n",
      "Epoch 91/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 0.2116 - mae: 0.3402 - mape: 91.6309 - val_loss: 0.1329 - val_mae: 0.2431 - val_mape: 81.9914\n",
      "Epoch 92/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - loss: 0.1797 - mae: 0.2740 - mape: 91.5112 - val_loss: 0.1329 - val_mae: 0.2425 - val_mape: 81.1874\n",
      "Epoch 93/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.2074 - mae: 0.3367 - mape: 155.7093 - val_loss: 0.1326 - val_mae: 0.2417 - val_mape: 80.1317\n",
      "Epoch 94/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.2039 - mae: 0.3260 - mape: 96.9138 - val_loss: 0.1323 - val_mae: 0.2409 - val_mape: 79.1299\n",
      "Epoch 95/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.2365 - mae: 0.3708 - mape: 138.5627 - val_loss: 0.1321 - val_mae: 0.2400 - val_mape: 78.1237\n",
      "Epoch 96/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.1875 - mae: 0.2988 - mape: 103.4716 - val_loss: 0.1318 - val_mae: 0.2392 - val_mape: 77.2313\n",
      "Epoch 97/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 0.2073 - mae: 0.3300 - mape: 104.7398 - val_loss: 0.1316 - val_mae: 0.2385 - val_mape: 76.3798\n",
      "Epoch 98/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - loss: 0.1979 - mae: 0.3063 - mape: 95.9116 - val_loss: 0.1314 - val_mae: 0.2377 - val_mape: 75.4792\n",
      "Epoch 99/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1827 - mae: 0.2972 - mape: 89.6351 - val_loss: 0.1313 - val_mae: 0.2370 - val_mape: 74.6931\n",
      "Epoch 100/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1843 - mae: 0.3071 - mape: 98.7328 - val_loss: 0.1314 - val_mae: 0.2363 - val_mape: 73.9904\n",
      "Epoch 101/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.2229 - mae: 0.3573 - mape: 148.2333 - val_loss: 0.1313 - val_mae: 0.2356 - val_mape: 73.2781\n",
      "Epoch 102/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.2390 - mae: 0.3616 - mape: 115.7456 - val_loss: 0.1310 - val_mae: 0.2347 - val_mape: 72.6734\n",
      "Epoch 103/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - loss: 0.3135 - mae: 0.4394 - mape: 112.6002 - val_loss: 0.1309 - val_mae: 0.2341 - val_mape: 72.4630\n",
      "Epoch 104/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 0.2371 - mae: 0.3293 - mape: 74.9300 - val_loss: 0.1310 - val_mae: 0.2336 - val_mape: 72.2309\n",
      "Epoch 105/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.2243 - mae: 0.3505 - mape: 91.0223 - val_loss: 0.1312 - val_mae: 0.2333 - val_mape: 72.0140\n",
      "Epoch 106/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1740 - mae: 0.2751 - mape: 68.6872 - val_loss: 0.1314 - val_mae: 0.2330 - val_mape: 71.6960\n",
      "Epoch 107/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.1759 - mae: 0.2759 - mape: 105.1316 - val_loss: 0.1316 - val_mae: 0.2324 - val_mape: 71.1037\n",
      "Epoch 108/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.1839 - mae: 0.2868 - mape: 99.3452 - val_loss: 0.1316 - val_mae: 0.2318 - val_mape: 70.4420\n",
      "Epoch 109/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - loss: 0.2706 - mae: 0.3903 - mape: 112.4043 - val_loss: 0.1315 - val_mae: 0.2314 - val_mape: 69.9858\n",
      "Epoch 110/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.2438 - mae: 0.3444 - mape: 133.2089 - val_loss: 0.1314 - val_mae: 0.2309 - val_mape: 69.5938\n",
      "Epoch 111/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.1846 - mae: 0.3071 - mape: 97.8250 - val_loss: 0.1312 - val_mae: 0.2301 - val_mape: 68.9291\n",
      "Epoch 112/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 0.1674 - mae: 0.2720 - mape: 76.2797 - val_loss: 0.1310 - val_mae: 0.2293 - val_mape: 68.1789\n",
      "Epoch 113/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - loss: 0.1666 - mae: 0.2657 - mape: 87.8570 - val_loss: 0.1309 - val_mae: 0.2286 - val_mape: 67.3604\n",
      "Epoch 114/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.2200 - mae: 0.3569 - mape: 122.0983 - val_loss: 0.1308 - val_mae: 0.2277 - val_mape: 66.5751\n",
      "Epoch 115/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.1983 - mae: 0.3308 - mape: 103.5740 - val_loss: 0.1308 - val_mae: 0.2271 - val_mape: 65.9076\n",
      "Epoch 116/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.2020 - mae: 0.3325 - mape: 90.2317 - val_loss: 0.1308 - val_mae: 0.2264 - val_mape: 65.1734\n",
      "Epoch 117/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.1799 - mae: 0.2960 - mape: 92.4363 - val_loss: 0.1309 - val_mae: 0.2261 - val_mape: 64.6392\n",
      "Epoch 118/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.2498 - mae: 0.3671 - mape: 101.8236 - val_loss: 0.1311 - val_mae: 0.2258 - val_mape: 64.2023\n",
      "Epoch 119/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.1995 - mae: 0.3256 - mape: 122.3740 - val_loss: 0.1312 - val_mae: 0.2255 - val_mape: 63.9716\n",
      "Epoch 120/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.3172 - mae: 0.3475 - mape: 101.2627 - val_loss: 0.1312 - val_mae: 0.2251 - val_mape: 63.3959\n",
      "Epoch 121/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.1612 - mae: 0.2619 - mape: 91.2282 - val_loss: 0.1313 - val_mae: 0.2246 - val_mape: 62.8651\n",
      "Epoch 122/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.2195 - mae: 0.3785 - mape: 148.8434 - val_loss: 0.1313 - val_mae: 0.2247 - val_mape: 62.8201\n",
      "Epoch 123/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1986 - mae: 0.3075 - mape: 87.0311 - val_loss: 0.1313 - val_mae: 0.2250 - val_mape: 63.1842\n",
      "Epoch 124/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1895 - mae: 0.3130 - mape: 89.3453 - val_loss: 0.1311 - val_mae: 0.2252 - val_mape: 63.4767\n",
      "Epoch 125/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.1653 - mae: 0.2715 - mape: 101.3255 - val_loss: 0.1311 - val_mae: 0.2254 - val_mape: 63.5953\n",
      "Epoch 126/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.1657 - mae: 0.2696 - mape: 80.2925 - val_loss: 0.1308 - val_mae: 0.2257 - val_mape: 63.8893\n",
      "Epoch 127/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - loss: 0.1940 - mae: 0.3388 - mape: 140.2414 - val_loss: 0.1306 - val_mae: 0.2261 - val_mape: 64.3947\n",
      "Epoch 128/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1753 - mae: 0.3036 - mape: 104.9517 - val_loss: 0.1304 - val_mae: 0.2264 - val_mape: 64.7146\n",
      "Epoch 129/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.2237 - mae: 0.3537 - mape: 118.6341 - val_loss: 0.1302 - val_mae: 0.2265 - val_mape: 64.8831\n",
      "Epoch 130/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.1739 - mae: 0.2877 - mape: 86.9987 - val_loss: 0.1299 - val_mae: 0.2266 - val_mape: 65.1660\n",
      "Epoch 131/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.2038 - mae: 0.3480 - mape: 111.5244 - val_loss: 0.1297 - val_mae: 0.2267 - val_mape: 65.5824\n",
      "Epoch 132/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.2539 - mae: 0.3673 - mape: 88.0779 - val_loss: 0.1296 - val_mae: 0.2269 - val_mape: 66.0681\n",
      "Epoch 133/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - loss: 0.1657 - mae: 0.2710 - mape: 93.3040 - val_loss: 0.1294 - val_mae: 0.2270 - val_mape: 66.4199\n",
      "Epoch 134/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.1715 - mae: 0.2829 - mape: 99.8702 - val_loss: 0.1292 - val_mae: 0.2268 - val_mape: 66.4658\n",
      "Epoch 135/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.2109 - mae: 0.3575 - mape: 119.6020 - val_loss: 0.1291 - val_mae: 0.2261 - val_mape: 66.3191\n",
      "Epoch 136/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.1717 - mae: 0.2858 - mape: 80.5703 - val_loss: 0.1291 - val_mae: 0.2254 - val_mape: 66.1158\n",
      "Epoch 137/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.2740 - mae: 0.3902 - mape: 108.5965 - val_loss: 0.1290 - val_mae: 0.2242 - val_mape: 65.7022\n",
      "Epoch 138/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.1693 - mae: 0.2795 - mape: 96.9760 - val_loss: 0.1291 - val_mae: 0.2233 - val_mape: 65.2649\n",
      "Epoch 139/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 0.1539 - mae: 0.2521 - mape: 82.1216 - val_loss: 0.1292 - val_mae: 0.2224 - val_mape: 64.7894\n",
      "Epoch 140/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.1750 - mae: 0.2933 - mape: 91.0272 - val_loss: 0.1295 - val_mae: 0.2216 - val_mape: 64.2873\n",
      "Epoch 141/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.1650 - mae: 0.2881 - mape: 108.4757 - val_loss: 0.1297 - val_mae: 0.2211 - val_mape: 63.8822\n",
      "Epoch 142/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.1811 - mae: 0.3049 - mape: 97.2014 - val_loss: 0.1300 - val_mae: 0.2213 - val_mape: 63.9333\n",
      "Epoch 143/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.2028 - mae: 0.3492 - mape: 114.6758 - val_loss: 0.1303 - val_mae: 0.2212 - val_mape: 63.7567\n",
      "Epoch 144/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.1693 - mae: 0.2828 - mape: 87.5772 - val_loss: 0.1306 - val_mae: 0.2212 - val_mape: 63.6066\n",
      "Epoch 145/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.2393 - mae: 0.3895 - mape: 135.0856 - val_loss: 0.1308 - val_mae: 0.2213 - val_mape: 63.5729\n",
      "Epoch 146/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.1790 - mae: 0.3020 - mape: 101.2390 - val_loss: 0.1310 - val_mae: 0.2213 - val_mape: 63.5102\n",
      "Epoch 147/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.1958 - mae: 0.3013 - mape: 80.8162 - val_loss: 0.1311 - val_mae: 0.2211 - val_mape: 63.1314\n",
      "Epoch 148/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.1908 - mae: 0.3217 - mape: 89.8433 - val_loss: 0.1308 - val_mae: 0.2207 - val_mape: 62.7151\n",
      "Epoch 149/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.1618 - mae: 0.2669 - mape: 82.8950 - val_loss: 0.1306 - val_mae: 0.2203 - val_mape: 62.1856\n",
      "Epoch 150/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.1845 - mae: 0.3184 - mape: 101.4710 - val_loss: 0.1304 - val_mae: 0.2197 - val_mape: 61.4292\n",
      "Epoch 151/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 0.1812 - mae: 0.2990 - mape: 110.3041 - val_loss: 0.1302 - val_mae: 0.2193 - val_mape: 60.9973\n",
      "Epoch 152/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.1689 - mae: 0.3004 - mape: 117.2335 - val_loss: 0.1299 - val_mae: 0.2189 - val_mape: 60.7242\n",
      "Epoch 153/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.1536 - mae: 0.2702 - mape: 87.6321 - val_loss: 0.1296 - val_mae: 0.2184 - val_mape: 60.3202\n",
      "Epoch 154/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.1784 - mae: 0.3133 - mape: 90.9895 - val_loss: 0.1295 - val_mae: 0.2181 - val_mape: 60.1859\n",
      "Epoch 155/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.1671 - mae: 0.2950 - mape: 112.9429 - val_loss: 0.1292 - val_mae: 0.2177 - val_mape: 59.9851\n",
      "Epoch 156/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.3093 - mae: 0.3248 - mape: 72.4712 - val_loss: 0.1291 - val_mae: 0.2173 - val_mape: 59.6789\n",
      "Epoch 157/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.1481 - mae: 0.2533 - mape: 86.9189 - val_loss: 0.1290 - val_mae: 0.2173 - val_mape: 59.6602\n",
      "Epoch 158/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 0.1489 - mae: 0.2640 - mape: 93.4185 - val_loss: 0.1290 - val_mae: 0.2178 - val_mape: 59.7221\n",
      "Epoch 159/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - loss: 0.1827 - mae: 0.3028 - mape: 72.1880 - val_loss: 0.1292 - val_mae: 0.2183 - val_mape: 59.9180\n",
      "Epoch 160/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - loss: 0.1544 - mae: 0.2699 - mape: 110.3609 - val_loss: 0.1294 - val_mae: 0.2190 - val_mape: 60.0710\n",
      "Epoch 161/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 0.2396 - mae: 0.3630 - mape: 129.4127 - val_loss: 0.1294 - val_mae: 0.2198 - val_mape: 60.4722\n",
      "Epoch 162/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.1562 - mae: 0.2761 - mape: 89.7112 - val_loss: 0.1295 - val_mae: 0.2204 - val_mape: 60.7727\n",
      "Epoch 163/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.2122 - mae: 0.3651 - mape: 140.3304 - val_loss: 0.1295 - val_mae: 0.2211 - val_mape: 61.1392\n",
      "Epoch 164/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.1581 - mae: 0.2813 - mape: 108.0645 - val_loss: 0.1296 - val_mae: 0.2217 - val_mape: 61.3320\n",
      "Epoch 165/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.2000 - mae: 0.3079 - mape: 90.1467 - val_loss: 0.1298 - val_mae: 0.2225 - val_mape: 61.2803\n",
      "Epoch 166/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.1434 - mae: 0.2508 - mape: 90.0578 - val_loss: 0.1300 - val_mae: 0.2233 - val_mape: 61.2887\n",
      "Epoch 167/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.1452 - mae: 0.2496 - mape: 66.0943 - val_loss: 0.1303 - val_mae: 0.2241 - val_mape: 61.2931\n",
      "Epoch 168/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.1453 - mae: 0.2573 - mape: 80.0540 - val_loss: 0.1306 - val_mae: 0.2248 - val_mape: 61.3138\n",
      "Epoch 169/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.1531 - mae: 0.2644 - mape: 87.6023 - val_loss: 0.1310 - val_mae: 0.2255 - val_mape: 61.2907\n",
      "Epoch 170/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.1787 - mae: 0.3124 - mape: 104.7761 - val_loss: 0.1315 - val_mae: 0.2261 - val_mape: 61.2360\n",
      "Epoch 171/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.1685 - mae: 0.2995 - mape: 100.0437 - val_loss: 0.1320 - val_mae: 0.2267 - val_mape: 61.1669\n",
      "Epoch 172/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.1712 - mae: 0.3029 - mape: 100.5280 - val_loss: 0.1325 - val_mae: 0.2271 - val_mape: 60.7703\n",
      "Epoch 173/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.1494 - mae: 0.2536 - mape: 93.3834 - val_loss: 0.1329 - val_mae: 0.2275 - val_mape: 60.2578\n",
      "Epoch 174/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.2836 - mae: 0.3508 - mape: 93.6810 - val_loss: 0.1330 - val_mae: 0.2274 - val_mape: 59.7852\n",
      "Epoch 175/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.1533 - mae: 0.2747 - mape: 105.6597 - val_loss: 0.1330 - val_mae: 0.2269 - val_mape: 59.0660\n",
      "Epoch 176/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.1495 - mae: 0.2438 - mape: 83.5856 - val_loss: 0.1329 - val_mae: 0.2263 - val_mape: 58.2358\n",
      "Epoch 177/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.1496 - mae: 0.2602 - mape: 83.5641 - val_loss: 0.1328 - val_mae: 0.2257 - val_mape: 57.4721\n",
      "Epoch 178/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1707 - mae: 0.3192 - mape: 144.4429 - val_loss: 0.1325 - val_mae: 0.2248 - val_mape: 56.7679\n",
      "Epoch 179/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.2859 - mae: 0.3697 - mape: 123.7630 - val_loss: 0.1322 - val_mae: 0.2249 - val_mape: 56.9779\n",
      "Epoch 180/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.1413 - mae: 0.2632 - mape: 100.2745 - val_loss: 0.1320 - val_mae: 0.2249 - val_mape: 57.1900\n",
      "Epoch 181/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - loss: 0.1368 - mae: 0.2402 - mape: 88.0425 - val_loss: 0.1317 - val_mae: 0.2257 - val_mape: 57.8753\n",
      "Epoch 182/500\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - loss: 0.1502 - mae: 0.2760 - mape: 118.8268 - val_loss: 0.1314 - val_mae: 0.2265 - val_mape: 58.4797\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='mse', metrics=['mae', 'mape'])\n",
    "\n",
    "early = EarlyStopping(monitor='val_loss', patience=25, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_z, y_train_s,\n",
    "    validation_data=(X_val_z, y_val_s),\n",
    "    epochs=500, batch_size=16, verbose=1, callbacks=[early]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HLV4VDdDjLmG",
    "outputId": "e06804a3-2b4d-4bc0-ad9d-206f2ab4cdb8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n"
     ]
    }
   ],
   "source": [
    "#Vorhersagen (zuerst im skalierten Raum, dann zurück in echte Einheiten)\n",
    "y_hat_train_s = model.predict(X_train_z, verbose=1)\n",
    "y_hat_val_s   = model.predict(X_val_z,   verbose=1)\n",
    "y_hat_test_s  = model.predict(X_test_z,  verbose=1)\n",
    "\n",
    "y_hat_train = inv_y(y_hat_train_s)\n",
    "y_hat_val   = inv_y(y_hat_val_s)\n",
    "y_hat_test  = inv_y(y_hat_test_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "xeWwgdE0jLkG"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error\n",
    "\n",
    "def report_2out(name, yt, yp, labels=(\"Mn\", \"Mw\")):\n",
    "    # yt و yp باید (n,2) باشن\n",
    "    for i, label in enumerate(labels):\n",
    "        y_true = yt[:, i].reshape(-1)\n",
    "        y_pred = yp[:, i].reshape(-1)\n",
    "\n",
    "        mae  = mean_absolute_error(y_true, y_pred)\n",
    "        mse  = mean_squared_error(y_true, y_pred)\n",
    "        r2   = r2_score(y_true, y_pred)\n",
    "        mape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "\n",
    "        print(f\"[{name}] {label}: MAE={mae:.6f}  MSE={mse:.6f}  R²={r2:.4f}  MAPE={mape:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ptHevHX-jLh_",
    "outputId": "ca2a17b0-6b4b-4e41-8e90-50bde0ab8b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Mn: MAE=418.815694  MSE=454503.787176  R²=0.6523  MAPE=0.127492\n",
      "[Train] Mw: MAE=561.508452  MSE=799189.338028  R²=0.6257  MAPE=0.127961\n",
      "[Val  ] Mn: MAE=369.988094  MSE=318464.849942  R²=0.1809  MAPE=0.146121\n",
      "[Val  ] Mw: MAE=529.929165  MSE=846812.767161  R²=0.2107  MAPE=0.121886\n",
      "[Test ] Mn: MAE=408.749437  MSE=289768.203487  R²=0.4381  MAPE=0.179023\n",
      "[Test ] Mw: MAE=516.897972  MSE=640635.927017  R²=0.5489  MAPE=0.148113\n"
     ]
    }
   ],
   "source": [
    "report_2out(\"Train\", y_train, y_hat_train)\n",
    "report_2out(\"Val  \", y_val,   y_hat_val)\n",
    "report_2out(\"Test \", y_test,  y_hat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fGS2LHAAjLfx",
    "outputId": "c5ef78f4-c870-4658-9a4c-388ffdd530b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\users\\brian\\pycharmprojects\\pythonproject1\\.venv\\lib\\site-packages (3.10.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\brian\\pycharmprojects\\pythonproject1\\.venv\\lib\\site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\brian\\pycharmprojects\\pythonproject1\\.venv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\brian\\pycharmprojects\\pythonproject1\\.venv\\lib\\site-packages (from matplotlib) (4.59.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\brian\\pycharmprojects\\pythonproject1\\.venv\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\brian\\pycharmprojects\\pythonproject1\\.venv\\lib\\site-packages (from matplotlib) (2.3.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\brian\\pycharmprojects\\pythonproject1\\.venv\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\brian\\pycharmprojects\\pythonproject1\\.venv\\lib\\site-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\brian\\pycharmprojects\\pythonproject1\\.venv\\lib\\site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\brian\\pycharmprojects\\pythonproject1\\.venv\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\brian\\pycharmprojects\\pythonproject1\\.venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "ZovIuGAXjLd4"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g-ZWVSsjjLby",
    "outputId": "224eda97-b2d7-4034-eb93-9c2c243abade"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split target   actual   predicted     residual   abs_error  pct_error\n",
      "Train     Mn 4663.770 3242.152588  1421.617412 1421.617412  30.482151\n",
      "Train     Mn 4663.040 3242.152588  1420.887412 1420.887412  30.471268\n",
      "Train     Mn 2955.830 2805.520264   150.309736  150.309736   5.085196\n",
      "Train     Mn 1265.880 1247.163330    18.716670   18.716670   1.478550\n",
      "Train     Mn 2752.840 2667.676758    85.163242   85.163242   3.093650\n",
      "Train     Mn 2590.790 2727.087158  -136.297158  136.297158   5.260834\n",
      "Train     Mn 2223.170 2354.296631  -131.126631  131.126631   5.898183\n",
      "Train     Mn 2525.270 2529.447021    -4.177021    4.177021   0.165409\n",
      "Train     Mn 1024.970 1207.910156  -182.940156  182.940156  17.848343\n",
      "Train     Mn 2298.620 2935.541504  -636.921504  636.921504  27.708865\n",
      "Train     Mw 5921.610 4008.720703  1912.889297 1912.889297  32.303534\n",
      "Train     Mw 5921.490 4008.720703  1912.769297 1912.769297  32.302162\n",
      "Train     Mw 2966.910 3273.745605  -306.835605  306.835605  10.341925\n",
      "Train     Mw 1458.130 1530.679565   -72.549565   72.549565   4.975521\n",
      "Train     Mw 3129.570 3254.447021  -124.877021  124.877021   3.990229\n",
      "Train     Mw 3517.860 3241.814453   276.045547  276.045547   7.846974\n",
      "Train     Mw 2989.000 3298.489990  -309.489990  309.489990  10.354299\n",
      "Train     Mw 3441.190 3463.021973   -21.831973   21.831973   0.634431\n",
      "Train     Mw 1339.350 1398.167725   -58.817725   58.817725   4.391513\n",
      "Train     Mw 2972.980 3591.958496  -618.978496  618.978496  20.820137\n",
      "  Val     Mn 3762.880 2991.377197   771.502803  771.502803  20.502987\n",
      "  Val     Mn 2951.900 2805.520264   146.379736  146.379736   4.958831\n",
      "  Val     Mn 1846.180 1928.118408   -81.938408   81.938408   4.438268\n",
      "  Val     Mn 2074.517 3205.964355 -1131.447355 1131.447355  54.540279\n",
      "  Val     Mn 2525.910 2529.447021    -3.537021    3.537021   0.140030\n",
      "  Val     Mn 2752.800 2667.676758    85.123242   85.123242   3.092242\n",
      "  Val     Mw 5752.810 3580.905762  2171.904238 2171.904238  37.753798\n",
      "  Val     Mw 2965.540 3273.745605  -308.205605  308.205605  10.392900\n",
      "  Val     Mw 2689.910 2190.004395   499.905605  499.905605  18.584473\n",
      "  Val     Mw 2970.820 2918.689453    52.130547   52.130547   1.754753\n",
      "  Val     Mw 3440.430 3463.021973   -22.591973   22.591973   0.656661\n",
      "  Val     Mw 3129.610 3254.447021  -124.837021  124.837021   3.988900\n",
      " Test     Mn 2073.900 3205.964355 -1132.064355 1132.064355  54.586256\n",
      " Test     Mn 1845.600 1928.118408   -82.518408   82.518408   4.471088\n",
      " Test     Mn 2322.830 2589.986572  -267.156572  267.156572  11.501340\n",
      " Test     Mn 2298.650 2935.541504  -636.891504  636.891504  27.707198\n",
      " Test     Mn 1950.000 2354.296631  -404.296631  404.296631  20.733161\n",
      " Test     Mn 3764.530 2991.377197   773.152803  773.152803  20.537831\n",
      " Test     Mn 1127.190 1225.451416   -98.261416   98.261416   8.717378\n",
      " Test     Mn 2322.860 2589.986572  -267.126572  267.126572  11.499900\n",
      " Test     Mn 1264.440 1247.163330    17.276670   17.276670   1.366350\n",
      " Test     Mw 2974.510 2918.689453    55.820547   55.820547   1.876630\n",
      " Test     Mw 2690.500 2190.004395   500.495605  500.495605  18.602327\n",
      " Test     Mw 2987.310 2669.304932   318.005068  318.005068  10.645198\n",
      " Test     Mw 2972.940 3591.958496  -619.018496  619.018496  20.821762\n",
      " Test     Mw 2878.900 3298.489990  -419.589990  419.589990  14.574664\n",
      " Test     Mw 5752.150 3580.905762  2171.244238 2171.244238  37.746655\n",
      " Test     Mw 1321.650 1497.123413  -175.473413  175.473413  13.276844\n",
      " Test     Mw 2987.280 2669.305176   317.974824  317.974824  10.644293\n",
      " Test     Mw 1456.220 1530.679565   -74.459565   74.459565   5.113209\n"
     ]
    }
   ],
   "source": [
    "import numpy as np, pandas as pd\n",
    "\n",
    "def make_table_multi(split, y_true, y_pred, target_names=(\"Mn\",\"Mw\"), n=None):\n",
    "    \"\"\"\n",
    "    Baut eine Tabelle für 2 (oder mehr) Outputs.\n",
    "    y_true, y_pred: Form (n, k)  -> k = Anzahl Outputs (hier 2: Mn, Mw)\n",
    "    pct_error in Prozent (0–100). Für Anteil (0–1) unten scale=1 nutzen.\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    assert y_true.shape == y_pred.shape and y_true.ndim == 2, \"y muss (n,k) sein\"\n",
    "    k = y_true.shape[1]\n",
    "    assert k == len(target_names), \"target_names-Länge passt nicht zu y.shape[1]\"\n",
    "\n",
    "    parts = []\n",
    "    for j, name in enumerate(target_names):\n",
    "        df = pd.DataFrame({\n",
    "            \"split\":     split,\n",
    "            \"target\":    name,\n",
    "            \"actual\":    y_true[:, j],\n",
    "            \"predicted\": y_pred[:, j],\n",
    "        })\n",
    "        df[\"residual\"]  = df[\"actual\"] - df[\"predicted\"]\n",
    "        df[\"abs_error\"] = df[\"residual\"].abs()\n",
    "        # Prozent-Fehler (×100). Wenn du Bruchteil willst, ersetze 100 durch 1.\n",
    "        df[\"pct_error\"] = 100 * df[\"abs_error\"] / df[\"actual\"].abs().clip(lower=1e-8)\n",
    "        parts.append(df if n is None else df.head(n))\n",
    "    return pd.concat(parts, ignore_index=True)\n",
    "\n",
    "# Beispiel-Nutzung (y_* und y_hat_* haben Form (n,2) = [Mn, Mw]):\n",
    "tbl_train = make_table_multi(\"Train\", y_train, y_hat_train, target_names=(\"Mn\",\"Mw\"))\n",
    "tbl_val   = make_table_multi(\"Val\",   y_val,   y_hat_val,   target_names=(\"Mn\",\"Mw\"))\n",
    "tbl_test  = make_table_multi(\"Test\",  y_test,  y_hat_test,  target_names=(\"Mn\",\"Mw\"))\n",
    "\n",
    "tbl_all = pd.concat([tbl_train, tbl_val, tbl_test], ignore_index=True)\n",
    "print(tbl_all.to_string(index=False))   # in Notebook: display(tbl_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "WqltLwhLjLZr"
   },
   "outputs": [],
   "source": [
    "# ---- 0) Reproducibility / Determinism ----\n",
    "import os, random, numpy as np, tensorflow as tf\n",
    "\n",
    "SEED = 42\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "os.environ[\"TF_DETERMINISTIC_OPS\"] = \"1\"\n",
    "\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "try:\n",
    "    tf.keras.utils.set_random_seed(SEED)\n",
    "    tf.config.experimental.enable_op_determinism(True)\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "t9miTBP8jLXi"
   },
   "outputs": [],
   "source": [
    "# ---- 5) Activation (Transfer Function) sweep on hidden layers ----\n",
    "import random, numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, optimizers\n",
    "\n",
    "activations = ['relu', 'tanh', 'sigmoid', 'softplus']\n",
    "labels      = ['ReLU', 'Tanh', 'Sigmoid', 'Softplus']\n",
    "\n",
    "val_mse_real  = []\n",
    "test_mse_real = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CJDODufEBSbC",
    "outputId": "cfa4e88f-ea10-4467-fde0-b49ff2fbd8ea",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\brian\\PycharmProjects\\PythonProject1\\.venv\\Lib\\site-packages\\keras\\src\\backend\\common\\global_state.py:82: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "Epoch 1/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 534ms/step - loss: 0.7409 - mae: 0.6907 - val_loss: 0.5693 - val_mae: 0.6294\n",
      "Epoch 2/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.7336 - mae: 0.6864 - val_loss: 0.5640 - val_mae: 0.6253\n",
      "Epoch 3/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.7263 - mae: 0.6820 - val_loss: 0.5590 - val_mae: 0.6213\n",
      "Epoch 4/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.7190 - mae: 0.6777 - val_loss: 0.5541 - val_mae: 0.6174\n",
      "Epoch 5/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.7119 - mae: 0.6733 - val_loss: 0.5491 - val_mae: 0.6134\n",
      "Epoch 6/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - loss: 0.7048 - mae: 0.6689 - val_loss: 0.5441 - val_mae: 0.6093\n",
      "Epoch 7/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.6977 - mae: 0.6645 - val_loss: 0.5391 - val_mae: 0.6053\n",
      "Epoch 8/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.6907 - mae: 0.6601 - val_loss: 0.5343 - val_mae: 0.6012\n",
      "Epoch 9/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.6838 - mae: 0.6557 - val_loss: 0.5294 - val_mae: 0.5971\n",
      "Epoch 10/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.6770 - mae: 0.6513 - val_loss: 0.5247 - val_mae: 0.5931\n",
      "Epoch 11/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.6702 - mae: 0.6470 - val_loss: 0.5200 - val_mae: 0.5890\n",
      "Epoch 12/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.6635 - mae: 0.6427 - val_loss: 0.5154 - val_mae: 0.5850\n",
      "Epoch 13/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.6568 - mae: 0.6384 - val_loss: 0.5109 - val_mae: 0.5810\n",
      "Epoch 14/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.6502 - mae: 0.6340 - val_loss: 0.5065 - val_mae: 0.5770\n",
      "Epoch 15/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.6436 - mae: 0.6297 - val_loss: 0.5021 - val_mae: 0.5731\n",
      "Epoch 16/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.6372 - mae: 0.6254 - val_loss: 0.4979 - val_mae: 0.5692\n",
      "Epoch 17/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.6307 - mae: 0.6212 - val_loss: 0.4937 - val_mae: 0.5653\n",
      "Epoch 18/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.6244 - mae: 0.6170 - val_loss: 0.4895 - val_mae: 0.5615\n",
      "Epoch 19/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.6181 - mae: 0.6131 - val_loss: 0.4855 - val_mae: 0.5577\n",
      "Epoch 20/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.6119 - mae: 0.6093 - val_loss: 0.4814 - val_mae: 0.5539\n",
      "Epoch 21/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.6057 - mae: 0.6056 - val_loss: 0.4775 - val_mae: 0.5502\n",
      "Epoch 22/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.5996 - mae: 0.6018 - val_loss: 0.4736 - val_mae: 0.5465\n",
      "Epoch 23/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.5935 - mae: 0.5981 - val_loss: 0.4698 - val_mae: 0.5429\n",
      "Epoch 24/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.5875 - mae: 0.5944 - val_loss: 0.4661 - val_mae: 0.5393\n",
      "Epoch 25/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.5816 - mae: 0.5907 - val_loss: 0.4623 - val_mae: 0.5358\n",
      "Epoch 26/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.5757 - mae: 0.5870 - val_loss: 0.4587 - val_mae: 0.5323\n",
      "Epoch 27/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.5698 - mae: 0.5834 - val_loss: 0.4551 - val_mae: 0.5289\n",
      "Epoch 28/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - loss: 0.5640 - mae: 0.5798 - val_loss: 0.4515 - val_mae: 0.5255\n",
      "Epoch 29/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.5583 - mae: 0.5762 - val_loss: 0.4479 - val_mae: 0.5221\n",
      "Epoch 30/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.5525 - mae: 0.5726 - val_loss: 0.4444 - val_mae: 0.5188\n",
      "Epoch 31/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.5468 - mae: 0.5690 - val_loss: 0.4410 - val_mae: 0.5155\n",
      "Epoch 32/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.5412 - mae: 0.5655 - val_loss: 0.4375 - val_mae: 0.5131\n",
      "Epoch 33/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.5357 - mae: 0.5625 - val_loss: 0.4341 - val_mae: 0.5108\n",
      "Epoch 34/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.5301 - mae: 0.5596 - val_loss: 0.4307 - val_mae: 0.5086\n",
      "Epoch 35/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.5246 - mae: 0.5567 - val_loss: 0.4274 - val_mae: 0.5063\n",
      "Epoch 36/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.5191 - mae: 0.5538 - val_loss: 0.4240 - val_mae: 0.5041\n",
      "Epoch 37/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.5136 - mae: 0.5509 - val_loss: 0.4207 - val_mae: 0.5019\n",
      "Epoch 38/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.5081 - mae: 0.5480 - val_loss: 0.4173 - val_mae: 0.4997\n",
      "Epoch 39/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.5026 - mae: 0.5451 - val_loss: 0.4140 - val_mae: 0.4974\n",
      "Epoch 40/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.4971 - mae: 0.5422 - val_loss: 0.4106 - val_mae: 0.4952\n",
      "Epoch 41/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.4916 - mae: 0.5393 - val_loss: 0.4073 - val_mae: 0.4930\n",
      "Epoch 42/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.4861 - mae: 0.5364 - val_loss: 0.4039 - val_mae: 0.4908\n",
      "Epoch 43/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.4806 - mae: 0.5335 - val_loss: 0.4006 - val_mae: 0.4885\n",
      "Epoch 44/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - loss: 0.4750 - mae: 0.5306 - val_loss: 0.3972 - val_mae: 0.4863\n",
      "Epoch 45/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.4694 - mae: 0.5277 - val_loss: 0.3938 - val_mae: 0.4840\n",
      "Epoch 46/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.4638 - mae: 0.5247 - val_loss: 0.3903 - val_mae: 0.4817\n",
      "Epoch 47/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.4582 - mae: 0.5217 - val_loss: 0.3869 - val_mae: 0.4794\n",
      "Epoch 48/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.4526 - mae: 0.5188 - val_loss: 0.3834 - val_mae: 0.4771\n",
      "Epoch 49/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.4469 - mae: 0.5158 - val_loss: 0.3798 - val_mae: 0.4747\n",
      "Epoch 50/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.4412 - mae: 0.5128 - val_loss: 0.3763 - val_mae: 0.4722\n",
      "Epoch 51/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.4354 - mae: 0.5097 - val_loss: 0.3727 - val_mae: 0.4697\n",
      "Epoch 52/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.4296 - mae: 0.5067 - val_loss: 0.3690 - val_mae: 0.4672\n",
      "Epoch 53/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.4238 - mae: 0.5036 - val_loss: 0.3653 - val_mae: 0.4647\n",
      "Epoch 54/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.4179 - mae: 0.5005 - val_loss: 0.3615 - val_mae: 0.4621\n",
      "Epoch 55/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.4119 - mae: 0.4973 - val_loss: 0.3577 - val_mae: 0.4594\n",
      "Epoch 56/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.4060 - mae: 0.4941 - val_loss: 0.3539 - val_mae: 0.4567\n",
      "Epoch 57/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.3999 - mae: 0.4909 - val_loss: 0.3500 - val_mae: 0.4540\n",
      "Epoch 58/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.3938 - mae: 0.4877 - val_loss: 0.3460 - val_mae: 0.4512\n",
      "Epoch 59/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - loss: 0.3877 - mae: 0.4844 - val_loss: 0.3420 - val_mae: 0.4483\n",
      "Epoch 60/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.3815 - mae: 0.4811 - val_loss: 0.3379 - val_mae: 0.4454\n",
      "Epoch 61/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.3753 - mae: 0.4777 - val_loss: 0.3337 - val_mae: 0.4424\n",
      "Epoch 62/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.3690 - mae: 0.4744 - val_loss: 0.3295 - val_mae: 0.4393\n",
      "Epoch 63/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.3627 - mae: 0.4710 - val_loss: 0.3252 - val_mae: 0.4362\n",
      "Epoch 64/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.3564 - mae: 0.4675 - val_loss: 0.3208 - val_mae: 0.4330\n",
      "Epoch 65/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.3500 - mae: 0.4640 - val_loss: 0.3164 - val_mae: 0.4297\n",
      "Epoch 66/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.3436 - mae: 0.4605 - val_loss: 0.3119 - val_mae: 0.4264\n",
      "Epoch 67/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.3371 - mae: 0.4570 - val_loss: 0.3074 - val_mae: 0.4231\n",
      "Epoch 68/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.3307 - mae: 0.4534 - val_loss: 0.3028 - val_mae: 0.4197\n",
      "Epoch 69/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.3243 - mae: 0.4498 - val_loss: 0.2981 - val_mae: 0.4162\n",
      "Epoch 70/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.3176 - mae: 0.4461 - val_loss: 0.2935 - val_mae: 0.4127\n",
      "Epoch 71/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - loss: 0.3109 - mae: 0.4423 - val_loss: 0.2887 - val_mae: 0.4091\n",
      "Epoch 72/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.3042 - mae: 0.4385 - val_loss: 0.2839 - val_mae: 0.4054\n",
      "Epoch 73/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.2974 - mae: 0.4347 - val_loss: 0.2791 - val_mae: 0.4016\n",
      "Epoch 74/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.2907 - mae: 0.4308 - val_loss: 0.2742 - val_mae: 0.3978\n",
      "Epoch 75/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.2841 - mae: 0.4269 - val_loss: 0.2694 - val_mae: 0.3939\n",
      "Epoch 76/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.2774 - mae: 0.4230 - val_loss: 0.2645 - val_mae: 0.3900\n",
      "Epoch 77/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.2709 - mae: 0.4191 - val_loss: 0.2596 - val_mae: 0.3860\n",
      "Epoch 78/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.2645 - mae: 0.4152 - val_loss: 0.2547 - val_mae: 0.3819\n",
      "Epoch 79/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.2583 - mae: 0.4114 - val_loss: 0.2498 - val_mae: 0.3779\n",
      "Epoch 80/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - loss: 0.2522 - mae: 0.4076 - val_loss: 0.2449 - val_mae: 0.3738\n",
      "Epoch 81/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.2462 - mae: 0.4038 - val_loss: 0.2401 - val_mae: 0.3696\n",
      "Epoch 82/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.2403 - mae: 0.4000 - val_loss: 0.2353 - val_mae: 0.3654\n",
      "Epoch 83/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.2345 - mae: 0.3962 - val_loss: 0.2305 - val_mae: 0.3613\n",
      "Epoch 84/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.2289 - mae: 0.3925 - val_loss: 0.2256 - val_mae: 0.3571\n",
      "Epoch 85/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.2234 - mae: 0.3888 - val_loss: 0.2207 - val_mae: 0.3540\n",
      "Epoch 86/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.2180 - mae: 0.3854 - val_loss: 0.2157 - val_mae: 0.3508\n",
      "Epoch 87/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.2127 - mae: 0.3820 - val_loss: 0.2106 - val_mae: 0.3475\n",
      "Epoch 88/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.2076 - mae: 0.3786 - val_loss: 0.2057 - val_mae: 0.3442\n",
      "Epoch 89/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.2025 - mae: 0.3752 - val_loss: 0.2007 - val_mae: 0.3408\n",
      "Epoch 90/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1977 - mae: 0.3719 - val_loss: 0.1959 - val_mae: 0.3374\n",
      "Epoch 91/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - loss: 0.1931 - mae: 0.3686 - val_loss: 0.1913 - val_mae: 0.3340\n",
      "Epoch 92/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1885 - mae: 0.3654 - val_loss: 0.1869 - val_mae: 0.3307\n",
      "Epoch 93/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1842 - mae: 0.3621 - val_loss: 0.1826 - val_mae: 0.3274\n",
      "Epoch 94/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.1799 - mae: 0.3589 - val_loss: 0.1786 - val_mae: 0.3242\n",
      "Epoch 95/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1758 - mae: 0.3556 - val_loss: 0.1747 - val_mae: 0.3211\n",
      "Epoch 96/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1719 - mae: 0.3524 - val_loss: 0.1710 - val_mae: 0.3180\n",
      "Epoch 97/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.1681 - mae: 0.3492 - val_loss: 0.1675 - val_mae: 0.3150\n",
      "Epoch 98/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1644 - mae: 0.3461 - val_loss: 0.1641 - val_mae: 0.3121\n",
      "Epoch 99/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.1610 - mae: 0.3430 - val_loss: 0.1608 - val_mae: 0.3091\n",
      "Epoch 100/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1577 - mae: 0.3400 - val_loss: 0.1576 - val_mae: 0.3061\n",
      "Epoch 101/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - loss: 0.1545 - mae: 0.3371 - val_loss: 0.1544 - val_mae: 0.3031\n",
      "Epoch 102/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.1514 - mae: 0.3342 - val_loss: 0.1512 - val_mae: 0.3000\n",
      "Epoch 103/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.1485 - mae: 0.3314 - val_loss: 0.1482 - val_mae: 0.2970\n",
      "Epoch 104/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.1458 - mae: 0.3287 - val_loss: 0.1453 - val_mae: 0.2939\n",
      "Epoch 105/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1431 - mae: 0.3260 - val_loss: 0.1424 - val_mae: 0.2908\n",
      "Epoch 106/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1405 - mae: 0.3233 - val_loss: 0.1397 - val_mae: 0.2878\n",
      "Epoch 107/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.1381 - mae: 0.3207 - val_loss: 0.1372 - val_mae: 0.2849\n",
      "Epoch 108/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.1358 - mae: 0.3181 - val_loss: 0.1349 - val_mae: 0.2822\n",
      "Epoch 109/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1336 - mae: 0.3155 - val_loss: 0.1327 - val_mae: 0.2796\n",
      "Epoch 110/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - loss: 0.1314 - mae: 0.3130 - val_loss: 0.1307 - val_mae: 0.2771\n",
      "Epoch 111/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1293 - mae: 0.3104 - val_loss: 0.1289 - val_mae: 0.2747\n",
      "Epoch 112/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1273 - mae: 0.3079 - val_loss: 0.1272 - val_mae: 0.2724\n",
      "Epoch 113/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1255 - mae: 0.3055 - val_loss: 0.1254 - val_mae: 0.2701\n",
      "Epoch 114/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.1237 - mae: 0.3032 - val_loss: 0.1237 - val_mae: 0.2678\n",
      "Epoch 115/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1220 - mae: 0.3010 - val_loss: 0.1221 - val_mae: 0.2655\n",
      "Epoch 116/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1204 - mae: 0.2988 - val_loss: 0.1205 - val_mae: 0.2632\n",
      "Epoch 117/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.1188 - mae: 0.2966 - val_loss: 0.1189 - val_mae: 0.2610\n",
      "Epoch 118/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - loss: 0.1173 - mae: 0.2946 - val_loss: 0.1174 - val_mae: 0.2589\n",
      "Epoch 119/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.1158 - mae: 0.2925 - val_loss: 0.1159 - val_mae: 0.2567\n",
      "Epoch 120/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1144 - mae: 0.2905 - val_loss: 0.1145 - val_mae: 0.2548\n",
      "Epoch 121/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.1131 - mae: 0.2886 - val_loss: 0.1133 - val_mae: 0.2529\n",
      "Epoch 122/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1118 - mae: 0.2867 - val_loss: 0.1121 - val_mae: 0.2511\n",
      "Epoch 123/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1105 - mae: 0.2849 - val_loss: 0.1109 - val_mae: 0.2493\n",
      "Epoch 124/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.1093 - mae: 0.2831 - val_loss: 0.1097 - val_mae: 0.2475\n",
      "Epoch 125/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1082 - mae: 0.2813 - val_loss: 0.1085 - val_mae: 0.2458\n",
      "Epoch 126/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - loss: 0.1070 - mae: 0.2796 - val_loss: 0.1075 - val_mae: 0.2448\n",
      "Epoch 127/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.1059 - mae: 0.2783 - val_loss: 0.1065 - val_mae: 0.2440\n",
      "Epoch 128/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.1049 - mae: 0.2771 - val_loss: 0.1055 - val_mae: 0.2431\n",
      "Epoch 129/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1039 - mae: 0.2760 - val_loss: 0.1045 - val_mae: 0.2423\n",
      "Epoch 130/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.1030 - mae: 0.2748 - val_loss: 0.1036 - val_mae: 0.2414\n",
      "Epoch 131/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1020 - mae: 0.2736 - val_loss: 0.1027 - val_mae: 0.2406\n",
      "Epoch 132/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.1011 - mae: 0.2724 - val_loss: 0.1019 - val_mae: 0.2398\n",
      "Epoch 133/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - loss: 0.1002 - mae: 0.2712 - val_loss: 0.1012 - val_mae: 0.2390\n",
      "Epoch 134/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.0993 - mae: 0.2700 - val_loss: 0.1005 - val_mae: 0.2383\n",
      "Epoch 135/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.0985 - mae: 0.2688 - val_loss: 0.0998 - val_mae: 0.2375\n",
      "Epoch 136/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.0976 - mae: 0.2676 - val_loss: 0.0990 - val_mae: 0.2368\n",
      "Epoch 137/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.0969 - mae: 0.2665 - val_loss: 0.0984 - val_mae: 0.2360\n",
      "Epoch 138/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.0961 - mae: 0.2654 - val_loss: 0.0977 - val_mae: 0.2353\n",
      "Epoch 139/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.0953 - mae: 0.2643 - val_loss: 0.0970 - val_mae: 0.2346\n",
      "Epoch 140/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - loss: 0.0946 - mae: 0.2632 - val_loss: 0.0963 - val_mae: 0.2338\n",
      "Epoch 141/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.0939 - mae: 0.2621 - val_loss: 0.0957 - val_mae: 0.2331\n",
      "Epoch 142/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.0932 - mae: 0.2611 - val_loss: 0.0950 - val_mae: 0.2323\n",
      "Epoch 143/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.0925 - mae: 0.2600 - val_loss: 0.0943 - val_mae: 0.2315\n",
      "Epoch 144/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.0918 - mae: 0.2589 - val_loss: 0.0937 - val_mae: 0.2307\n",
      "Epoch 145/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.0911 - mae: 0.2578 - val_loss: 0.0930 - val_mae: 0.2299\n",
      "Epoch 146/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.0904 - mae: 0.2567 - val_loss: 0.0924 - val_mae: 0.2290\n",
      "Epoch 147/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - loss: 0.0897 - mae: 0.2556 - val_loss: 0.0918 - val_mae: 0.2282\n",
      "Epoch 148/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.0891 - mae: 0.2545 - val_loss: 0.0912 - val_mae: 0.2274\n",
      "Epoch 149/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.0884 - mae: 0.2534 - val_loss: 0.0906 - val_mae: 0.2266\n",
      "Epoch 150/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.0878 - mae: 0.2523 - val_loss: 0.0901 - val_mae: 0.2259\n",
      "Epoch 151/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.0872 - mae: 0.2512 - val_loss: 0.0897 - val_mae: 0.2251\n",
      "Epoch 152/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.0865 - mae: 0.2500 - val_loss: 0.0892 - val_mae: 0.2244\n",
      "Epoch 153/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - loss: 0.0859 - mae: 0.2489 - val_loss: 0.0887 - val_mae: 0.2237\n",
      "Epoch 154/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.0853 - mae: 0.2478 - val_loss: 0.0882 - val_mae: 0.2229\n",
      "Epoch 155/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.0847 - mae: 0.2467 - val_loss: 0.0878 - val_mae: 0.2222\n",
      "Epoch 156/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.0842 - mae: 0.2456 - val_loss: 0.0873 - val_mae: 0.2214\n",
      "Epoch 157/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.0836 - mae: 0.2446 - val_loss: 0.0869 - val_mae: 0.2207\n",
      "Epoch 158/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.0830 - mae: 0.2435 - val_loss: 0.0864 - val_mae: 0.2200\n",
      "Epoch 159/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - loss: 0.0825 - mae: 0.2425 - val_loss: 0.0860 - val_mae: 0.2193\n",
      "Epoch 160/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.0819 - mae: 0.2414 - val_loss: 0.0856 - val_mae: 0.2186\n",
      "Epoch 161/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.0814 - mae: 0.2404 - val_loss: 0.0852 - val_mae: 0.2179\n",
      "Epoch 162/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.0809 - mae: 0.2394 - val_loss: 0.0847 - val_mae: 0.2172\n",
      "Epoch 163/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.0804 - mae: 0.2384 - val_loss: 0.0843 - val_mae: 0.2165\n",
      "Epoch 164/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.0799 - mae: 0.2374 - val_loss: 0.0839 - val_mae: 0.2159\n",
      "Epoch 165/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - loss: 0.0794 - mae: 0.2365 - val_loss: 0.0835 - val_mae: 0.2152\n",
      "Epoch 166/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.0789 - mae: 0.2355 - val_loss: 0.0832 - val_mae: 0.2146\n",
      "Epoch 167/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.0784 - mae: 0.2346 - val_loss: 0.0828 - val_mae: 0.2139\n",
      "Epoch 168/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.0779 - mae: 0.2336 - val_loss: 0.0825 - val_mae: 0.2133\n",
      "Epoch 169/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.0775 - mae: 0.2327 - val_loss: 0.0821 - val_mae: 0.2127\n",
      "Epoch 170/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.0770 - mae: 0.2318 - val_loss: 0.0818 - val_mae: 0.2121\n",
      "Epoch 171/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - loss: 0.0766 - mae: 0.2309 - val_loss: 0.0815 - val_mae: 0.2115\n",
      "Epoch 172/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.0761 - mae: 0.2300 - val_loss: 0.0812 - val_mae: 0.2110\n",
      "Epoch 173/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.0757 - mae: 0.2291 - val_loss: 0.0810 - val_mae: 0.2105\n",
      "Epoch 174/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.0753 - mae: 0.2282 - val_loss: 0.0807 - val_mae: 0.2101\n",
      "Epoch 175/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.0748 - mae: 0.2274 - val_loss: 0.0805 - val_mae: 0.2098\n",
      "Epoch 176/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - loss: 0.0744 - mae: 0.2266 - val_loss: 0.0802 - val_mae: 0.2095\n",
      "Epoch 177/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.0740 - mae: 0.2259 - val_loss: 0.0800 - val_mae: 0.2091\n",
      "Epoch 178/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.0736 - mae: 0.2251 - val_loss: 0.0797 - val_mae: 0.2088\n",
      "Epoch 179/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.0732 - mae: 0.2244 - val_loss: 0.0795 - val_mae: 0.2085\n",
      "Epoch 180/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.0729 - mae: 0.2238 - val_loss: 0.0792 - val_mae: 0.2081\n",
      "Epoch 181/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - loss: 0.0725 - mae: 0.2231 - val_loss: 0.0791 - val_mae: 0.2079\n",
      "Epoch 182/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.0721 - mae: 0.2224 - val_loss: 0.0789 - val_mae: 0.2076\n",
      "Epoch 183/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.0718 - mae: 0.2217 - val_loss: 0.0788 - val_mae: 0.2073\n",
      "Epoch 184/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.0714 - mae: 0.2210 - val_loss: 0.0786 - val_mae: 0.2070\n",
      "Epoch 185/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.0711 - mae: 0.2203 - val_loss: 0.0784 - val_mae: 0.2067\n",
      "Epoch 186/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - loss: 0.0707 - mae: 0.2197 - val_loss: 0.0782 - val_mae: 0.2065\n",
      "Epoch 187/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.0704 - mae: 0.2190 - val_loss: 0.0781 - val_mae: 0.2062\n",
      "Epoch 188/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.0700 - mae: 0.2184 - val_loss: 0.0779 - val_mae: 0.2059\n",
      "Epoch 189/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.0697 - mae: 0.2177 - val_loss: 0.0777 - val_mae: 0.2056\n",
      "Epoch 190/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.0694 - mae: 0.2171 - val_loss: 0.0776 - val_mae: 0.2053\n",
      "Epoch 191/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - loss: 0.0691 - mae: 0.2165 - val_loss: 0.0774 - val_mae: 0.2050\n",
      "Epoch 192/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.0687 - mae: 0.2159 - val_loss: 0.0772 - val_mae: 0.2048\n",
      "Epoch 193/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.0684 - mae: 0.2153 - val_loss: 0.0771 - val_mae: 0.2045\n",
      "Epoch 194/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.0681 - mae: 0.2147 - val_loss: 0.0769 - val_mae: 0.2042\n",
      "Epoch 195/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.0678 - mae: 0.2141 - val_loss: 0.0768 - val_mae: 0.2040\n",
      "Epoch 196/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 0.0675 - mae: 0.2135 - val_loss: 0.0766 - val_mae: 0.2037\n",
      "Epoch 197/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 0.0672 - mae: 0.2129 - val_loss: 0.0765 - val_mae: 0.2035\n",
      "Epoch 198/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.0669 - mae: 0.2123 - val_loss: 0.0765 - val_mae: 0.2033\n",
      "Epoch 199/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.0667 - mae: 0.2117 - val_loss: 0.0764 - val_mae: 0.2031\n",
      "Epoch 200/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.0664 - mae: 0.2111 - val_loss: 0.0763 - val_mae: 0.2029\n",
      "Epoch 1/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 504ms/step - loss: 0.8544 - mae: 0.7559 - val_loss: 0.3958 - val_mae: 0.4728\n",
      "Epoch 2/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.8437 - mae: 0.7496 - val_loss: 0.3905 - val_mae: 0.4679\n",
      "Epoch 3/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.8331 - mae: 0.7433 - val_loss: 0.3853 - val_mae: 0.4629\n",
      "Epoch 4/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.8224 - mae: 0.7369 - val_loss: 0.3801 - val_mae: 0.4579\n",
      "Epoch 5/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.8119 - mae: 0.7303 - val_loss: 0.3750 - val_mae: 0.4529\n",
      "Epoch 6/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.8013 - mae: 0.7238 - val_loss: 0.3698 - val_mae: 0.4478\n",
      "Epoch 7/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.7909 - mae: 0.7171 - val_loss: 0.3647 - val_mae: 0.4427\n",
      "Epoch 8/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.7805 - mae: 0.7104 - val_loss: 0.3596 - val_mae: 0.4376\n",
      "Epoch 9/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 0.7702 - mae: 0.7036 - val_loss: 0.3545 - val_mae: 0.4326\n",
      "Epoch 10/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 0.7600 - mae: 0.6967 - val_loss: 0.3494 - val_mae: 0.4287\n",
      "Epoch 11/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - loss: 0.7499 - mae: 0.6898 - val_loss: 0.3444 - val_mae: 0.4246\n",
      "Epoch 12/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - loss: 0.7399 - mae: 0.6828 - val_loss: 0.3393 - val_mae: 0.4206\n",
      "Epoch 13/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 0.7300 - mae: 0.6758 - val_loss: 0.3343 - val_mae: 0.4164\n",
      "Epoch 14/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.7203 - mae: 0.6687 - val_loss: 0.3293 - val_mae: 0.4122\n",
      "Epoch 15/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.7106 - mae: 0.6616 - val_loss: 0.3244 - val_mae: 0.4079\n",
      "Epoch 16/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - loss: 0.7011 - mae: 0.6544 - val_loss: 0.3194 - val_mae: 0.4036\n",
      "Epoch 17/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 337ms/step - loss: 0.6917 - mae: 0.6472 - val_loss: 0.3145 - val_mae: 0.3992\n",
      "Epoch 18/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - loss: 0.6825 - mae: 0.6403 - val_loss: 0.3096 - val_mae: 0.3948\n",
      "Epoch 19/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - loss: 0.6733 - mae: 0.6341 - val_loss: 0.3048 - val_mae: 0.3903\n",
      "Epoch 20/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.6644 - mae: 0.6278 - val_loss: 0.3000 - val_mae: 0.3858\n",
      "Epoch 21/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - loss: 0.6555 - mae: 0.6216 - val_loss: 0.2952 - val_mae: 0.3813\n",
      "Epoch 22/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.6469 - mae: 0.6153 - val_loss: 0.2905 - val_mae: 0.3767\n",
      "Epoch 23/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.6383 - mae: 0.6090 - val_loss: 0.2859 - val_mae: 0.3722\n",
      "Epoch 24/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.6300 - mae: 0.6027 - val_loss: 0.2812 - val_mae: 0.3676\n",
      "Epoch 25/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.6218 - mae: 0.5964 - val_loss: 0.2767 - val_mae: 0.3630\n",
      "Epoch 26/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.6137 - mae: 0.5913 - val_loss: 0.2721 - val_mae: 0.3584\n",
      "Epoch 27/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.6058 - mae: 0.5866 - val_loss: 0.2677 - val_mae: 0.3537\n",
      "Epoch 28/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.5980 - mae: 0.5820 - val_loss: 0.2632 - val_mae: 0.3491\n",
      "Epoch 29/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - loss: 0.5905 - mae: 0.5773 - val_loss: 0.2589 - val_mae: 0.3444\n",
      "Epoch 30/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.5830 - mae: 0.5726 - val_loss: 0.2545 - val_mae: 0.3398\n",
      "Epoch 31/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.5757 - mae: 0.5679 - val_loss: 0.2503 - val_mae: 0.3351\n",
      "Epoch 32/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.5686 - mae: 0.5633 - val_loss: 0.2461 - val_mae: 0.3304\n",
      "Epoch 33/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.5616 - mae: 0.5587 - val_loss: 0.2420 - val_mae: 0.3258\n",
      "Epoch 34/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.5548 - mae: 0.5540 - val_loss: 0.2379 - val_mae: 0.3211\n",
      "Epoch 35/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.5480 - mae: 0.5495 - val_loss: 0.2339 - val_mae: 0.3165\n",
      "Epoch 36/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.5415 - mae: 0.5456 - val_loss: 0.2299 - val_mae: 0.3118\n",
      "Epoch 37/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.5350 - mae: 0.5418 - val_loss: 0.2260 - val_mae: 0.3072\n",
      "Epoch 38/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - loss: 0.5287 - mae: 0.5379 - val_loss: 0.2222 - val_mae: 0.3026\n",
      "Epoch 39/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.5225 - mae: 0.5341 - val_loss: 0.2184 - val_mae: 0.2980\n",
      "Epoch 40/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.5164 - mae: 0.5302 - val_loss: 0.2147 - val_mae: 0.2939\n",
      "Epoch 41/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.5105 - mae: 0.5264 - val_loss: 0.2111 - val_mae: 0.2901\n",
      "Epoch 42/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.5046 - mae: 0.5225 - val_loss: 0.2075 - val_mae: 0.2863\n",
      "Epoch 43/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.4988 - mae: 0.5186 - val_loss: 0.2040 - val_mae: 0.2826\n",
      "Epoch 44/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.4931 - mae: 0.5148 - val_loss: 0.2006 - val_mae: 0.2789\n",
      "Epoch 45/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.4875 - mae: 0.5109 - val_loss: 0.1972 - val_mae: 0.2757\n",
      "Epoch 46/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.4820 - mae: 0.5078 - val_loss: 0.1939 - val_mae: 0.2732\n",
      "Epoch 47/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - loss: 0.4765 - mae: 0.5057 - val_loss: 0.1907 - val_mae: 0.2708\n",
      "Epoch 48/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.4711 - mae: 0.5036 - val_loss: 0.1875 - val_mae: 0.2687\n",
      "Epoch 49/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.4658 - mae: 0.5015 - val_loss: 0.1845 - val_mae: 0.2669\n",
      "Epoch 50/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.4605 - mae: 0.4997 - val_loss: 0.1814 - val_mae: 0.2651\n",
      "Epoch 51/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.4553 - mae: 0.4978 - val_loss: 0.1785 - val_mae: 0.2640\n",
      "Epoch 52/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.4501 - mae: 0.4964 - val_loss: 0.1756 - val_mae: 0.2630\n",
      "Epoch 53/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.4449 - mae: 0.4950 - val_loss: 0.1728 - val_mae: 0.2619\n",
      "Epoch 54/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.4398 - mae: 0.4935 - val_loss: 0.1701 - val_mae: 0.2607\n",
      "Epoch 55/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.4347 - mae: 0.4919 - val_loss: 0.1674 - val_mae: 0.2596\n",
      "Epoch 56/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - loss: 0.4297 - mae: 0.4903 - val_loss: 0.1648 - val_mae: 0.2585\n",
      "Epoch 57/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.4246 - mae: 0.4886 - val_loss: 0.1623 - val_mae: 0.2573\n",
      "Epoch 58/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.4196 - mae: 0.4869 - val_loss: 0.1599 - val_mae: 0.2561\n",
      "Epoch 59/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.4146 - mae: 0.4852 - val_loss: 0.1575 - val_mae: 0.2549\n",
      "Epoch 60/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.4096 - mae: 0.4834 - val_loss: 0.1552 - val_mae: 0.2537\n",
      "Epoch 61/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.4047 - mae: 0.4815 - val_loss: 0.1529 - val_mae: 0.2525\n",
      "Epoch 62/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.3997 - mae: 0.4796 - val_loss: 0.1508 - val_mae: 0.2512\n",
      "Epoch 63/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.3948 - mae: 0.4777 - val_loss: 0.1487 - val_mae: 0.2500\n",
      "Epoch 64/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - loss: 0.3898 - mae: 0.4757 - val_loss: 0.1466 - val_mae: 0.2487\n",
      "Epoch 65/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.3849 - mae: 0.4737 - val_loss: 0.1447 - val_mae: 0.2475\n",
      "Epoch 66/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.3800 - mae: 0.4716 - val_loss: 0.1428 - val_mae: 0.2462\n",
      "Epoch 67/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.3751 - mae: 0.4695 - val_loss: 0.1409 - val_mae: 0.2449\n",
      "Epoch 68/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.3702 - mae: 0.4674 - val_loss: 0.1392 - val_mae: 0.2436\n",
      "Epoch 69/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.3654 - mae: 0.4652 - val_loss: 0.1375 - val_mae: 0.2423\n",
      "Epoch 70/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.3605 - mae: 0.4630 - val_loss: 0.1358 - val_mae: 0.2410\n",
      "Epoch 71/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.3557 - mae: 0.4608 - val_loss: 0.1343 - val_mae: 0.2398\n",
      "Epoch 72/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - loss: 0.3509 - mae: 0.4585 - val_loss: 0.1328 - val_mae: 0.2385\n",
      "Epoch 73/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.3462 - mae: 0.4562 - val_loss: 0.1313 - val_mae: 0.2371\n",
      "Epoch 74/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.3415 - mae: 0.4539 - val_loss: 0.1300 - val_mae: 0.2358\n",
      "Epoch 75/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.3368 - mae: 0.4515 - val_loss: 0.1286 - val_mae: 0.2345\n",
      "Epoch 76/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.3321 - mae: 0.4491 - val_loss: 0.1274 - val_mae: 0.2332\n",
      "Epoch 77/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.3275 - mae: 0.4467 - val_loss: 0.1262 - val_mae: 0.2319\n",
      "Epoch 78/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.3230 - mae: 0.4443 - val_loss: 0.1251 - val_mae: 0.2309\n",
      "Epoch 79/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.3185 - mae: 0.4420 - val_loss: 0.1240 - val_mae: 0.2305\n",
      "Epoch 80/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - loss: 0.3140 - mae: 0.4401 - val_loss: 0.1230 - val_mae: 0.2300\n",
      "Epoch 81/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.3097 - mae: 0.4381 - val_loss: 0.1220 - val_mae: 0.2295\n",
      "Epoch 82/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.3053 - mae: 0.4361 - val_loss: 0.1211 - val_mae: 0.2290\n",
      "Epoch 83/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.3011 - mae: 0.4341 - val_loss: 0.1203 - val_mae: 0.2285\n",
      "Epoch 84/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.2969 - mae: 0.4321 - val_loss: 0.1195 - val_mae: 0.2283\n",
      "Epoch 85/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.2928 - mae: 0.4301 - val_loss: 0.1187 - val_mae: 0.2281\n",
      "Epoch 86/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.2888 - mae: 0.4280 - val_loss: 0.1180 - val_mae: 0.2279\n",
      "Epoch 87/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - loss: 0.2849 - mae: 0.4260 - val_loss: 0.1174 - val_mae: 0.2277\n",
      "Epoch 88/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.2810 - mae: 0.4240 - val_loss: 0.1168 - val_mae: 0.2284\n",
      "Epoch 89/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.2772 - mae: 0.4219 - val_loss: 0.1162 - val_mae: 0.2294\n",
      "Epoch 90/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.2736 - mae: 0.4199 - val_loss: 0.1157 - val_mae: 0.2303\n",
      "Epoch 91/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.2700 - mae: 0.4178 - val_loss: 0.1152 - val_mae: 0.2312\n",
      "Epoch 92/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.2665 - mae: 0.4158 - val_loss: 0.1148 - val_mae: 0.2321\n",
      "Epoch 93/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.2631 - mae: 0.4138 - val_loss: 0.1144 - val_mae: 0.2330\n",
      "Epoch 94/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - loss: 0.2598 - mae: 0.4118 - val_loss: 0.1141 - val_mae: 0.2338\n",
      "Epoch 95/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.2566 - mae: 0.4098 - val_loss: 0.1138 - val_mae: 0.2346\n",
      "Epoch 96/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.2535 - mae: 0.4078 - val_loss: 0.1135 - val_mae: 0.2353\n",
      "Epoch 97/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.2505 - mae: 0.4058 - val_loss: 0.1133 - val_mae: 0.2360\n",
      "Epoch 98/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.2475 - mae: 0.4038 - val_loss: 0.1130 - val_mae: 0.2367\n",
      "Epoch 99/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.2447 - mae: 0.4019 - val_loss: 0.1129 - val_mae: 0.2374\n",
      "Epoch 100/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.2420 - mae: 0.4000 - val_loss: 0.1127 - val_mae: 0.2380\n",
      "Epoch 101/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 0.2394 - mae: 0.3981 - val_loss: 0.1126 - val_mae: 0.2386\n",
      "Epoch 102/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.2368 - mae: 0.3962 - val_loss: 0.1125 - val_mae: 0.2391\n",
      "Epoch 103/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.2344 - mae: 0.3943 - val_loss: 0.1124 - val_mae: 0.2396\n",
      "Epoch 104/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.2320 - mae: 0.3926 - val_loss: 0.1123 - val_mae: 0.2401\n",
      "Epoch 105/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.2297 - mae: 0.3911 - val_loss: 0.1123 - val_mae: 0.2406\n",
      "Epoch 106/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.2275 - mae: 0.3897 - val_loss: 0.1123 - val_mae: 0.2411\n",
      "Epoch 107/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.2254 - mae: 0.3882 - val_loss: 0.1123 - val_mae: 0.2415\n",
      "Epoch 108/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - loss: 0.2234 - mae: 0.3868 - val_loss: 0.1123 - val_mae: 0.2419\n",
      "Epoch 109/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.2214 - mae: 0.3854 - val_loss: 0.1123 - val_mae: 0.2422\n",
      "Epoch 110/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 0.2195 - mae: 0.3840 - val_loss: 0.1124 - val_mae: 0.2426\n",
      "Epoch 111/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.2177 - mae: 0.3825 - val_loss: 0.1124 - val_mae: 0.2429\n",
      "Epoch 112/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.2159 - mae: 0.3812 - val_loss: 0.1125 - val_mae: 0.2432\n",
      "Epoch 113/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.2142 - mae: 0.3798 - val_loss: 0.1125 - val_mae: 0.2434\n",
      "Epoch 114/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - loss: 0.2126 - mae: 0.3784 - val_loss: 0.1126 - val_mae: 0.2437\n",
      "Epoch 115/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.2110 - mae: 0.3770 - val_loss: 0.1127 - val_mae: 0.2439\n",
      "Epoch 116/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.2095 - mae: 0.3757 - val_loss: 0.1128 - val_mae: 0.2441\n",
      "Epoch 117/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.2080 - mae: 0.3743 - val_loss: 0.1129 - val_mae: 0.2443\n",
      "Epoch 118/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.2066 - mae: 0.3730 - val_loss: 0.1130 - val_mae: 0.2444\n",
      "Epoch 119/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.2052 - mae: 0.3717 - val_loss: 0.1131 - val_mae: 0.2446\n",
      "Epoch 120/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - loss: 0.2039 - mae: 0.3704 - val_loss: 0.1131 - val_mae: 0.2447\n",
      "Epoch 121/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.2026 - mae: 0.3692 - val_loss: 0.1132 - val_mae: 0.2449\n",
      "Epoch 122/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.2014 - mae: 0.3680 - val_loss: 0.1133 - val_mae: 0.2451\n",
      "Epoch 123/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.2002 - mae: 0.3669 - val_loss: 0.1134 - val_mae: 0.2453\n",
      "Epoch 124/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1990 - mae: 0.3657 - val_loss: 0.1135 - val_mae: 0.2455\n",
      "Epoch 125/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1979 - mae: 0.3645 - val_loss: 0.1136 - val_mae: 0.2457\n",
      "Epoch 126/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.1968 - mae: 0.3634 - val_loss: 0.1137 - val_mae: 0.2458\n",
      "Epoch 127/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1957 - mae: 0.3622 - val_loss: 0.1138 - val_mae: 0.2460\n",
      "Epoch 128/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.1946 - mae: 0.3611 - val_loss: 0.1139 - val_mae: 0.2461\n",
      "Epoch 129/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.1936 - mae: 0.3600 - val_loss: 0.1140 - val_mae: 0.2462\n",
      "Epoch 130/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1926 - mae: 0.3588 - val_loss: 0.1140 - val_mae: 0.2463\n",
      "Epoch 131/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.1917 - mae: 0.3577 - val_loss: 0.1141 - val_mae: 0.2464\n",
      "Epoch 132/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 0.1907 - mae: 0.3566 - val_loss: 0.1141 - val_mae: 0.2464\n",
      "Epoch 133/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1898 - mae: 0.3555 - val_loss: 0.1142 - val_mae: 0.2464\n",
      "Epoch 134/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1889 - mae: 0.3544 - val_loss: 0.1142 - val_mae: 0.2465\n",
      "Epoch 135/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.1880 - mae: 0.3534 - val_loss: 0.1143 - val_mae: 0.2465\n",
      "Epoch 136/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1871 - mae: 0.3523 - val_loss: 0.1143 - val_mae: 0.2465\n",
      "Epoch 137/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.1862 - mae: 0.3512 - val_loss: 0.1144 - val_mae: 0.2464\n",
      "Epoch 138/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - loss: 0.1854 - mae: 0.3502 - val_loss: 0.1144 - val_mae: 0.2464\n",
      "Epoch 139/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1846 - mae: 0.3492 - val_loss: 0.1144 - val_mae: 0.2463\n",
      "Epoch 140/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1838 - mae: 0.3481 - val_loss: 0.1144 - val_mae: 0.2463\n",
      "Epoch 141/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.1830 - mae: 0.3471 - val_loss: 0.1144 - val_mae: 0.2462\n",
      "Epoch 142/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1822 - mae: 0.3461 - val_loss: 0.1144 - val_mae: 0.2461\n",
      "Epoch 143/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.1814 - mae: 0.3451 - val_loss: 0.1144 - val_mae: 0.2460\n",
      "Epoch 144/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - loss: 0.1806 - mae: 0.3440 - val_loss: 0.1143 - val_mae: 0.2458\n",
      "Epoch 145/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1799 - mae: 0.3431 - val_loss: 0.1143 - val_mae: 0.2457\n",
      "Epoch 146/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1792 - mae: 0.3421 - val_loss: 0.1142 - val_mae: 0.2455\n",
      "Epoch 147/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1784 - mae: 0.3411 - val_loss: 0.1142 - val_mae: 0.2453\n",
      "Epoch 148/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1777 - mae: 0.3401 - val_loss: 0.1141 - val_mae: 0.2452\n",
      "Epoch 149/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.1770 - mae: 0.3391 - val_loss: 0.1141 - val_mae: 0.2450\n",
      "Epoch 150/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - loss: 0.1763 - mae: 0.3382 - val_loss: 0.1140 - val_mae: 0.2448\n",
      "Epoch 151/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1756 - mae: 0.3372 - val_loss: 0.1139 - val_mae: 0.2445\n",
      "Epoch 152/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.1749 - mae: 0.3363 - val_loss: 0.1138 - val_mae: 0.2443\n",
      "Epoch 153/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.1742 - mae: 0.3353 - val_loss: 0.1137 - val_mae: 0.2440\n",
      "Epoch 154/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.1736 - mae: 0.3344 - val_loss: 0.1136 - val_mae: 0.2438\n",
      "Epoch 155/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - loss: 0.1729 - mae: 0.3335 - val_loss: 0.1135 - val_mae: 0.2435\n",
      "Epoch 156/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.1722 - mae: 0.3325 - val_loss: 0.1134 - val_mae: 0.2432\n",
      "Epoch 157/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.1716 - mae: 0.3316 - val_loss: 0.1132 - val_mae: 0.2429\n",
      "Epoch 158/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1709 - mae: 0.3307 - val_loss: 0.1131 - val_mae: 0.2427\n",
      "Epoch 159/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1703 - mae: 0.3299 - val_loss: 0.1129 - val_mae: 0.2427\n",
      "Epoch 160/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 0.1697 - mae: 0.3291 - val_loss: 0.1128 - val_mae: 0.2426\n",
      "Epoch 161/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1691 - mae: 0.3284 - val_loss: 0.1126 - val_mae: 0.2425\n",
      "Epoch 162/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1684 - mae: 0.3277 - val_loss: 0.1125 - val_mae: 0.2424\n",
      "Epoch 163/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.1678 - mae: 0.3270 - val_loss: 0.1123 - val_mae: 0.2423\n",
      "Epoch 164/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1672 - mae: 0.3263 - val_loss: 0.1121 - val_mae: 0.2422\n",
      "Epoch 165/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - loss: 0.1666 - mae: 0.3257 - val_loss: 0.1119 - val_mae: 0.2421\n",
      "Epoch 166/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1660 - mae: 0.3252 - val_loss: 0.1117 - val_mae: 0.2419\n",
      "Epoch 167/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1654 - mae: 0.3246 - val_loss: 0.1115 - val_mae: 0.2417\n",
      "Epoch 168/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.1648 - mae: 0.3241 - val_loss: 0.1113 - val_mae: 0.2416\n",
      "Epoch 169/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1643 - mae: 0.3235 - val_loss: 0.1111 - val_mae: 0.2414\n",
      "Epoch 170/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - loss: 0.1637 - mae: 0.3230 - val_loss: 0.1108 - val_mae: 0.2412\n",
      "Epoch 171/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.1631 - mae: 0.3224 - val_loss: 0.1106 - val_mae: 0.2410\n",
      "Epoch 172/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.1625 - mae: 0.3219 - val_loss: 0.1104 - val_mae: 0.2409\n",
      "Epoch 173/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.1620 - mae: 0.3213 - val_loss: 0.1101 - val_mae: 0.2407\n",
      "Epoch 174/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.1614 - mae: 0.3208 - val_loss: 0.1099 - val_mae: 0.2405\n",
      "Epoch 175/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - loss: 0.1609 - mae: 0.3203 - val_loss: 0.1096 - val_mae: 0.2403\n",
      "Epoch 176/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.1603 - mae: 0.3197 - val_loss: 0.1094 - val_mae: 0.2401\n",
      "Epoch 177/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.1598 - mae: 0.3192 - val_loss: 0.1091 - val_mae: 0.2398\n",
      "Epoch 178/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.1592 - mae: 0.3187 - val_loss: 0.1088 - val_mae: 0.2396\n",
      "Epoch 179/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.1587 - mae: 0.3181 - val_loss: 0.1086 - val_mae: 0.2394\n",
      "Epoch 180/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - loss: 0.1582 - mae: 0.3176 - val_loss: 0.1083 - val_mae: 0.2391\n",
      "Epoch 181/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.1577 - mae: 0.3171 - val_loss: 0.1080 - val_mae: 0.2388\n",
      "Epoch 182/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.1571 - mae: 0.3166 - val_loss: 0.1077 - val_mae: 0.2386\n",
      "Epoch 183/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.1566 - mae: 0.3161 - val_loss: 0.1074 - val_mae: 0.2383\n",
      "Epoch 184/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.1561 - mae: 0.3155 - val_loss: 0.1071 - val_mae: 0.2380\n",
      "Epoch 185/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.1556 - mae: 0.3150 - val_loss: 0.1068 - val_mae: 0.2377\n",
      "Epoch 186/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - loss: 0.1551 - mae: 0.3145 - val_loss: 0.1065 - val_mae: 0.2374\n",
      "Epoch 187/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.1546 - mae: 0.3140 - val_loss: 0.1062 - val_mae: 0.2371\n",
      "Epoch 188/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.1541 - mae: 0.3135 - val_loss: 0.1059 - val_mae: 0.2368\n",
      "Epoch 189/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.1536 - mae: 0.3131 - val_loss: 0.1056 - val_mae: 0.2364\n",
      "Epoch 190/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.1531 - mae: 0.3126 - val_loss: 0.1053 - val_mae: 0.2361\n",
      "Epoch 191/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 0.1526 - mae: 0.3122 - val_loss: 0.1049 - val_mae: 0.2357\n",
      "Epoch 192/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 0.1521 - mae: 0.3117 - val_loss: 0.1046 - val_mae: 0.2354\n",
      "Epoch 193/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.1516 - mae: 0.3113 - val_loss: 0.1043 - val_mae: 0.2350\n",
      "Epoch 194/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.1511 - mae: 0.3108 - val_loss: 0.1040 - val_mae: 0.2347\n",
      "Epoch 195/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.1507 - mae: 0.3104 - val_loss: 0.1036 - val_mae: 0.2343\n",
      "Epoch 196/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.1502 - mae: 0.3099 - val_loss: 0.1033 - val_mae: 0.2339\n",
      "Epoch 197/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - loss: 0.1497 - mae: 0.3094 - val_loss: 0.1030 - val_mae: 0.2336\n",
      "Epoch 198/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.1492 - mae: 0.3090 - val_loss: 0.1026 - val_mae: 0.2332\n",
      "Epoch 199/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.1488 - mae: 0.3085 - val_loss: 0.1023 - val_mae: 0.2328\n",
      "Epoch 200/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.1483 - mae: 0.3081 - val_loss: 0.1019 - val_mae: 0.2324\n",
      "Epoch 1/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 540ms/step - loss: 0.7615 - mae: 0.7561 - val_loss: 0.4344 - val_mae: 0.6089\n",
      "Epoch 2/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.7534 - mae: 0.7512 - val_loss: 0.4253 - val_mae: 0.6026\n",
      "Epoch 3/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.7453 - mae: 0.7463 - val_loss: 0.4164 - val_mae: 0.5963\n",
      "Epoch 4/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.7372 - mae: 0.7414 - val_loss: 0.4075 - val_mae: 0.5899\n",
      "Epoch 5/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.7291 - mae: 0.7364 - val_loss: 0.3987 - val_mae: 0.5834\n",
      "Epoch 6/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.7211 - mae: 0.7314 - val_loss: 0.3900 - val_mae: 0.5769\n",
      "Epoch 7/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.7131 - mae: 0.7264 - val_loss: 0.3814 - val_mae: 0.5703\n",
      "Epoch 8/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.7052 - mae: 0.7213 - val_loss: 0.3729 - val_mae: 0.5637\n",
      "Epoch 9/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.6973 - mae: 0.7162 - val_loss: 0.3645 - val_mae: 0.5571\n",
      "Epoch 10/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.6895 - mae: 0.7111 - val_loss: 0.3562 - val_mae: 0.5504\n",
      "Epoch 11/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.6817 - mae: 0.7059 - val_loss: 0.3481 - val_mae: 0.5437\n",
      "Epoch 12/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.6740 - mae: 0.7007 - val_loss: 0.3401 - val_mae: 0.5369\n",
      "Epoch 13/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.6663 - mae: 0.6955 - val_loss: 0.3323 - val_mae: 0.5302\n",
      "Epoch 14/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.6587 - mae: 0.6903 - val_loss: 0.3246 - val_mae: 0.5234\n",
      "Epoch 15/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.6512 - mae: 0.6850 - val_loss: 0.3171 - val_mae: 0.5166\n",
      "Epoch 16/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.6437 - mae: 0.6797 - val_loss: 0.3097 - val_mae: 0.5098\n",
      "Epoch 17/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.6364 - mae: 0.6744 - val_loss: 0.3025 - val_mae: 0.5029\n",
      "Epoch 18/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - loss: 0.6291 - mae: 0.6692 - val_loss: 0.2954 - val_mae: 0.4961\n",
      "Epoch 19/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.6219 - mae: 0.6639 - val_loss: 0.2886 - val_mae: 0.4893\n",
      "Epoch 20/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.6148 - mae: 0.6585 - val_loss: 0.2819 - val_mae: 0.4825\n",
      "Epoch 21/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.6077 - mae: 0.6532 - val_loss: 0.2754 - val_mae: 0.4757\n",
      "Epoch 22/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.6008 - mae: 0.6479 - val_loss: 0.2690 - val_mae: 0.4689\n",
      "Epoch 23/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - loss: 0.5940 - mae: 0.6426 - val_loss: 0.2629 - val_mae: 0.4621\n",
      "Epoch 24/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 0.5872 - mae: 0.6373 - val_loss: 0.2569 - val_mae: 0.4554\n",
      "Epoch 25/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.5806 - mae: 0.6321 - val_loss: 0.2512 - val_mae: 0.4487\n",
      "Epoch 26/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.5741 - mae: 0.6268 - val_loss: 0.2456 - val_mae: 0.4420\n",
      "Epoch 27/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.5677 - mae: 0.6215 - val_loss: 0.2402 - val_mae: 0.4354\n",
      "Epoch 28/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.5614 - mae: 0.6163 - val_loss: 0.2350 - val_mae: 0.4288\n",
      "Epoch 29/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - loss: 0.5552 - mae: 0.6111 - val_loss: 0.2300 - val_mae: 0.4222\n",
      "Epoch 30/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.5492 - mae: 0.6059 - val_loss: 0.2252 - val_mae: 0.4157\n",
      "Epoch 31/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.5432 - mae: 0.6008 - val_loss: 0.2206 - val_mae: 0.4092\n",
      "Epoch 32/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.5374 - mae: 0.5957 - val_loss: 0.2162 - val_mae: 0.4028\n",
      "Epoch 33/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.5317 - mae: 0.5907 - val_loss: 0.2119 - val_mae: 0.3965\n",
      "Epoch 34/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.5261 - mae: 0.5863 - val_loss: 0.2078 - val_mae: 0.3902\n",
      "Epoch 35/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 0.5206 - mae: 0.5819 - val_loss: 0.2039 - val_mae: 0.3840\n",
      "Epoch 36/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.5153 - mae: 0.5775 - val_loss: 0.2002 - val_mae: 0.3778\n",
      "Epoch 37/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.5101 - mae: 0.5732 - val_loss: 0.1967 - val_mae: 0.3717\n",
      "Epoch 38/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.5050 - mae: 0.5689 - val_loss: 0.1933 - val_mae: 0.3657\n",
      "Epoch 39/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.5000 - mae: 0.5647 - val_loss: 0.1901 - val_mae: 0.3598\n",
      "Epoch 40/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.4952 - mae: 0.5605 - val_loss: 0.1870 - val_mae: 0.3539\n",
      "Epoch 41/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - loss: 0.4904 - mae: 0.5563 - val_loss: 0.1841 - val_mae: 0.3482\n",
      "Epoch 42/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.4858 - mae: 0.5522 - val_loss: 0.1814 - val_mae: 0.3425\n",
      "Epoch 43/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.4813 - mae: 0.5481 - val_loss: 0.1788 - val_mae: 0.3369\n",
      "Epoch 44/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.4769 - mae: 0.5441 - val_loss: 0.1763 - val_mae: 0.3313\n",
      "Epoch 45/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.4727 - mae: 0.5401 - val_loss: 0.1740 - val_mae: 0.3259\n",
      "Epoch 46/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - loss: 0.4685 - mae: 0.5362 - val_loss: 0.1718 - val_mae: 0.3206\n",
      "Epoch 47/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.4645 - mae: 0.5324 - val_loss: 0.1697 - val_mae: 0.3166\n",
      "Epoch 48/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.4606 - mae: 0.5296 - val_loss: 0.1678 - val_mae: 0.3135\n",
      "Epoch 49/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.4568 - mae: 0.5270 - val_loss: 0.1660 - val_mae: 0.3104\n",
      "Epoch 50/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.4530 - mae: 0.5245 - val_loss: 0.1643 - val_mae: 0.3073\n",
      "Epoch 51/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.4494 - mae: 0.5220 - val_loss: 0.1627 - val_mae: 0.3043\n",
      "Epoch 52/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - loss: 0.4459 - mae: 0.5196 - val_loss: 0.1612 - val_mae: 0.3013\n",
      "Epoch 53/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.4425 - mae: 0.5172 - val_loss: 0.1598 - val_mae: 0.2983\n",
      "Epoch 54/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.4392 - mae: 0.5148 - val_loss: 0.1584 - val_mae: 0.2955\n",
      "Epoch 55/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.4360 - mae: 0.5124 - val_loss: 0.1572 - val_mae: 0.2926\n",
      "Epoch 56/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.4328 - mae: 0.5101 - val_loss: 0.1561 - val_mae: 0.2898\n",
      "Epoch 57/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.4298 - mae: 0.5078 - val_loss: 0.1550 - val_mae: 0.2871\n",
      "Epoch 58/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - loss: 0.4268 - mae: 0.5056 - val_loss: 0.1540 - val_mae: 0.2844\n",
      "Epoch 59/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.4239 - mae: 0.5034 - val_loss: 0.1530 - val_mae: 0.2818\n",
      "Epoch 60/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.4211 - mae: 0.5013 - val_loss: 0.1521 - val_mae: 0.2798\n",
      "Epoch 61/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.4183 - mae: 0.4996 - val_loss: 0.1513 - val_mae: 0.2786\n",
      "Epoch 62/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.4156 - mae: 0.4982 - val_loss: 0.1505 - val_mae: 0.2773\n",
      "Epoch 63/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.4130 - mae: 0.4969 - val_loss: 0.1498 - val_mae: 0.2761\n",
      "Epoch 64/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - loss: 0.4105 - mae: 0.4957 - val_loss: 0.1491 - val_mae: 0.2748\n",
      "Epoch 65/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.4080 - mae: 0.4944 - val_loss: 0.1485 - val_mae: 0.2736\n",
      "Epoch 66/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.4055 - mae: 0.4932 - val_loss: 0.1479 - val_mae: 0.2725\n",
      "Epoch 67/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.4031 - mae: 0.4920 - val_loss: 0.1473 - val_mae: 0.2713\n",
      "Epoch 68/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.4008 - mae: 0.4908 - val_loss: 0.1467 - val_mae: 0.2702\n",
      "Epoch 69/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.3985 - mae: 0.4897 - val_loss: 0.1462 - val_mae: 0.2691\n",
      "Epoch 70/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 0.3963 - mae: 0.4886 - val_loss: 0.1457 - val_mae: 0.2681\n",
      "Epoch 71/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.3941 - mae: 0.4875 - val_loss: 0.1452 - val_mae: 0.2670\n",
      "Epoch 72/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.3920 - mae: 0.4864 - val_loss: 0.1448 - val_mae: 0.2660\n",
      "Epoch 73/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.3899 - mae: 0.4854 - val_loss: 0.1443 - val_mae: 0.2650\n",
      "Epoch 74/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.3878 - mae: 0.4843 - val_loss: 0.1439 - val_mae: 0.2640\n",
      "Epoch 75/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.3858 - mae: 0.4834 - val_loss: 0.1434 - val_mae: 0.2630\n",
      "Epoch 76/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - loss: 0.3838 - mae: 0.4824 - val_loss: 0.1430 - val_mae: 0.2621\n",
      "Epoch 77/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.3818 - mae: 0.4815 - val_loss: 0.1426 - val_mae: 0.2612\n",
      "Epoch 78/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.3799 - mae: 0.4805 - val_loss: 0.1422 - val_mae: 0.2603\n",
      "Epoch 79/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.3780 - mae: 0.4797 - val_loss: 0.1418 - val_mae: 0.2594\n",
      "Epoch 80/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.3761 - mae: 0.4788 - val_loss: 0.1414 - val_mae: 0.2586\n",
      "Epoch 81/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.3743 - mae: 0.4780 - val_loss: 0.1410 - val_mae: 0.2578\n",
      "Epoch 82/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - loss: 0.3725 - mae: 0.4771 - val_loss: 0.1406 - val_mae: 0.2570\n",
      "Epoch 83/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.3707 - mae: 0.4763 - val_loss: 0.1402 - val_mae: 0.2562\n",
      "Epoch 84/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.3689 - mae: 0.4756 - val_loss: 0.1398 - val_mae: 0.2554\n",
      "Epoch 85/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.3672 - mae: 0.4748 - val_loss: 0.1395 - val_mae: 0.2547\n",
      "Epoch 86/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.3655 - mae: 0.4741 - val_loss: 0.1391 - val_mae: 0.2539\n",
      "Epoch 87/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.3638 - mae: 0.4734 - val_loss: 0.1387 - val_mae: 0.2532\n",
      "Epoch 88/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - loss: 0.3621 - mae: 0.4727 - val_loss: 0.1383 - val_mae: 0.2525\n",
      "Epoch 89/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.3605 - mae: 0.4720 - val_loss: 0.1379 - val_mae: 0.2518\n",
      "Epoch 90/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.3588 - mae: 0.4713 - val_loss: 0.1375 - val_mae: 0.2511\n",
      "Epoch 91/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.3572 - mae: 0.4707 - val_loss: 0.1371 - val_mae: 0.2505\n",
      "Epoch 92/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.3557 - mae: 0.4701 - val_loss: 0.1367 - val_mae: 0.2498\n",
      "Epoch 93/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.3541 - mae: 0.4695 - val_loss: 0.1363 - val_mae: 0.2492\n",
      "Epoch 94/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - loss: 0.3526 - mae: 0.4689 - val_loss: 0.1359 - val_mae: 0.2486\n",
      "Epoch 95/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.3510 - mae: 0.4683 - val_loss: 0.1355 - val_mae: 0.2480\n",
      "Epoch 96/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.3495 - mae: 0.4677 - val_loss: 0.1351 - val_mae: 0.2474\n",
      "Epoch 97/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.3480 - mae: 0.4672 - val_loss: 0.1347 - val_mae: 0.2468\n",
      "Epoch 98/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.3466 - mae: 0.4666 - val_loss: 0.1343 - val_mae: 0.2462\n",
      "Epoch 99/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.3451 - mae: 0.4661 - val_loss: 0.1339 - val_mae: 0.2457\n",
      "Epoch 100/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 0.3437 - mae: 0.4656 - val_loss: 0.1335 - val_mae: 0.2451\n",
      "Epoch 101/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.3423 - mae: 0.4651 - val_loss: 0.1330 - val_mae: 0.2446\n",
      "Epoch 102/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.3409 - mae: 0.4646 - val_loss: 0.1326 - val_mae: 0.2441\n",
      "Epoch 103/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.3395 - mae: 0.4641 - val_loss: 0.1322 - val_mae: 0.2435\n",
      "Epoch 104/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.3382 - mae: 0.4636 - val_loss: 0.1318 - val_mae: 0.2430\n",
      "Epoch 105/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - loss: 0.3368 - mae: 0.4631 - val_loss: 0.1314 - val_mae: 0.2425\n",
      "Epoch 106/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.3355 - mae: 0.4626 - val_loss: 0.1310 - val_mae: 0.2420\n",
      "Epoch 107/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.3342 - mae: 0.4622 - val_loss: 0.1306 - val_mae: 0.2415\n",
      "Epoch 108/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.3329 - mae: 0.4617 - val_loss: 0.1302 - val_mae: 0.2410\n",
      "Epoch 109/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.3316 - mae: 0.4613 - val_loss: 0.1298 - val_mae: 0.2405\n",
      "Epoch 110/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.3303 - mae: 0.4609 - val_loss: 0.1294 - val_mae: 0.2400\n",
      "Epoch 111/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 0.3291 - mae: 0.4604 - val_loss: 0.1290 - val_mae: 0.2396\n",
      "Epoch 112/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.3279 - mae: 0.4600 - val_loss: 0.1287 - val_mae: 0.2391\n",
      "Epoch 113/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.3266 - mae: 0.4596 - val_loss: 0.1283 - val_mae: 0.2386\n",
      "Epoch 114/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.3254 - mae: 0.4591 - val_loss: 0.1279 - val_mae: 0.2382\n",
      "Epoch 115/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.3243 - mae: 0.4587 - val_loss: 0.1275 - val_mae: 0.2377\n",
      "Epoch 116/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.3231 - mae: 0.4583 - val_loss: 0.1271 - val_mae: 0.2373\n",
      "Epoch 117/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - loss: 0.3219 - mae: 0.4579 - val_loss: 0.1268 - val_mae: 0.2368\n",
      "Epoch 118/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.3208 - mae: 0.4575 - val_loss: 0.1264 - val_mae: 0.2364\n",
      "Epoch 119/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.3197 - mae: 0.4571 - val_loss: 0.1260 - val_mae: 0.2359\n",
      "Epoch 120/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.3185 - mae: 0.4567 - val_loss: 0.1257 - val_mae: 0.2355\n",
      "Epoch 121/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.3174 - mae: 0.4563 - val_loss: 0.1253 - val_mae: 0.2350\n",
      "Epoch 122/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.3164 - mae: 0.4559 - val_loss: 0.1250 - val_mae: 0.2346\n",
      "Epoch 123/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - loss: 0.3153 - mae: 0.4555 - val_loss: 0.1246 - val_mae: 0.2342\n",
      "Epoch 124/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.3142 - mae: 0.4551 - val_loss: 0.1243 - val_mae: 0.2337\n",
      "Epoch 125/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.3132 - mae: 0.4547 - val_loss: 0.1240 - val_mae: 0.2333\n",
      "Epoch 126/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.3121 - mae: 0.4543 - val_loss: 0.1236 - val_mae: 0.2329\n",
      "Epoch 127/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.3111 - mae: 0.4539 - val_loss: 0.1233 - val_mae: 0.2324\n",
      "Epoch 128/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.3101 - mae: 0.4535 - val_loss: 0.1230 - val_mae: 0.2320\n",
      "Epoch 129/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - loss: 0.3091 - mae: 0.4531 - val_loss: 0.1227 - val_mae: 0.2316\n",
      "Epoch 130/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.3081 - mae: 0.4527 - val_loss: 0.1224 - val_mae: 0.2312\n",
      "Epoch 131/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.3071 - mae: 0.4523 - val_loss: 0.1221 - val_mae: 0.2307\n",
      "Epoch 132/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.3062 - mae: 0.4519 - val_loss: 0.1218 - val_mae: 0.2303\n",
      "Epoch 133/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.3052 - mae: 0.4516 - val_loss: 0.1215 - val_mae: 0.2299\n",
      "Epoch 134/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.3043 - mae: 0.4512 - val_loss: 0.1212 - val_mae: 0.2295\n",
      "Epoch 135/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 0.3034 - mae: 0.4508 - val_loss: 0.1209 - val_mae: 0.2291\n",
      "Epoch 136/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.3024 - mae: 0.4504 - val_loss: 0.1206 - val_mae: 0.2286\n",
      "Epoch 137/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.3015 - mae: 0.4500 - val_loss: 0.1203 - val_mae: 0.2282\n",
      "Epoch 138/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.3006 - mae: 0.4496 - val_loss: 0.1200 - val_mae: 0.2278\n",
      "Epoch 139/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.2997 - mae: 0.4492 - val_loss: 0.1198 - val_mae: 0.2274\n",
      "Epoch 140/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.2989 - mae: 0.4488 - val_loss: 0.1195 - val_mae: 0.2270\n",
      "Epoch 141/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - loss: 0.2980 - mae: 0.4485 - val_loss: 0.1192 - val_mae: 0.2266\n",
      "Epoch 142/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.2971 - mae: 0.4481 - val_loss: 0.1190 - val_mae: 0.2262\n",
      "Epoch 143/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.2963 - mae: 0.4477 - val_loss: 0.1187 - val_mae: 0.2258\n",
      "Epoch 144/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.2954 - mae: 0.4473 - val_loss: 0.1185 - val_mae: 0.2253\n",
      "Epoch 145/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.2946 - mae: 0.4469 - val_loss: 0.1182 - val_mae: 0.2249\n",
      "Epoch 146/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.2938 - mae: 0.4465 - val_loss: 0.1180 - val_mae: 0.2245\n",
      "Epoch 147/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - loss: 0.2930 - mae: 0.4461 - val_loss: 0.1177 - val_mae: 0.2241\n",
      "Epoch 148/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.2922 - mae: 0.4458 - val_loss: 0.1175 - val_mae: 0.2237\n",
      "Epoch 149/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.2914 - mae: 0.4454 - val_loss: 0.1173 - val_mae: 0.2233\n",
      "Epoch 150/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.2906 - mae: 0.4450 - val_loss: 0.1170 - val_mae: 0.2229\n",
      "Epoch 151/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.2898 - mae: 0.4446 - val_loss: 0.1168 - val_mae: 0.2225\n",
      "Epoch 152/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.2890 - mae: 0.4442 - val_loss: 0.1166 - val_mae: 0.2221\n",
      "Epoch 153/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - loss: 0.2882 - mae: 0.4438 - val_loss: 0.1164 - val_mae: 0.2217\n",
      "Epoch 154/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.2875 - mae: 0.4434 - val_loss: 0.1162 - val_mae: 0.2213\n",
      "Epoch 155/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.2867 - mae: 0.4431 - val_loss: 0.1160 - val_mae: 0.2209\n",
      "Epoch 156/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.2860 - mae: 0.4427 - val_loss: 0.1157 - val_mae: 0.2205\n",
      "Epoch 157/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.2852 - mae: 0.4423 - val_loss: 0.1155 - val_mae: 0.2203\n",
      "Epoch 158/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.2845 - mae: 0.4419 - val_loss: 0.1153 - val_mae: 0.2202\n",
      "Epoch 159/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - loss: 0.2838 - mae: 0.4415 - val_loss: 0.1151 - val_mae: 0.2201\n",
      "Epoch 160/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.2831 - mae: 0.4412 - val_loss: 0.1149 - val_mae: 0.2201\n",
      "Epoch 161/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.2824 - mae: 0.4408 - val_loss: 0.1147 - val_mae: 0.2200\n",
      "Epoch 162/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.2817 - mae: 0.4406 - val_loss: 0.1145 - val_mae: 0.2199\n",
      "Epoch 163/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.2810 - mae: 0.4403 - val_loss: 0.1143 - val_mae: 0.2199\n",
      "Epoch 164/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - loss: 0.2803 - mae: 0.4400 - val_loss: 0.1142 - val_mae: 0.2198\n",
      "Epoch 165/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.2796 - mae: 0.4397 - val_loss: 0.1140 - val_mae: 0.2197\n",
      "Epoch 166/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.2789 - mae: 0.4395 - val_loss: 0.1138 - val_mae: 0.2197\n",
      "Epoch 167/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.2782 - mae: 0.4392 - val_loss: 0.1136 - val_mae: 0.2196\n",
      "Epoch 168/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.2776 - mae: 0.4389 - val_loss: 0.1134 - val_mae: 0.2195\n",
      "Epoch 169/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.2769 - mae: 0.4387 - val_loss: 0.1132 - val_mae: 0.2194\n",
      "Epoch 170/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.2763 - mae: 0.4384 - val_loss: 0.1131 - val_mae: 0.2194\n",
      "Epoch 171/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.2756 - mae: 0.4381 - val_loss: 0.1129 - val_mae: 0.2193\n",
      "Epoch 172/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.2750 - mae: 0.4378 - val_loss: 0.1127 - val_mae: 0.2192\n",
      "Epoch 173/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.2743 - mae: 0.4375 - val_loss: 0.1125 - val_mae: 0.2191\n",
      "Epoch 174/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.2737 - mae: 0.4373 - val_loss: 0.1124 - val_mae: 0.2190\n",
      "Epoch 175/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.2731 - mae: 0.4370 - val_loss: 0.1122 - val_mae: 0.2189\n",
      "Epoch 176/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - loss: 0.2724 - mae: 0.4367 - val_loss: 0.1120 - val_mae: 0.2188\n",
      "Epoch 177/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.2718 - mae: 0.4364 - val_loss: 0.1119 - val_mae: 0.2187\n",
      "Epoch 178/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.2712 - mae: 0.4361 - val_loss: 0.1117 - val_mae: 0.2186\n",
      "Epoch 179/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.2706 - mae: 0.4359 - val_loss: 0.1115 - val_mae: 0.2185\n",
      "Epoch 180/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.2700 - mae: 0.4356 - val_loss: 0.1114 - val_mae: 0.2184\n",
      "Epoch 181/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.2694 - mae: 0.4353 - val_loss: 0.1112 - val_mae: 0.2183\n",
      "Epoch 182/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - loss: 0.2688 - mae: 0.4350 - val_loss: 0.1110 - val_mae: 0.2182\n",
      "Epoch 183/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.2682 - mae: 0.4347 - val_loss: 0.1109 - val_mae: 0.2181\n",
      "Epoch 184/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - loss: 0.2676 - mae: 0.4344 - val_loss: 0.1107 - val_mae: 0.2180\n",
      "Epoch 185/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.2670 - mae: 0.4341 - val_loss: 0.1106 - val_mae: 0.2179\n",
      "Epoch 186/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.2664 - mae: 0.4338 - val_loss: 0.1104 - val_mae: 0.2178\n",
      "Epoch 187/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - loss: 0.2659 - mae: 0.4335 - val_loss: 0.1103 - val_mae: 0.2177\n",
      "Epoch 188/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.2653 - mae: 0.4332 - val_loss: 0.1101 - val_mae: 0.2177\n",
      "Epoch 189/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.2647 - mae: 0.4330 - val_loss: 0.1100 - val_mae: 0.2177\n",
      "Epoch 190/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.2642 - mae: 0.4327 - val_loss: 0.1098 - val_mae: 0.2177\n",
      "Epoch 191/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.2636 - mae: 0.4324 - val_loss: 0.1097 - val_mae: 0.2177\n",
      "Epoch 192/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.2630 - mae: 0.4321 - val_loss: 0.1095 - val_mae: 0.2177\n",
      "Epoch 193/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 0.2625 - mae: 0.4318 - val_loss: 0.1094 - val_mae: 0.2177\n",
      "Epoch 194/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.2619 - mae: 0.4315 - val_loss: 0.1092 - val_mae: 0.2176\n",
      "Epoch 195/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.2614 - mae: 0.4312 - val_loss: 0.1091 - val_mae: 0.2176\n",
      "Epoch 196/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.2609 - mae: 0.4309 - val_loss: 0.1089 - val_mae: 0.2176\n",
      "Epoch 197/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.2603 - mae: 0.4306 - val_loss: 0.1088 - val_mae: 0.2175\n",
      "Epoch 198/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.2598 - mae: 0.4303 - val_loss: 0.1086 - val_mae: 0.2175\n",
      "Epoch 199/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - loss: 0.2592 - mae: 0.4300 - val_loss: 0.1085 - val_mae: 0.2174\n",
      "Epoch 200/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.2587 - mae: 0.4297 - val_loss: 0.1084 - val_mae: 0.2174\n",
      "Epoch 1/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 543ms/step - loss: 0.9598 - mae: 0.8250 - val_loss: 0.6926 - val_mae: 0.7692\n",
      "Epoch 2/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.9489 - mae: 0.8188 - val_loss: 0.6805 - val_mae: 0.7610\n",
      "Epoch 3/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.9380 - mae: 0.8125 - val_loss: 0.6684 - val_mae: 0.7527\n",
      "Epoch 4/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.9271 - mae: 0.8062 - val_loss: 0.6564 - val_mae: 0.7442\n",
      "Epoch 5/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 0.9162 - mae: 0.7998 - val_loss: 0.6444 - val_mae: 0.7356\n",
      "Epoch 6/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.9052 - mae: 0.7932 - val_loss: 0.6324 - val_mae: 0.7269\n",
      "Epoch 7/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.8943 - mae: 0.7867 - val_loss: 0.6205 - val_mae: 0.7180\n",
      "Epoch 8/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.8834 - mae: 0.7800 - val_loss: 0.6086 - val_mae: 0.7090\n",
      "Epoch 9/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.8724 - mae: 0.7733 - val_loss: 0.5968 - val_mae: 0.6999\n",
      "Epoch 10/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - loss: 0.8615 - mae: 0.7678 - val_loss: 0.5851 - val_mae: 0.6907\n",
      "Epoch 11/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - loss: 0.8506 - mae: 0.7622 - val_loss: 0.5735 - val_mae: 0.6813\n",
      "Epoch 12/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.8397 - mae: 0.7566 - val_loss: 0.5621 - val_mae: 0.6719\n",
      "Epoch 13/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.8289 - mae: 0.7508 - val_loss: 0.5507 - val_mae: 0.6623\n",
      "Epoch 14/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.8181 - mae: 0.7450 - val_loss: 0.5395 - val_mae: 0.6527\n",
      "Epoch 15/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.8074 - mae: 0.7392 - val_loss: 0.5284 - val_mae: 0.6429\n",
      "Epoch 16/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.7968 - mae: 0.7332 - val_loss: 0.5175 - val_mae: 0.6331\n",
      "Epoch 17/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 0.7862 - mae: 0.7272 - val_loss: 0.5067 - val_mae: 0.6232\n",
      "Epoch 18/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.7757 - mae: 0.7212 - val_loss: 0.4961 - val_mae: 0.6133\n",
      "Epoch 19/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.7654 - mae: 0.7151 - val_loss: 0.4857 - val_mae: 0.6032\n",
      "Epoch 20/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.7551 - mae: 0.7090 - val_loss: 0.4754 - val_mae: 0.5932\n",
      "Epoch 21/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.7449 - mae: 0.7028 - val_loss: 0.4654 - val_mae: 0.5830\n",
      "Epoch 22/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - loss: 0.7349 - mae: 0.6966 - val_loss: 0.4556 - val_mae: 0.5729\n",
      "Epoch 23/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 0.7250 - mae: 0.6904 - val_loss: 0.4459 - val_mae: 0.5630\n",
      "Epoch 24/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.7152 - mae: 0.6841 - val_loss: 0.4365 - val_mae: 0.5542\n",
      "Epoch 25/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.7056 - mae: 0.6778 - val_loss: 0.4273 - val_mae: 0.5454\n",
      "Epoch 26/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.6961 - mae: 0.6716 - val_loss: 0.4183 - val_mae: 0.5366\n",
      "Epoch 27/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.6868 - mae: 0.6653 - val_loss: 0.4095 - val_mae: 0.5279\n",
      "Epoch 28/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.6777 - mae: 0.6590 - val_loss: 0.4009 - val_mae: 0.5192\n",
      "Epoch 29/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - loss: 0.6687 - mae: 0.6528 - val_loss: 0.3925 - val_mae: 0.5105\n",
      "Epoch 30/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.6599 - mae: 0.6466 - val_loss: 0.3843 - val_mae: 0.5018\n",
      "Epoch 31/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.6513 - mae: 0.6404 - val_loss: 0.3763 - val_mae: 0.4932\n",
      "Epoch 32/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.6428 - mae: 0.6342 - val_loss: 0.3685 - val_mae: 0.4846\n",
      "Epoch 33/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.6346 - mae: 0.6281 - val_loss: 0.3609 - val_mae: 0.4761\n",
      "Epoch 34/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - loss: 0.6265 - mae: 0.6220 - val_loss: 0.3535 - val_mae: 0.4677\n",
      "Epoch 35/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.6186 - mae: 0.6160 - val_loss: 0.3463 - val_mae: 0.4599\n",
      "Epoch 36/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.6108 - mae: 0.6103 - val_loss: 0.3392 - val_mae: 0.4542\n",
      "Epoch 37/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.6032 - mae: 0.6059 - val_loss: 0.3323 - val_mae: 0.4485\n",
      "Epoch 38/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.5958 - mae: 0.6016 - val_loss: 0.3256 - val_mae: 0.4428\n",
      "Epoch 39/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.5886 - mae: 0.5973 - val_loss: 0.3191 - val_mae: 0.4376\n",
      "Epoch 40/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - loss: 0.5815 - mae: 0.5934 - val_loss: 0.3126 - val_mae: 0.4326\n",
      "Epoch 41/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.5746 - mae: 0.5895 - val_loss: 0.3064 - val_mae: 0.4276\n",
      "Epoch 42/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.5678 - mae: 0.5857 - val_loss: 0.3002 - val_mae: 0.4226\n",
      "Epoch 43/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.5611 - mae: 0.5819 - val_loss: 0.2942 - val_mae: 0.4175\n",
      "Epoch 44/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.5546 - mae: 0.5781 - val_loss: 0.2884 - val_mae: 0.4124\n",
      "Epoch 45/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.5483 - mae: 0.5743 - val_loss: 0.2826 - val_mae: 0.4073\n",
      "Epoch 46/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - loss: 0.5420 - mae: 0.5706 - val_loss: 0.2770 - val_mae: 0.4022\n",
      "Epoch 47/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.5358 - mae: 0.5668 - val_loss: 0.2715 - val_mae: 0.3970\n",
      "Epoch 48/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.5298 - mae: 0.5631 - val_loss: 0.2661 - val_mae: 0.3918\n",
      "Epoch 49/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.5238 - mae: 0.5594 - val_loss: 0.2609 - val_mae: 0.3866\n",
      "Epoch 50/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.5179 - mae: 0.5557 - val_loss: 0.2557 - val_mae: 0.3814\n",
      "Epoch 51/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.5121 - mae: 0.5520 - val_loss: 0.2507 - val_mae: 0.3766\n",
      "Epoch 52/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.5063 - mae: 0.5488 - val_loss: 0.2457 - val_mae: 0.3721\n",
      "Epoch 53/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.5006 - mae: 0.5456 - val_loss: 0.2409 - val_mae: 0.3675\n",
      "Epoch 54/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.4950 - mae: 0.5424 - val_loss: 0.2362 - val_mae: 0.3629\n",
      "Epoch 55/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.4894 - mae: 0.5392 - val_loss: 0.2316 - val_mae: 0.3583\n",
      "Epoch 56/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.4838 - mae: 0.5360 - val_loss: 0.2271 - val_mae: 0.3535\n",
      "Epoch 57/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.4782 - mae: 0.5328 - val_loss: 0.2228 - val_mae: 0.3488\n",
      "Epoch 58/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.4727 - mae: 0.5295 - val_loss: 0.2185 - val_mae: 0.3440\n",
      "Epoch 59/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.4672 - mae: 0.5263 - val_loss: 0.2144 - val_mae: 0.3392\n",
      "Epoch 60/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.4617 - mae: 0.5230 - val_loss: 0.2104 - val_mae: 0.3344\n",
      "Epoch 61/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.4562 - mae: 0.5198 - val_loss: 0.2066 - val_mae: 0.3295\n",
      "Epoch 62/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.4507 - mae: 0.5165 - val_loss: 0.2029 - val_mae: 0.3246\n",
      "Epoch 63/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - loss: 0.4452 - mae: 0.5132 - val_loss: 0.1993 - val_mae: 0.3197\n",
      "Epoch 64/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.4397 - mae: 0.5099 - val_loss: 0.1958 - val_mae: 0.3148\n",
      "Epoch 65/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.4342 - mae: 0.5066 - val_loss: 0.1925 - val_mae: 0.3099\n",
      "Epoch 66/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.4287 - mae: 0.5032 - val_loss: 0.1892 - val_mae: 0.3051\n",
      "Epoch 67/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.4232 - mae: 0.4999 - val_loss: 0.1862 - val_mae: 0.3002\n",
      "Epoch 68/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.4178 - mae: 0.4966 - val_loss: 0.1832 - val_mae: 0.2953\n",
      "Epoch 69/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - loss: 0.4123 - mae: 0.4932 - val_loss: 0.1804 - val_mae: 0.2904\n",
      "Epoch 70/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.4068 - mae: 0.4899 - val_loss: 0.1777 - val_mae: 0.2856\n",
      "Epoch 71/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.4014 - mae: 0.4865 - val_loss: 0.1752 - val_mae: 0.2808\n",
      "Epoch 72/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.3960 - mae: 0.4831 - val_loss: 0.1727 - val_mae: 0.2760\n",
      "Epoch 73/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.3907 - mae: 0.4798 - val_loss: 0.1704 - val_mae: 0.2713\n",
      "Epoch 74/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.3853 - mae: 0.4764 - val_loss: 0.1682 - val_mae: 0.2666\n",
      "Epoch 75/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - loss: 0.3800 - mae: 0.4731 - val_loss: 0.1661 - val_mae: 0.2622\n",
      "Epoch 76/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.3748 - mae: 0.4699 - val_loss: 0.1641 - val_mae: 0.2600\n",
      "Epoch 77/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.3696 - mae: 0.4680 - val_loss: 0.1622 - val_mae: 0.2578\n",
      "Epoch 78/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.3645 - mae: 0.4661 - val_loss: 0.1604 - val_mae: 0.2557\n",
      "Epoch 79/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.3594 - mae: 0.4641 - val_loss: 0.1586 - val_mae: 0.2535\n",
      "Epoch 80/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.3544 - mae: 0.4622 - val_loss: 0.1570 - val_mae: 0.2513\n",
      "Epoch 81/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - loss: 0.3495 - mae: 0.4602 - val_loss: 0.1554 - val_mae: 0.2495\n",
      "Epoch 82/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.3447 - mae: 0.4584 - val_loss: 0.1539 - val_mae: 0.2476\n",
      "Epoch 83/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.3399 - mae: 0.4566 - val_loss: 0.1524 - val_mae: 0.2458\n",
      "Epoch 84/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.3352 - mae: 0.4548 - val_loss: 0.1510 - val_mae: 0.2439\n",
      "Epoch 85/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.3307 - mae: 0.4529 - val_loss: 0.1496 - val_mae: 0.2420\n",
      "Epoch 86/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.3262 - mae: 0.4510 - val_loss: 0.1483 - val_mae: 0.2402\n",
      "Epoch 87/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - loss: 0.3218 - mae: 0.4491 - val_loss: 0.1470 - val_mae: 0.2391\n",
      "Epoch 88/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.3175 - mae: 0.4472 - val_loss: 0.1457 - val_mae: 0.2379\n",
      "Epoch 89/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.3134 - mae: 0.4453 - val_loss: 0.1444 - val_mae: 0.2367\n",
      "Epoch 90/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.3093 - mae: 0.4433 - val_loss: 0.1432 - val_mae: 0.2355\n",
      "Epoch 91/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.3053 - mae: 0.4414 - val_loss: 0.1419 - val_mae: 0.2342\n",
      "Epoch 92/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.3014 - mae: 0.4394 - val_loss: 0.1407 - val_mae: 0.2329\n",
      "Epoch 93/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - loss: 0.2976 - mae: 0.4375 - val_loss: 0.1395 - val_mae: 0.2315\n",
      "Epoch 94/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.2940 - mae: 0.4355 - val_loss: 0.1383 - val_mae: 0.2301\n",
      "Epoch 95/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.2904 - mae: 0.4336 - val_loss: 0.1370 - val_mae: 0.2287\n",
      "Epoch 96/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.2869 - mae: 0.4316 - val_loss: 0.1358 - val_mae: 0.2273\n",
      "Epoch 97/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.2835 - mae: 0.4297 - val_loss: 0.1346 - val_mae: 0.2258\n",
      "Epoch 98/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.2802 - mae: 0.4278 - val_loss: 0.1334 - val_mae: 0.2243\n",
      "Epoch 99/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - loss: 0.2770 - mae: 0.4258 - val_loss: 0.1322 - val_mae: 0.2230\n",
      "Epoch 100/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.2738 - mae: 0.4239 - val_loss: 0.1309 - val_mae: 0.2218\n",
      "Epoch 101/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.2708 - mae: 0.4220 - val_loss: 0.1297 - val_mae: 0.2205\n",
      "Epoch 102/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.2678 - mae: 0.4203 - val_loss: 0.1285 - val_mae: 0.2193\n",
      "Epoch 103/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.2650 - mae: 0.4186 - val_loss: 0.1273 - val_mae: 0.2181\n",
      "Epoch 104/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - loss: 0.2622 - mae: 0.4169 - val_loss: 0.1261 - val_mae: 0.2168\n",
      "Epoch 105/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.2594 - mae: 0.4152 - val_loss: 0.1248 - val_mae: 0.2156\n",
      "Epoch 106/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.2568 - mae: 0.4135 - val_loss: 0.1236 - val_mae: 0.2143\n",
      "Epoch 107/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.2542 - mae: 0.4118 - val_loss: 0.1225 - val_mae: 0.2131\n",
      "Epoch 108/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.2517 - mae: 0.4102 - val_loss: 0.1213 - val_mae: 0.2118\n",
      "Epoch 109/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.2493 - mae: 0.4085 - val_loss: 0.1201 - val_mae: 0.2106\n",
      "Epoch 110/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - loss: 0.2469 - mae: 0.4069 - val_loss: 0.1189 - val_mae: 0.2101\n",
      "Epoch 111/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.2446 - mae: 0.4056 - val_loss: 0.1178 - val_mae: 0.2096\n",
      "Epoch 112/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.2424 - mae: 0.4045 - val_loss: 0.1167 - val_mae: 0.2092\n",
      "Epoch 113/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.2402 - mae: 0.4033 - val_loss: 0.1156 - val_mae: 0.2088\n",
      "Epoch 114/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.2381 - mae: 0.4022 - val_loss: 0.1145 - val_mae: 0.2083\n",
      "Epoch 115/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.2360 - mae: 0.4011 - val_loss: 0.1134 - val_mae: 0.2079\n",
      "Epoch 116/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 0.2340 - mae: 0.4000 - val_loss: 0.1124 - val_mae: 0.2075\n",
      "Epoch 117/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.2320 - mae: 0.3988 - val_loss: 0.1114 - val_mae: 0.2071\n",
      "Epoch 118/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.2301 - mae: 0.3977 - val_loss: 0.1104 - val_mae: 0.2067\n",
      "Epoch 119/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.2283 - mae: 0.3966 - val_loss: 0.1094 - val_mae: 0.2063\n",
      "Epoch 120/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.2264 - mae: 0.3955 - val_loss: 0.1085 - val_mae: 0.2058\n",
      "Epoch 121/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.2247 - mae: 0.3945 - val_loss: 0.1075 - val_mae: 0.2054\n",
      "Epoch 122/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.2229 - mae: 0.3934 - val_loss: 0.1067 - val_mae: 0.2050\n",
      "Epoch 123/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.2212 - mae: 0.3923 - val_loss: 0.1058 - val_mae: 0.2046\n",
      "Epoch 124/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.2196 - mae: 0.3912 - val_loss: 0.1049 - val_mae: 0.2042\n",
      "Epoch 125/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.2180 - mae: 0.3901 - val_loss: 0.1041 - val_mae: 0.2039\n",
      "Epoch 126/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.2164 - mae: 0.3890 - val_loss: 0.1033 - val_mae: 0.2035\n",
      "Epoch 127/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - loss: 0.2149 - mae: 0.3880 - val_loss: 0.1025 - val_mae: 0.2031\n",
      "Epoch 128/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.2133 - mae: 0.3869 - val_loss: 0.1018 - val_mae: 0.2027\n",
      "Epoch 129/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.2119 - mae: 0.3858 - val_loss: 0.1011 - val_mae: 0.2023\n",
      "Epoch 130/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.2104 - mae: 0.3847 - val_loss: 0.1004 - val_mae: 0.2019\n",
      "Epoch 131/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.2090 - mae: 0.3837 - val_loss: 0.0997 - val_mae: 0.2015\n",
      "Epoch 132/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.2076 - mae: 0.3826 - val_loss: 0.0990 - val_mae: 0.2012\n",
      "Epoch 133/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - loss: 0.2063 - mae: 0.3816 - val_loss: 0.0983 - val_mae: 0.2008\n",
      "Epoch 134/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.2049 - mae: 0.3805 - val_loss: 0.0977 - val_mae: 0.2004\n",
      "Epoch 135/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.2036 - mae: 0.3795 - val_loss: 0.0971 - val_mae: 0.2000\n",
      "Epoch 136/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.2023 - mae: 0.3784 - val_loss: 0.0965 - val_mae: 0.1997\n",
      "Epoch 137/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.2011 - mae: 0.3774 - val_loss: 0.0959 - val_mae: 0.1993\n",
      "Epoch 138/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1998 - mae: 0.3763 - val_loss: 0.0953 - val_mae: 0.1989\n",
      "Epoch 139/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.1986 - mae: 0.3753 - val_loss: 0.0948 - val_mae: 0.1987\n",
      "Epoch 140/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.1974 - mae: 0.3743 - val_loss: 0.0942 - val_mae: 0.1986\n",
      "Epoch 141/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1962 - mae: 0.3733 - val_loss: 0.0937 - val_mae: 0.1985\n",
      "Epoch 142/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.1951 - mae: 0.3722 - val_loss: 0.0931 - val_mae: 0.1984\n",
      "Epoch 143/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.1939 - mae: 0.3712 - val_loss: 0.0926 - val_mae: 0.1983\n",
      "Epoch 144/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.1928 - mae: 0.3702 - val_loss: 0.0921 - val_mae: 0.1982\n",
      "Epoch 145/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - loss: 0.1917 - mae: 0.3692 - val_loss: 0.0916 - val_mae: 0.1980\n",
      "Epoch 146/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.1906 - mae: 0.3683 - val_loss: 0.0911 - val_mae: 0.1979\n",
      "Epoch 147/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.1896 - mae: 0.3673 - val_loss: 0.0906 - val_mae: 0.1977\n",
      "Epoch 148/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.1885 - mae: 0.3663 - val_loss: 0.0901 - val_mae: 0.1976\n",
      "Epoch 149/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.1875 - mae: 0.3653 - val_loss: 0.0896 - val_mae: 0.1974\n",
      "Epoch 150/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.1865 - mae: 0.3644 - val_loss: 0.0891 - val_mae: 0.1973\n",
      "Epoch 151/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - loss: 0.1855 - mae: 0.3634 - val_loss: 0.0887 - val_mae: 0.1971\n",
      "Epoch 152/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.1845 - mae: 0.3625 - val_loss: 0.0882 - val_mae: 0.1969\n",
      "Epoch 153/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.1835 - mae: 0.3615 - val_loss: 0.0877 - val_mae: 0.1967\n",
      "Epoch 154/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.1826 - mae: 0.3606 - val_loss: 0.0873 - val_mae: 0.1965\n",
      "Epoch 155/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.1816 - mae: 0.3597 - val_loss: 0.0868 - val_mae: 0.1963\n",
      "Epoch 156/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.1807 - mae: 0.3588 - val_loss: 0.0864 - val_mae: 0.1961\n",
      "Epoch 157/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 0.1797 - mae: 0.3578 - val_loss: 0.0859 - val_mae: 0.1959\n",
      "Epoch 158/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1788 - mae: 0.3569 - val_loss: 0.0855 - val_mae: 0.1957\n",
      "Epoch 159/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.1779 - mae: 0.3560 - val_loss: 0.0851 - val_mae: 0.1955\n",
      "Epoch 160/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.1770 - mae: 0.3551 - val_loss: 0.0846 - val_mae: 0.1952\n",
      "Epoch 161/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.1762 - mae: 0.3543 - val_loss: 0.0842 - val_mae: 0.1950\n",
      "Epoch 162/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - loss: 0.1753 - mae: 0.3534 - val_loss: 0.0838 - val_mae: 0.1948\n",
      "Epoch 163/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.1745 - mae: 0.3525 - val_loss: 0.0834 - val_mae: 0.1945\n",
      "Epoch 164/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.1736 - mae: 0.3516 - val_loss: 0.0830 - val_mae: 0.1943\n",
      "Epoch 165/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.1728 - mae: 0.3508 - val_loss: 0.0825 - val_mae: 0.1941\n",
      "Epoch 166/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.1720 - mae: 0.3499 - val_loss: 0.0821 - val_mae: 0.1938\n",
      "Epoch 167/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.1711 - mae: 0.3491 - val_loss: 0.0817 - val_mae: 0.1936\n",
      "Epoch 168/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - loss: 0.1703 - mae: 0.3482 - val_loss: 0.0813 - val_mae: 0.1933\n",
      "Epoch 169/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.1695 - mae: 0.3474 - val_loss: 0.0809 - val_mae: 0.1931\n",
      "Epoch 170/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.1687 - mae: 0.3465 - val_loss: 0.0806 - val_mae: 0.1928\n",
      "Epoch 171/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1680 - mae: 0.3457 - val_loss: 0.0802 - val_mae: 0.1925\n",
      "Epoch 172/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.1672 - mae: 0.3449 - val_loss: 0.0798 - val_mae: 0.1923\n",
      "Epoch 173/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1664 - mae: 0.3440 - val_loss: 0.0794 - val_mae: 0.1920\n",
      "Epoch 174/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - loss: 0.1657 - mae: 0.3432 - val_loss: 0.0790 - val_mae: 0.1918\n",
      "Epoch 175/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.1649 - mae: 0.3424 - val_loss: 0.0787 - val_mae: 0.1915\n",
      "Epoch 176/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1642 - mae: 0.3416 - val_loss: 0.0783 - val_mae: 0.1912\n",
      "Epoch 177/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.1635 - mae: 0.3408 - val_loss: 0.0779 - val_mae: 0.1910\n",
      "Epoch 178/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.1628 - mae: 0.3400 - val_loss: 0.0776 - val_mae: 0.1907\n",
      "Epoch 179/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 0.1621 - mae: 0.3392 - val_loss: 0.0772 - val_mae: 0.1904\n",
      "Epoch 180/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - loss: 0.1613 - mae: 0.3384 - val_loss: 0.0769 - val_mae: 0.1901\n",
      "Epoch 181/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 0.1607 - mae: 0.3376 - val_loss: 0.0765 - val_mae: 0.1899\n",
      "Epoch 182/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.1600 - mae: 0.3368 - val_loss: 0.0762 - val_mae: 0.1896\n",
      "Epoch 183/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.1593 - mae: 0.3360 - val_loss: 0.0759 - val_mae: 0.1893\n",
      "Epoch 184/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.1586 - mae: 0.3352 - val_loss: 0.0755 - val_mae: 0.1891\n",
      "Epoch 185/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - loss: 0.1579 - mae: 0.3345 - val_loss: 0.0752 - val_mae: 0.1888\n",
      "Epoch 186/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 0.1573 - mae: 0.3337 - val_loss: 0.0749 - val_mae: 0.1885\n",
      "Epoch 187/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.1566 - mae: 0.3329 - val_loss: 0.0746 - val_mae: 0.1882\n",
      "Epoch 188/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.1560 - mae: 0.3321 - val_loss: 0.0742 - val_mae: 0.1880\n",
      "Epoch 189/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.1553 - mae: 0.3314 - val_loss: 0.0739 - val_mae: 0.1877\n",
      "Epoch 190/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.1547 - mae: 0.3306 - val_loss: 0.0736 - val_mae: 0.1874\n",
      "Epoch 191/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - loss: 0.1541 - mae: 0.3299 - val_loss: 0.0733 - val_mae: 0.1871\n",
      "Epoch 192/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.1534 - mae: 0.3291 - val_loss: 0.0730 - val_mae: 0.1868\n",
      "Epoch 193/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 0.1528 - mae: 0.3284 - val_loss: 0.0727 - val_mae: 0.1866\n",
      "Epoch 194/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.1522 - mae: 0.3276 - val_loss: 0.0724 - val_mae: 0.1863\n",
      "Epoch 195/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.1516 - mae: 0.3269 - val_loss: 0.0721 - val_mae: 0.1860\n",
      "Epoch 196/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.1510 - mae: 0.3262 - val_loss: 0.0718 - val_mae: 0.1857\n",
      "Epoch 197/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - loss: 0.1504 - mae: 0.3254 - val_loss: 0.0715 - val_mae: 0.1855\n",
      "Epoch 198/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.1498 - mae: 0.3247 - val_loss: 0.0712 - val_mae: 0.1852\n",
      "Epoch 199/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.1493 - mae: 0.3240 - val_loss: 0.0709 - val_mae: 0.1849\n",
      "Epoch 200/200\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.1487 - mae: 0.3233 - val_loss: 0.0706 - val_mae: 0.1846\n"
     ]
    }
   ],
   "source": [
    "for act in activations:\n",
    "\n",
    "    tf.keras.backend.clear_session()\n",
    "    np.random.seed(SEED); random.seed(SEED); tf.random.set_seed(SEED)\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Input(shape=(X_train_z.shape[1],)),\n",
    "        layers.Dense(16, activation=act),\n",
    "        layers.Dense(2,  activation='tanh')     # ⬅️ zwei Ausgänge: [Mn, Mw]\n",
    "    ])\n",
    "    model.compile(optimizer=optimizers.Adam(learning_rate=1e-3),\n",
    "                  loss='mse', metrics=['mae'])\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train_z, y_train_s,                    # y_train_s: (n,2)\n",
    "        validation_data=(X_val_z, y_val_s),\n",
    "        epochs=200, batch_size=16, shuffle=True, verbose=1,\n",
    "        callbacks=[]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "0YBfUSrKBSZN"
   },
   "outputs": [],
   "source": [
    "\n",
    "    # Vorhersage (skaliert) -> zurückskalieren\n",
    "    y_hat_train_s = model.predict(X_train_z, verbose=0)\n",
    "    y_hat_val_s   = model.predict(X_val_z,   verbose=0)\n",
    "    y_hat_test_s  = model.predict(X_test_z,  verbose=0)\n",
    "\n",
    "    y_hat_train = inv_y(y_hat_train_s)          # (n,2) in realen Einheiten\n",
    "    y_hat_val   = inv_y(y_hat_val_s)\n",
    "    y_hat_test  = inv_y(y_hat_test_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "Q9ZIfV65BSWm",
    "outputId": "6998d8e4-f6a1-4037-c092-cbc25bf21aaf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 10 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000020F92389C60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000020F93511580> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "transfer_functions = {\n",
    "    'relu': 'ReLU',\n",
    "    'tanh': 'Tanh',\n",
    "    'sigmoid': 'Sigmoid',\n",
    "    'softplus': 'Softplus'\n",
    "}\n",
    "\n",
    "mse_results = []\n",
    "\n",
    "for activation, name in transfer_functions.items():\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(X_train.shape[1],)))\n",
    "    model.add(Dense(16, activation=activation))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(2, activation='tanh'))   # ⬅️ zwei Outputs: [Mn, Mw]\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    model.fit(X_train, y_train, batch_size=4, epochs=50, verbose=0)\n",
    "    y_pred = model.predict(X_test, verbose=0)  # Form (n,2)\n",
    "    # Durchschnitts-MSE über beide Outputs (Mn & Mw)\n",
    "    mse = mean_squared_error(y_test, y_pred)   # = uniform_average\n",
    "    mse_results.append(mse)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(list(transfer_functions.values()), mse_results, marker='o', linestyle='-')\n",
    "plt.title('Mean MSE (Mn & Mw) vs. Transfer Function')\n",
    "plt.xlabel('Transfer Function')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ky2flHEEBSUj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mEyyme3-BSSL"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KhVOphxKBSPj"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyM6viJ1X4oRLwr65SDqw9Ni",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
